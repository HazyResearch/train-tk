Submodule ThunderKittens 5d18ccf...0000000 (submodule deleted)
diff --git a/assets/banner.png b/assets/banner.png
deleted file mode 100644
index 20110b9..0000000
Binary files a/assets/banner.png and /dev/null differ
diff --git a/assets/tradeoff.png b/assets/tradeoff.png
deleted file mode 100644
index a55b6d8..0000000
Binary files a/assets/tradeoff.png and /dev/null differ
diff --git a/based/__init__.py b/based/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/based/csrc/layer_norm/README.md b/based/csrc/layer_norm/README.md
deleted file mode 100644
index 0d3cba2..0000000
--- a/based/csrc/layer_norm/README.md
+++ /dev/null
@@ -1,17 +0,0 @@
-Source: Tri Dao, Flash attention
-This CUDA extension implements fused dropout + residual + LayerNorm, building on
-Apex's [FastLayerNorm](https://github.com/NVIDIA/apex/tree/master/apex/contrib/layer_norm).
-Major changes:
-- Add dropout and residual.
-- Make it work for both pre-norm and post-norm architecture.
-- Support more hidden dimensions (all dimensions divisible by 8, up to 8192).
-- Implement RMSNorm as an option.
-- Support layer norm with parallel residual (e.g., GPT-J, GPT-NeoX, PaLM).
-
-If you want to use it for dimensions larger than 8k, please file an issue.
-
-This extension has only been tested on A100s.
-
-```sh
-cd csrc/layer_norm && pip install .
-```
diff --git a/based/csrc/layer_norm/ln.h b/based/csrc/layer_norm/ln.h
deleted file mode 100644
index 9830c09..0000000
--- a/based/csrc/layer_norm/ln.h
+++ /dev/null
@@ -1,281 +0,0 @@
-#pragma once
-
-#include <unordered_map>
-#include <cuda_fp16.h>
-#include <cuda_bf16.h>
-
-#ifdef OLD_GENERATOR_PATH
-#include <ATen/CUDAGeneratorImpl.h>
-#else
-#include <ATen/cuda/CUDAGeneratorImpl.h>
-#endif
-
-namespace layer_norm {
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-template<typename Params>
-struct LaunchParams{
-
-    size_t elts_per_thread;
-    size_t workspace_bytes;
-    size_t barrier_size;
-
-    cudaDeviceProp * props;
-
-    cudaStream_t stream;
-
-    Params params;
-
-};
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-struct ParamsBase {
-    ParamsBase()
-        : ctas_per_col(0)
-        , rows(0)
-        , cols(0)
-        , x(nullptr)
-        , mu(nullptr)
-        , rs(nullptr)
-        , gamma(nullptr)
-        , gamma1(nullptr)
-        , rowscale(nullptr)
-        , colscale(nullptr)
-        , dropout_keep_p(1.f)
-        , dropout_scale(1.f)
-        , is_rms_norm(false)
-        , workspace(nullptr)
-        , barrier(nullptr)
-    {
-    }
-
-    // For Multi-CTA, number of different CTA groups. Otherwise same as gridDim.x.
-    int ctas_per_col;
-
-    // Input is interpreted as matrix. We normalize across columns.
-    int rows;
-    int cols;
-
-    // Common data pointers.
-    void *x0;
-    void *x1;
-    void *residual;
-    void *x;
-    void *dmask;
-    void *dmask1;
-    void *mu;
-    void *rs;
-    void *gamma;
-    void *gamma1;
-    void *rowscale;
-    void *colscale;
-    void *x0_subset;
-    void *z_subset;
-
-    float inverse_cols;
-
-    float dropout_keep_p;
-    float dropout_scale;
-    float rowscale_const;
-
-    bool is_rms_norm;
-
-    // Multi-CTA workspace in gmem.
-    void *workspace;
-
-    // Multi-CTA sync barriers in gmem.
-    int *barrier;
-
-};
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-struct FwdParams : public ParamsBase {
-    FwdParams()
-        : ParamsBase()
-        , z(nullptr)
-        , z1(nullptr)
-        , beta(nullptr)
-        , beta1(nullptr)
-        , epsilon(0.f)
-    {
-    }
-
-    // Output of LN FWD.
-    void *z;
-    void *z1;
-    void *beta;
-    void *beta1;
-    float epsilon;
-
-    // Random state.
-    at::PhiloxCudaState philox_args;
-};
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-struct BwdParams : public ParamsBase {
-    BwdParams()
-        : ParamsBase()
-        , dz(nullptr)
-        , dz1(nullptr)
-        , dx(nullptr)
-        , dbeta_part(nullptr)
-        , dgamma_part(nullptr)
-        , dbeta1_part(nullptr)
-        , dgamma1_part(nullptr)
-        , dcolscale_part(nullptr)
-        , dx0(nullptr)
-        , dx1(nullptr)
-        , dresidual(nullptr)
-        , dbeta(nullptr)
-        , dgamma(nullptr)
-        , dbeta1(nullptr)
-        , dgamma1(nullptr)
-        , dcolscale(nullptr)
-    {
-    }
-
-    // Input: gradient wrt. LN FWD output.
-    void *dz;
-    void *dz1;
-    // Input: gradient wrt residual.
-    void *dx;
-
-    // Workspace for Wgrad pre-reduction.
-    void *dbeta_part;
-    void *dgamma_part;
-    void *dbeta1_part;
-    void *dgamma1_part;
-    void *dcolscale_part;
-
-    // Output: Dgrad.
-    void *dx0;
-    void *dx1;
-    void *dresidual;
-    // Output: Wgrad.
-    void *dbeta;
-    void *dgamma;
-    void *dbeta1;
-    void *dgamma1;
-    void *dcolscale;
-
-};
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-using FwdFunction = std::function<void(LaunchParams<FwdParams>&, const bool)>;
-using BwdFunction = std::function<void(LaunchParams<BwdParams>&, const bool)>;
-using FunctionKey = uint64_t;
-using FwdRegistry = std::unordered_map<FunctionKey, FwdFunction>;
-using BwdRegistry = std::unordered_map<FunctionKey, BwdFunction>;
-
-extern FwdRegistry FWD_FUNCS, PARALLEL_FWD_FUNCS;
-extern BwdRegistry BWD_FUNCS, PARALLEL_BWD_FUNCS;
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-using fp32 = float;
-using fp16 = half;
-using bf16 = nv_bfloat16;
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-template<typename T>
-struct TypeId{};
-
-template<>
-struct TypeId<fp16>{
-    constexpr static uint32_t Value = 0;
-};
-
-template<>
-struct TypeId<bf16>{
-    constexpr static uint32_t Value = 1;
-};
-
-template<>
-struct TypeId<fp32>{
-    constexpr static uint32_t Value = 2;
-};
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-template<typename T, int S>
-struct Type2Key{
-    constexpr static uint32_t Value = TypeId<T>::Value << S;
-};
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-template<typename T>
-struct WeightType2Key : public Type2Key<T, 0>{};
-
-template<typename T>
-struct InputType2Key : public Type2Key<T, 2>{};
-
-template<typename T>
-struct ResidualType2Key : public Type2Key<T, 4>{};
-
-template<typename T>
-struct OutputType2Key : public Type2Key<T, 6>{};
-
-template<typename T>
-struct ComputeType2Key : public Type2Key<T, 8>{};
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-template<typename W, typename I, typename R, typename O, typename C>
-struct Types2Key{
-    constexpr static uint32_t Value = WeightType2Key<W>::Value | InputType2Key<I>::Value | ResidualType2Key<R>::Value | OutputType2Key<O>::Value | ComputeType2Key<C>::Value;
-    constexpr static inline uint64_t get(const uint64_t hidden_size){
-        constexpr uint64_t type_key = Value;
-        return (type_key << 32) | hidden_size;
-    }
-};
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-template<typename W, typename I, typename R, typename O, typename C, uint64_t HIDDEN_SIZE>
-struct FwdRegistrar{
-    FwdRegistrar(FwdFunction f){
-        uint64_t key = Types2Key<W,I,R,O,C>::get(HIDDEN_SIZE);
-        FWD_FUNCS.insert({ key, f });
-    }
-};
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-template<typename W, typename I, typename R, typename O, typename C, uint64_t HIDDEN_SIZE>
-struct BwdRegistrar{
-    BwdRegistrar(BwdFunction f){
-        uint64_t key = Types2Key<W,I,R,O,C>::get(HIDDEN_SIZE);
-        BWD_FUNCS.insert({ key, f });
-    }
-};
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-template<typename W, typename I, typename R, typename O, typename C, uint64_t HIDDEN_SIZE>
-struct FwdParallelRegistrar{
-    FwdParallelRegistrar(FwdFunction f){
-        uint64_t key = Types2Key<W,I,R,O,C>::get(HIDDEN_SIZE);
-        PARALLEL_FWD_FUNCS.insert({ key, f });
-    }
-};
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-template<typename W, typename I, typename R, typename O, typename C, uint64_t HIDDEN_SIZE>
-struct BwdParallelRegistrar{
-    BwdParallelRegistrar(BwdFunction f){
-        uint64_t key = Types2Key<W,I,R,O,C>::get(HIDDEN_SIZE);
-        PARALLEL_BWD_FUNCS.insert({ key, f });
-    }
-};
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-}  // namespace layer_norm
diff --git a/based/csrc/layer_norm/ln_api.cpp b/based/csrc/layer_norm/ln_api.cpp
deleted file mode 100644
index 3981bba..0000000
--- a/based/csrc/layer_norm/ln_api.cpp
+++ /dev/null
@@ -1,850 +0,0 @@
-#include <torch/extension.h>
-#include "ATen/cuda/CUDAContext.h"
-#include <c10/cuda/CUDAGuard.h>
-
-#include "ln.h"
-
-/*
-
-Supported Type combinations:
-
-input  residual   compute   weights   output
-============================================
-fp32     fp32      fp32      fp32      fp32
-fp16     fp32      fp32      fp32      fp16
-fp16     fp16      fp32      fp32      fp16
-bf16     fp32      fp32      fp32      bf16
-bf16     bf16      fp32      fp32      bf16
-fp16     fp16      fp32      fp16      fp16
-bf16     bf16      fp32      bf16      bf16
-
-Remarks:
-Output type = Input type
-Compute always in FP32
-
-*/
-
-namespace layer_norm {
-
-// Create registries and provide runtime versions of config hash functions.
-
-FwdRegistry FWD_FUNCS, PARALLEL_FWD_FUNCS;
-BwdRegistry BWD_FUNCS, PARALLEL_BWD_FUNCS;
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-uint32_t get_type_id(torch::Dtype dtype){
-    if( dtype == torch::kFloat16 ) {
-        return TypeId<fp16>::Value;
-    } else if( dtype == torch::kBFloat16 ) {
-        return TypeId<bf16>::Value;
-    } else if( dtype == torch::kFloat32 ) {
-        return TypeId<fp32>::Value;
-    } else {
-        TORCH_CHECK(false, "Type not supported: ", dtype);
-    }
-}
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-uint64_t get_key(torch::Dtype wtype, torch::Dtype itype, torch::Dtype rtype, torch::Dtype otype, torch::Dtype ctype, uint64_t hidden_size) {
-    using namespace layer_norm;
-    uint64_t type_key = get_type_id(wtype) | (get_type_id(itype) << 2) | (get_type_id(rtype) << 4) | (get_type_id(otype) << 6) | (get_type_id(ctype) << 8);
-    uint64_t launcher_key = (type_key << 32) | hidden_size;
-    return launcher_key;
-}
-
-}  // namespace layer_norm
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-layer_norm::FwdFunction & get_fwd_launcher(torch::Dtype wtype, torch::Dtype itype, torch::Dtype rtype, torch::Dtype otype, torch::Dtype ctype, uint32_t hidden_size) {
-    auto iter = layer_norm::FWD_FUNCS.find(layer_norm::get_key(wtype, itype, rtype, otype, ctype, hidden_size));
-    if( iter != layer_norm::FWD_FUNCS.end() ) {
-        return iter->second;
-    } else {
-        TORCH_CHECK(false, "FWD: Unsupported hidden_size or types: ", hidden_size, wtype, itype, rtype, otype, ctype);
-    }
-}
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-layer_norm::BwdFunction & get_bwd_launcher(torch::Dtype wtype, torch::Dtype itype, torch::Dtype rtype, torch::Dtype otype, torch::Dtype ctype, uint32_t hidden_size) {
-    auto iter = layer_norm::BWD_FUNCS.find(layer_norm::get_key(wtype, itype, rtype, otype, ctype, hidden_size));
-    if( iter != layer_norm::BWD_FUNCS.end() ) {
-        return iter->second;
-    } else {
-        TORCH_CHECK(false, "BWD: Unsupported hidden_size or types: ", hidden_size, wtype, itype, rtype, otype, ctype);
-    }
-}
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-layer_norm::FwdFunction & get_parallel_fwd_launcher(torch::Dtype wtype, torch::Dtype itype, torch::Dtype rtype, torch::Dtype otype, torch::Dtype ctype, uint32_t hidden_size) {
-    auto iter = layer_norm::PARALLEL_FWD_FUNCS.find(layer_norm::get_key(wtype, itype, rtype, otype, ctype, hidden_size));
-    if( iter != layer_norm::PARALLEL_FWD_FUNCS.end() ) {
-        return iter->second;
-    } else {
-        TORCH_CHECK(false, "FWD: Unsupported hidden_size or types: ", hidden_size, wtype, itype, rtype, otype, ctype);
-    }
-}
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-layer_norm::BwdFunction & get_parallel_bwd_launcher(torch::Dtype wtype, torch::Dtype itype, torch::Dtype rtype, torch::Dtype otype, torch::Dtype ctype, uint32_t hidden_size) {
-    auto iter = layer_norm::PARALLEL_BWD_FUNCS.find(layer_norm::get_key(wtype, itype, rtype, otype, ctype, hidden_size));
-    if( iter != layer_norm::PARALLEL_BWD_FUNCS.end() ) {
-        return iter->second;
-    } else {
-        TORCH_CHECK(false, "BWD: Unsupported hidden_size or types: ", hidden_size, wtype, itype, rtype, otype, ctype);
-    }
-}
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-std::vector<at::Tensor> dropout_add_ln_fwd(const at::Tensor &x0,      // Input: BxSxhidden_size
-                                           c10::optional<const at::Tensor> &residual_,  // Residual: BxSxhidden_size
-                                           const at::Tensor &gamma,   // hidden_size
-                                           c10::optional<const at::Tensor> &beta_,   // hidden_size
-                                           c10::optional<const at::Tensor> &rowscale_,      // BxS
-                                           c10::optional<const at::Tensor> &colscale_,      // hidden_size
-                                           c10::optional<const at::Tensor> &x0_subset_,      // BxS
-                                           c10::optional<const at::Tensor> &z_subset_,      // BxS
-                                           const float dropout_p,
-                                           const float epsilon,
-                                           const float rowscale_const,
-                                           const int64_t z_numrows,
-                                           c10::optional<at::Generator> gen_,
-                                           bool residual_in_fp32=false,
-                                           bool is_rms_norm=false
-) {
-    auto itype = x0.scalar_type();
-    auto rtype = residual_.has_value()
-        ? residual_.value().scalar_type()
-        : (residual_in_fp32 ? torch::kFloat32 : x0.scalar_type());
-    auto wtype = gamma.scalar_type();
-    auto otype = itype;
-    auto ctype = torch::kFloat32;
-    auto mtype = torch::kUInt8;
-
-    TORCH_CHECK(x0.is_cuda());
-    TORCH_CHECK(gamma.is_cuda());
-
-    TORCH_CHECK(x0.is_contiguous());
-    // c10::IntArrayRef does not own the storage, so we need to construct a vector.
-    // Otherwise just constructing IntArrayRef({blah}) will cause uninitialized memory because
-    // blah is then deallocated.
-    std::vector<int64_t> sizes_vec {!x0_subset_.has_value() ? x0.size(0) : x0_subset_.value().size(0), x0.size(1)};
-    auto sizes = c10::IntArrayRef(sizes_vec);
-    TORCH_CHECK(x0.dim() == 2);
-    TORCH_CHECK(sizes.size() == 2);
-
-    const int rows = sizes[0];
-    const int cols = sizes[1];
-    auto hidden_size = gamma.numel();
-    TORCH_CHECK(hidden_size == cols);
-
-    if (beta_.has_value()) {
-        auto beta = beta_.value();
-        TORCH_CHECK(beta.dtype() == wtype);
-        TORCH_CHECK(beta.is_cuda());
-        TORCH_CHECK(beta.is_contiguous());
-        TORCH_CHECK(beta.sizes() == gamma.sizes());
-    }
-
-    if (residual_.has_value()) {
-        auto residual = residual_.value();
-        TORCH_CHECK(residual.is_cuda());
-        TORCH_CHECK(residual.is_contiguous());
-        TORCH_CHECK(residual.sizes() == sizes);
-    }
-
-    if (rowscale_.has_value()) {
-        auto rowscale = rowscale_.value();
-        TORCH_CHECK(rowscale.is_cuda());
-        TORCH_CHECK(rowscale.is_contiguous());
-        TORCH_CHECK(rowscale.sizes() == c10::IntArrayRef{rows});
-        TORCH_CHECK(rowscale.dtype() == itype);
-    }
-
-    if (colscale_.has_value()) {
-        auto colscale = colscale_.value();
-        TORCH_CHECK(colscale.is_cuda());
-        TORCH_CHECK(colscale.is_contiguous());
-        TORCH_CHECK(colscale.sizes() == c10::IntArrayRef{cols});
-        TORCH_CHECK(colscale.dtype() == wtype);
-    }
-
-    if (x0_subset_.has_value()) {
-        auto x0_subset = x0_subset_.value();
-        TORCH_CHECK(x0_subset.is_cuda());
-        TORCH_CHECK(x0_subset.is_contiguous());
-        TORCH_CHECK(x0_subset.sizes() == c10::IntArrayRef{rows});
-        TORCH_CHECK(x0_subset.dtype() == torch::kInt32);
-
-        TORCH_CHECK(z_subset_.has_value());
-        auto z_subset = z_subset_.value();
-        TORCH_CHECK(z_subset.is_cuda());
-        TORCH_CHECK(z_subset.is_contiguous());
-        TORCH_CHECK(z_subset.sizes() == c10::IntArrayRef{rows});
-        TORCH_CHECK(z_subset.dtype() == torch::kInt32);
-    }
-
-    TORCH_CHECK((hidden_size % 8 == 0) && (hidden_size <= 8192));
-    TORCH_CHECK(epsilon >= 0.f);
-
-    // Otherwise the kernel will be launched from cuda:0 device
-    // Cast to char to avoid compiler warning about narrowing
-    at::cuda::CUDAGuard device_guard{(char)x0.get_device()};
-
-    auto opts = x0.options();
-
-    bool save_x = residual_.has_value() || (dropout_p > 0.f) || rowscale_.has_value() || colscale_.has_value() || x0_subset_.has_value() || (itype != rtype);
-    at::Tensor x;
-    if (save_x) { x = torch::empty(sizes, opts.dtype(rtype)); }
-    at::Tensor dmask;
-    if (dropout_p > 0.f) { dmask = torch::empty(x0.sizes(), opts.dtype(mtype)); };
-    auto z = torch::empty(z_subset_.has_value() ? c10::IntArrayRef{z_numrows, cols} : sizes, opts.dtype(otype));
-
-    auto mu = torch::empty({ rows }, opts.dtype(ctype));
-    auto rsigma = torch::empty({ rows }, opts.dtype(ctype));
-
-    layer_norm::LaunchParams<layer_norm::FwdParams> launch_params;
-
-    launch_params.props = at::cuda::getCurrentDeviceProperties();
-    launch_params.stream = at::cuda::getCurrentCUDAStream().stream();
-    TORCH_CHECK(dropout_p < 1.f);
-    launch_params.params.dropout_keep_p = 1.f - dropout_p;
-    launch_params.params.residual = residual_.has_value() ? residual_.value().data_ptr() : nullptr;
-    launch_params.params.rowscale = rowscale_.has_value() ? rowscale_.value().data_ptr() : nullptr;
-    launch_params.params.colscale = colscale_.has_value() ? colscale_.value().data_ptr() : nullptr;
-    launch_params.params.x0_subset = x0_subset_.has_value() ? x0_subset_.value().data_ptr() : nullptr;
-    launch_params.params.z_subset = z_subset_.has_value() ? z_subset_.value().data_ptr() : nullptr;
-
-    auto gen = at::get_generator_or_default<at::CUDAGeneratorImpl>(
-        gen_, at::cuda::detail::getDefaultCUDAGenerator());
-
-    auto round_multiple = [](int x, int m) { return (x + m - 1) / m * m; };
-    const int multiple = hidden_size <= 1536 ? 256 : (hidden_size <= 3072 ? 512 : 1024);
-    // Request the kernel launcher.
-    auto launcher = get_fwd_launcher(wtype, itype, rtype, otype, ctype, round_multiple(hidden_size, multiple));
-
-    // Set the kernel runtime parameters.
-    layer_norm::FwdParams &params = launch_params.params;
-    params.rows = rows;
-    params.cols = cols;
-    params.x0 = x0.data_ptr();
-    params.x = save_x ? x.data_ptr() : nullptr;
-    params.dmask = dropout_p > 0.f ? dmask.data_ptr() : nullptr;
-    params.mu = mu.data_ptr();
-    params.rs = rsigma.data_ptr();
-    params.gamma = gamma.data_ptr();
-    params.beta = beta_.has_value() ? beta_.value().data_ptr() : nullptr;
-    params.z = z.data_ptr();
-    params.epsilon = epsilon;
-    params.dropout_scale = 1.f / (1.f - dropout_p);
-    params.inverse_cols = 1.f / float(params.cols);
-    params.rowscale_const = rowscale_const;
-    params.is_rms_norm = is_rms_norm;
-
-    // Query the kernel-specific launch parameters.
-    launcher(launch_params, true);
-
-    at::Tensor workspace, barrier;
-
-    if (dropout_p > 0.f) {
-        // number of times random will be generated per thread, to offset philox counter in thc random
-        // state
-        int64_t counter_offset = launch_params.elts_per_thread;
-
-        // See Note [Acquire lock when using random generators]
-        {
-            std::lock_guard<std::mutex> lock(gen->mutex_);
-            params.philox_args = gen->philox_cuda_state(counter_offset);
-        }
-    }
-
-    if( launch_params.barrier_size > 0 ) {
-        auto options = x0.options();
-        barrier = torch::zeros(launch_params.barrier_size, options.dtype(torch::kInt32));
-        workspace = torch::empty(launch_params.workspace_bytes, options.dtype(torch::kChar));
-        params.workspace = workspace.data_ptr();
-        params.barrier = barrier.data_ptr<int>();
-    }
-
-    // Launch the kernel.
-    launcher(launch_params, false);
-
-    return { z, x, dmask, mu, rsigma };
-}
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-std::vector<at::Tensor> dropout_add_ln_bwd(const at::Tensor &dz,     // BxSxhidden_size
-                                           c10::optional<const at::Tensor> &dx_,     // BxSxhidden_size
-                                           const at::Tensor &x,      // BxSxhidden_size
-                                           c10::optional<const at::Tensor> &x0_,     // BxSxhidden_size
-                                           c10::optional<const at::Tensor> &dmask_,  // BxSxhidden_size
-                                           const at::Tensor &mu,     // BxS, FP32!
-                                           const at::Tensor &rsigma, // BxS, FP32!
-                                           const at::Tensor &gamma,   // hidden_size
-                                           c10::optional<const at::Tensor> &rowscale_,      // BxS
-                                           c10::optional<const at::Tensor> &colscale_,      // hidden_size
-                                           c10::optional<const at::Tensor> &x0_subset_,      // BxS
-                                           c10::optional<const at::Tensor> &z_subset_,      // BxS
-                                           const float dropout_p,
-                                           const float rowscale_const,
-                                           const int64_t x0_numrows,
-                                           const bool has_residual,
-                                           bool is_rms_norm=false
-) {
-
-    auto itype = dz.scalar_type();
-    auto rtype = x.scalar_type();
-    auto wtype = gamma.scalar_type();
-    auto otype = itype;
-    auto ctype = torch::kFloat32;
-    auto mtype = torch::kUInt8;
-
-    if (dropout_p > 0.f) { TORCH_CHECK(dmask_.has_value()); }
-
-    TORCH_CHECK(dz.dtype() == otype);
-    TORCH_CHECK(mu.dtype() == ctype);
-    TORCH_CHECK(rsigma.dtype() == ctype);
-
-    TORCH_CHECK(x.is_cuda());
-    TORCH_CHECK(dz.is_cuda());
-    TORCH_CHECK(mu.is_cuda());
-    TORCH_CHECK(rsigma.is_cuda());
-    TORCH_CHECK(gamma.is_cuda());
-
-    TORCH_CHECK(x.is_contiguous());
-    TORCH_CHECK(dz.is_contiguous());
-
-    auto sizes = x.sizes();
-    TORCH_CHECK(sizes.size() == 2);
-    auto rows = sizes[0];
-    auto cols = sizes[1];
-    TORCH_CHECK(dz.dim() == 2);
-    TORCH_CHECK(dz.size(1) == cols);
-    auto hidden_size = gamma.numel();
-    TORCH_CHECK(hidden_size == cols);
-
-    // c10::IntArrayRef does not own the storage, so we need to construct a vector.
-    // Otherwise just constructing IntArrayRef({blah}) will cause uninitialized memory because
-    // blah is then deallocated.
-    std::vector<int64_t> x0_sizes_vec {!x0_subset_.has_value() ? rows : x0_numrows, cols};
-    auto x0_sizes = c10::IntArrayRef(x0_sizes_vec);
-
-    if (dx_.has_value()) {
-        auto dx = dx_.value();
-        TORCH_CHECK(dx.dtype() == rtype);
-        TORCH_CHECK(dx.is_cuda());
-        TORCH_CHECK(dx.is_contiguous());
-        TORCH_CHECK(dx.sizes() == sizes);
-    }
-
-    if (dmask_.has_value()) {
-        auto dmask = dmask_.value();
-        TORCH_CHECK(dmask.dtype() == mtype);
-        TORCH_CHECK(dmask.is_cuda());
-        TORCH_CHECK(dmask.is_contiguous());
-        TORCH_CHECK(dmask.sizes() == x0_sizes);
-    }
-
-    if (rowscale_.has_value()) {
-        auto rowscale = rowscale_.value();
-        TORCH_CHECK(rowscale.is_cuda());
-        TORCH_CHECK(rowscale.is_contiguous());
-        TORCH_CHECK(rowscale.sizes() == c10::IntArrayRef{rows});
-        TORCH_CHECK(rowscale.dtype() == itype);
-    }
-
-    if (colscale_.has_value()) {
-        auto colscale = colscale_.value();
-        TORCH_CHECK(colscale.is_cuda());
-        TORCH_CHECK(colscale.is_contiguous());
-        TORCH_CHECK(colscale.sizes() == c10::IntArrayRef{cols});
-        TORCH_CHECK(colscale.dtype() == wtype);
-
-        TORCH_CHECK(x0_.has_value());
-        auto x0 = x0_.value();
-        TORCH_CHECK(x0.is_cuda());
-        TORCH_CHECK(x0.is_contiguous());
-        TORCH_CHECK(x0.sizes() == x0_sizes);
-        TORCH_CHECK(x0.dtype() == itype);
-    }
-
-    if (x0_subset_.has_value()) {
-        auto x0_subset = x0_subset_.value();
-        TORCH_CHECK(x0_subset.is_cuda());
-        TORCH_CHECK(x0_subset.is_contiguous());
-        TORCH_CHECK(x0_subset.sizes() == c10::IntArrayRef{rows});
-        TORCH_CHECK(x0_subset.dtype() == torch::kInt32);
-
-        TORCH_CHECK(z_subset_.has_value());
-        auto z_subset = z_subset_.value();
-        TORCH_CHECK(z_subset.is_cuda());
-        TORCH_CHECK(z_subset.is_contiguous());
-        TORCH_CHECK(z_subset.sizes() == c10::IntArrayRef{rows});
-        TORCH_CHECK(z_subset.dtype() == torch::kInt32);
-    }
-
-    TORCH_CHECK((hidden_size % 8 == 0) && (hidden_size <= 8192));
-
-    TORCH_CHECK(mu.numel() == rows);
-    TORCH_CHECK(mu.sizes() == rsigma.sizes());
-
-    TORCH_CHECK(gamma.numel() == cols);
-
-    // Otherwise the kernel will be launched from cuda:0 device
-    // Cast to char to avoid compiler warning about narrowing
-    at::cuda::CUDAGuard device_guard{(char)dz.get_device()};
-
-    auto opts = x.options();
-
-    auto dx0 = torch::empty(x0_sizes, opts.dtype(itype));
-    at::Tensor dresidual;
-    if (has_residual) { dresidual = torch::empty_like(x, opts.dtype(rtype)); }
-    auto dgamma = torch::empty_like(gamma);
-    auto dbeta = torch::empty_like(gamma);
-    at::Tensor dcolscale;
-    if (colscale_.has_value()) {
-        dcolscale = torch::empty_like(colscale_.value());
-    }
-
-    layer_norm::LaunchParams<layer_norm::BwdParams> launch_params;
-    launch_params.stream = at::cuda::getCurrentCUDAStream().stream();
-    launch_params.props = at::cuda::getCurrentDeviceProperties();
-    TORCH_CHECK(dropout_p < 1.f);
-    launch_params.params.dropout_keep_p = 1.f - dropout_p;
-    launch_params.params.dresidual = has_residual ? dresidual.data_ptr() : nullptr;
-    launch_params.params.rowscale = rowscale_.has_value() ? rowscale_.value().data_ptr() : nullptr;
-    launch_params.params.colscale = colscale_.has_value() ? colscale_.value().data_ptr() : nullptr;
-    launch_params.params.x0_subset = x0_subset_.has_value() ? x0_subset_.value().data_ptr() : nullptr;
-    launch_params.params.z_subset = z_subset_.has_value() ? z_subset_.value().data_ptr() : nullptr;
-
-    auto round_multiple = [](int x, int m) { return (x + m - 1) / m * m; };
-    const int multiple = hidden_size <= 1536 ? 256 : (hidden_size <= 3072 ? 512 : 1024);
-    auto launcher = get_bwd_launcher(wtype, itype, rtype, otype, ctype, round_multiple(hidden_size, multiple));
-
-    launcher(launch_params, true);
-
-    auto dgamma_part = torch::empty({ launch_params.params.ctas_per_col, hidden_size }, opts.dtype(ctype));
-    auto dbeta_part = torch::empty({ launch_params.params.ctas_per_col, hidden_size }, opts.dtype(ctype));
-    at::Tensor dcolscale_part;
-    if (colscale_.has_value()) {
-        dcolscale_part = torch::empty({ launch_params.params.ctas_per_col, hidden_size }, opts.dtype(ctype));
-    }
-    at::Tensor workspace, barrier;
-
-    layer_norm::BwdParams &params = launch_params.params;
-    params.rows = rows;
-    params.cols = cols;
-    params.x = x.data_ptr();
-    params.x0 = x0_.has_value() ? x0_.value().data_ptr() : nullptr;
-    params.dmask = dropout_p > 0.f ? dmask_.value().data_ptr() : nullptr;
-    params.mu = mu.data_ptr();
-    params.rs = rsigma.data_ptr();
-    params.gamma = gamma.data_ptr();
-    params.dz = dz.data_ptr();
-    params.dx = dx_.has_value() ? dx_.value().data_ptr() : nullptr;
-    params.dx0 = dx0.data_ptr();
-    params.dbeta = dbeta.data_ptr();
-    params.dgamma = dgamma.data_ptr();
-    params.dcolscale = colscale_.has_value() ? dcolscale.data_ptr() : nullptr;
-    params.dbeta_part = dbeta_part.data_ptr();
-    params.dgamma_part = dgamma_part.data_ptr();
-    params.dcolscale_part = colscale_.has_value() ? dcolscale_part.data_ptr() : nullptr;
-    params.dropout_scale = 1.f / (1.f - dropout_p);
-    params.inverse_cols = 1.f / float(params.cols);
-    params.rowscale_const = rowscale_const;
-    params.is_rms_norm = is_rms_norm;
-
-    if( launch_params.barrier_size > 0 ) {
-        // TODO Any way to avoid this?
-        barrier = torch::zeros(launch_params.barrier_size, opts.dtype(torch::kInt32));
-        workspace = torch::empty(launch_params.workspace_bytes, opts.dtype(torch::kChar));
-        params.workspace = workspace.data_ptr();
-        params.barrier = barrier.data_ptr<int>();
-    }
-
-    launcher(launch_params, false);
-
-    std::vector<at::Tensor> result = { dx0, dresidual, dgamma, dbeta, dgamma_part, dbeta_part };
-    if (colscale_.has_value()) {
-        result.push_back(dcolscale);
-        result.push_back(dcolscale_part);
-    }
-    return result;
-}
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-std::vector<at::Tensor> dropout_add_ln_parallel_residual_fwd(
-    const at::Tensor &x0,      // Input: BxSxhidden_size
-    c10::optional<const at::Tensor> &x1_,      // Input: BxSxhidden_size
-    c10::optional<const at::Tensor> &residual_,  // Residual: BxSxhidden_size
-    const at::Tensor &gamma0,   // hidden_size
-    c10::optional<const at::Tensor> &beta0_,   // hidden_size
-    c10::optional<const at::Tensor> &gamma1_,   // hidden_size
-    c10::optional<const at::Tensor> &beta1_,   // hidden_size
-    const float dropout_p,
-    const float epsilon,
-    c10::optional<at::Generator> gen_,
-    bool residual_in_fp32=false,
-    bool is_rms_norm=false
-) {
-    auto itype = x0.scalar_type();
-    auto rtype = residual_.has_value()
-        ? residual_.value().scalar_type()
-        : (residual_in_fp32 ? torch::kFloat32 : x0.scalar_type());
-    auto wtype = gamma0.scalar_type();
-    auto otype = itype;
-    auto ctype = torch::kFloat32;
-    auto mtype = torch::kUInt8;
-
-    TORCH_CHECK(x0.is_cuda());
-    TORCH_CHECK(gamma0.is_cuda());
-
-    TORCH_CHECK(x0.is_contiguous());
-    const auto sizes = x0.sizes();
-    TORCH_CHECK(x0.dim() == 2);
-
-    const int rows = sizes[0];
-    const int cols = sizes[1];
-    auto hidden_size = gamma0.numel();
-    TORCH_CHECK(hidden_size == cols);
-
-    if (x1_.has_value()) {
-        auto x1 = x1_.value();
-        TORCH_CHECK(x1.is_cuda());
-        TORCH_CHECK(x1.is_contiguous());
-        TORCH_CHECK(x1.sizes() == sizes);
-    }
-
-    if (residual_.has_value()) {
-        auto residual = residual_.value();
-        TORCH_CHECK(residual.is_cuda());
-        TORCH_CHECK(residual.is_contiguous());
-        TORCH_CHECK(residual.sizes() == sizes);
-    }
-
-    if (beta0_.has_value()) {
-        auto beta0 = beta0_.value();
-        TORCH_CHECK(beta0.dtype() == wtype);
-        TORCH_CHECK(beta0.is_cuda());
-        TORCH_CHECK(beta0.is_contiguous());
-        TORCH_CHECK(beta0.sizes() == gamma0.sizes());
-    }
-
-    if (gamma1_.has_value()) {
-        auto gamma1 = gamma1_.value();
-        TORCH_CHECK(gamma1.dtype() == wtype);
-        TORCH_CHECK(gamma1.is_cuda());
-        TORCH_CHECK(gamma1.is_contiguous());
-        TORCH_CHECK(gamma1.sizes() == gamma0.sizes());
-    }
-
-    if (beta1_.has_value()) {
-        auto beta1 = beta1_.value();
-        TORCH_CHECK(beta1.dtype() == wtype);
-        TORCH_CHECK(beta1.is_cuda());
-        TORCH_CHECK(beta1.is_contiguous());
-        TORCH_CHECK(beta1.sizes() == gamma0.sizes());
-    }
-
-    TORCH_CHECK((hidden_size % 8 == 0) && (hidden_size <= 8192));
-    TORCH_CHECK(epsilon >= 0.f);
-
-    // Otherwise the kernel will be launched from cuda:0 device
-    // Cast to char to avoid compiler warning about narrowing
-    at::cuda::CUDAGuard device_guard{(char)x0.get_device()};
-
-    auto opts = x0.options();
-
-    bool save_x = residual_.has_value() || x1_.has_value() || (dropout_p > 0.f) || (itype != rtype);
-    at::Tensor x;
-    if (save_x) { x = torch::empty(sizes, opts.dtype(rtype)); }
-    at::Tensor dmask0, dmask1;
-    if (dropout_p > 0.f) {
-        dmask0 = torch::empty(x0.sizes(), opts.dtype(mtype));
-        if (x1_.has_value()) { dmask1 = torch::empty(x0.sizes(), opts.dtype(mtype)); }
-    };
-    auto z0 = torch::empty(sizes, opts.dtype(otype));
-    at::Tensor z1;
-    if (gamma1_.has_value()) { z1 = torch::empty(sizes, opts.dtype(otype)); }
-
-    auto mu = torch::empty({ rows }, opts.dtype(ctype));
-    auto rsigma = torch::empty({ rows }, opts.dtype(ctype));
-
-    layer_norm::LaunchParams<layer_norm::FwdParams> launch_params;
-
-    launch_params.props = at::cuda::getCurrentDeviceProperties();
-    launch_params.stream = at::cuda::getCurrentCUDAStream().stream();
-    TORCH_CHECK(dropout_p < 1.f);
-    launch_params.params.dropout_keep_p = 1.f - dropout_p;
-    launch_params.params.residual = residual_.has_value() ? residual_.value().data_ptr() : nullptr;
-
-    auto gen = at::get_generator_or_default<at::CUDAGeneratorImpl>(
-        gen_, at::cuda::detail::getDefaultCUDAGenerator());
-
-    auto round_multiple = [](int x, int m) { return (x + m - 1) / m * m; };
-    const int multiple = hidden_size <= 1536 ? 256 : (hidden_size <= 3072 ? 512 : 1024);
-    // Request the kernel launcher.
-    auto launcher = get_parallel_fwd_launcher(wtype, itype, rtype, otype, ctype, round_multiple(hidden_size, multiple));
-
-    // Set the kernel runtime parameters.
-    layer_norm::FwdParams &params = launch_params.params;
-    params.rows = rows;
-    params.cols = cols;
-    params.x0 = x0.data_ptr();
-    params.x1 = x1_.has_value() ? x1_.value().data_ptr() : nullptr;
-    params.x = save_x ? x.data_ptr() : nullptr;
-    params.dmask = dropout_p > 0.f ? dmask0.data_ptr() : nullptr;
-    params.dmask1 = (dropout_p > 0.f && x1_.has_value()) ? dmask1.data_ptr() : nullptr;
-    params.mu = mu.data_ptr();
-    params.rs = rsigma.data_ptr();
-    params.gamma = gamma0.data_ptr();
-    params.gamma1 = gamma1_.has_value() ? gamma1_.value().data_ptr() : nullptr;
-    params.beta = beta0_.has_value() ? beta0_.value().data_ptr() : nullptr;
-    params.beta1 = beta1_.has_value() ? beta1_.value().data_ptr() : nullptr;
-    params.z = z0.data_ptr();
-    params.z1 = gamma1_.has_value() ? z1.data_ptr() : nullptr;
-    params.epsilon = epsilon;
-    params.dropout_scale = 1.f / (1.f - dropout_p);
-    params.inverse_cols = 1.f / float(params.cols);
-    params.is_rms_norm = is_rms_norm;
-
-    // Query the kernel-specific launch parameters.
-    launcher(launch_params, true);
-
-    at::Tensor workspace, barrier;
-
-    if (dropout_p > 0.f) {
-        // number of times random will be generated per thread, to offset philox counter in thc random
-        // state
-        int64_t counter_offset = 2 * launch_params.elts_per_thread;
-
-        // See Note [Acquire lock when using random generators]
-        {
-            std::lock_guard<std::mutex> lock(gen->mutex_);
-            params.philox_args = gen->philox_cuda_state(counter_offset);
-        }
-    }
-
-    if( launch_params.barrier_size > 0 ) {
-        auto options = x0.options();
-        barrier = torch::zeros(launch_params.barrier_size, options.dtype(torch::kInt32));
-        workspace = torch::empty(launch_params.workspace_bytes, options.dtype(torch::kChar));
-        params.workspace = workspace.data_ptr();
-        params.barrier = barrier.data_ptr<int>();
-    }
-
-    // Launch the kernel.
-    launcher(launch_params, false);
-
-    return { z0, z1, x, dmask0, dmask1, mu, rsigma };
-}
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-std::vector<at::Tensor> dropout_add_ln_parallel_residual_bwd(
-    const at::Tensor &dz0,     // BxSxhidden_size
-    c10::optional<const at::Tensor> &dz1_,     // BxSxhidden_size
-    c10::optional<const at::Tensor> &dx_,     // BxSxhidden_size
-    const at::Tensor &x,      // BxSxhidden_size
-    c10::optional<const at::Tensor> &dmask0_,  // BxSxhidden_size
-    c10::optional<const at::Tensor> &dmask1_,  // BxSxhidden_size
-    const at::Tensor &mu,     // BxS, FP32!
-    const at::Tensor &rsigma, // BxS, FP32!
-    const at::Tensor &gamma0,   // hidden_size
-    c10::optional<const at::Tensor> &gamma1_,   // hidden_size
-    const float dropout_p,
-    const bool has_x1,
-    const bool has_residual,
-    bool is_rms_norm=false
-) {
-
-    auto itype = dz0.scalar_type();
-    auto rtype = x.scalar_type();
-    auto wtype = gamma0.scalar_type();
-    auto otype = itype;
-    auto ctype = torch::kFloat32;
-    auto mtype = torch::kUInt8;
-
-    if (dropout_p > 0.f) { TORCH_CHECK(dmask0_.has_value()); }
-
-    TORCH_CHECK(dz0.dtype() == otype);
-    TORCH_CHECK(dz0.dtype() == otype);
-    TORCH_CHECK(mu.dtype() == ctype);
-    TORCH_CHECK(rsigma.dtype() == ctype);
-
-    TORCH_CHECK(x.is_cuda());
-    TORCH_CHECK(dz0.is_cuda());
-    TORCH_CHECK(mu.is_cuda());
-    TORCH_CHECK(rsigma.is_cuda());
-    TORCH_CHECK(gamma0.is_cuda());
-
-    TORCH_CHECK(x.is_contiguous());
-    TORCH_CHECK(dz0.is_contiguous());
-
-    auto sizes = x.sizes();
-    TORCH_CHECK(sizes.size() == 2);
-    auto rows = sizes[0];
-    auto cols = sizes[1];
-    TORCH_CHECK(dz0.dim() == 2);
-    TORCH_CHECK(dz0.size(1) == cols);
-    auto hidden_size = gamma0.numel();
-    TORCH_CHECK(hidden_size == cols);
-
-    if (dz1_.has_value()) {
-        auto dz1 = dz1_.value();
-        TORCH_CHECK(dz1.dtype() == otype);
-        TORCH_CHECK(dz1.is_cuda());
-        TORCH_CHECK(dz1.is_contiguous());
-        TORCH_CHECK(dz1.sizes() == sizes);
-
-        TORCH_CHECK(gamma1_.has_value());
-        auto gamma1 = gamma1_.value();
-        TORCH_CHECK(gamma1.dtype() == wtype);
-        TORCH_CHECK(gamma1.is_cuda());
-        TORCH_CHECK(gamma1.is_contiguous());
-        TORCH_CHECK(gamma1.sizes() == gamma0.sizes());
-    }
-
-    if (dx_.has_value()) {
-        auto dx = dx_.value();
-        TORCH_CHECK(dx.dtype() == rtype);
-        TORCH_CHECK(dx.is_cuda());
-        TORCH_CHECK(dx.is_contiguous());
-        TORCH_CHECK(dx.sizes() == sizes);
-    }
-
-    if (dmask0_.has_value()) {
-        auto dmask0 = dmask0_.value();
-        TORCH_CHECK(dmask0.dtype() == mtype);
-        TORCH_CHECK(dmask0.is_cuda());
-        TORCH_CHECK(dmask0.is_contiguous());
-        TORCH_CHECK(dmask0.sizes() == sizes);
-
-        if (has_x1) {
-            TORCH_CHECK(dmask1_.has_value());
-            auto dmask1 = dmask1_.value();
-            TORCH_CHECK(dmask1.dtype() == mtype);
-            TORCH_CHECK(dmask1.is_cuda());
-            TORCH_CHECK(dmask1.is_contiguous());
-            TORCH_CHECK(dmask1.sizes() == sizes);
-        }
-    }
-
-    TORCH_CHECK((hidden_size % 8 == 0) && (hidden_size <= 8192));
-
-    TORCH_CHECK(mu.numel() == rows);
-    TORCH_CHECK(mu.sizes() == rsigma.sizes());
-
-    // Otherwise the kernel will be launched from cuda:0 device
-    // Cast to char to avoid compiler warning about narrowing
-    at::cuda::CUDAGuard device_guard{(char)dz0.get_device()};
-
-    auto opts = x.options();
-
-    auto dx0 = torch::empty(sizes, opts.dtype(itype));
-    at::Tensor dx1;
-    if (has_x1) { dx1 = torch::empty(sizes, opts.dtype(itype)); }
-    at::Tensor dresidual;
-    if (has_residual) { dresidual = torch::empty_like(x, opts.dtype(rtype)); }
-    auto dgamma0 = torch::empty_like(gamma0);
-    auto dbeta0 = torch::empty_like(gamma0);
-    at::Tensor dgamma1, dbeta1;
-    if (gamma1_.has_value()) {
-        dgamma1 = torch::empty_like(gamma0);
-        dbeta1 = torch::empty_like(gamma0);
-    }
-
-    layer_norm::LaunchParams<layer_norm::BwdParams> launch_params;
-    launch_params.stream = at::cuda::getCurrentCUDAStream().stream();
-    launch_params.props = at::cuda::getCurrentDeviceProperties();
-    TORCH_CHECK(dropout_p < 1.f);
-    launch_params.params.dropout_keep_p = 1.f - dropout_p;
-    launch_params.params.dresidual = has_residual ? dresidual.data_ptr() : nullptr;
-
-    auto round_multiple = [](int x, int m) { return (x + m - 1) / m * m; };
-    const int multiple = hidden_size <= 1536 ? 256 : (hidden_size <= 3072 ? 512 : 1024);
-    auto launcher = get_parallel_bwd_launcher(wtype, itype, rtype, otype, ctype, round_multiple(hidden_size, multiple));
-
-    launcher(launch_params, true);
-
-    auto dgamma0_part = torch::zeros({ launch_params.params.ctas_per_col, hidden_size }, opts.dtype(ctype));
-    auto dbeta0_part = torch::zeros({ launch_params.params.ctas_per_col, hidden_size }, opts.dtype(ctype));
-    at::Tensor dgamma1_part, dbeta1_part;
-    if (gamma1_.has_value()) {
-        dgamma1_part = torch::zeros_like(dgamma0_part);
-        dbeta1_part = torch::zeros_like(dbeta0_part);
-    }
-    at::Tensor workspace, barrier;
-
-    layer_norm::BwdParams &params = launch_params.params;
-    params.rows = rows;
-    params.cols = cols;
-    params.x = x.data_ptr();
-    params.dmask = dropout_p > 0.f ? dmask0_.value().data_ptr() : nullptr;
-    params.dmask1 = (dropout_p > 0.f && has_x1) ? dmask1_.value().data_ptr() : nullptr;
-    params.mu = mu.data_ptr();
-    params.rs = rsigma.data_ptr();
-    params.gamma = gamma0.data_ptr();
-    params.gamma1 = gamma1_.has_value() ? gamma1_.value().data_ptr() : nullptr;
-    params.dz = dz0.data_ptr();
-    params.dz1 = dz1_.has_value() ? dz1_.value().data_ptr() : nullptr;
-    params.dx = dx_.has_value() ? dx_.value().data_ptr() : nullptr;
-    params.dx0 = dx0.data_ptr();
-    params.dx1 = has_x1 ? dx1.data_ptr() : nullptr;
-    params.dbeta = dbeta0.data_ptr();
-    params.dgamma = dgamma0.data_ptr();
-    params.dbeta1 = gamma1_.has_value() ? dbeta1.data_ptr() : nullptr;
-    params.dgamma1 = gamma1_.has_value() ? dgamma1.data_ptr() : nullptr;
-    params.dbeta_part = dbeta0_part.data_ptr();
-    params.dgamma_part = dgamma0_part.data_ptr();
-    params.dbeta1_part = gamma1_.has_value() ? dbeta1_part.data_ptr() : nullptr;
-    params.dgamma1_part = gamma1_.has_value() ? dgamma1_part.data_ptr() : nullptr;
-    params.dropout_scale = 1.f / (1.f - dropout_p);
-    params.inverse_cols = 1.f / float(params.cols);
-    params.is_rms_norm = is_rms_norm;
-
-    if( launch_params.barrier_size > 0 ) {
-        // TODO Any way to avoid this?
-        barrier = torch::zeros(launch_params.barrier_size, opts.dtype(torch::kInt32));
-        workspace = torch::empty(launch_params.workspace_bytes, opts.dtype(torch::kChar));
-        params.workspace = workspace.data_ptr();
-        params.barrier = barrier.data_ptr<int>();
-    }
-
-    launcher(launch_params, false);
-
-    std::vector<at::Tensor> result = { dx0, dx1, dresidual, dgamma0, dbeta0, dgamma1, dbeta1, dgamma0_part, dbeta0_part, dgamma1_part, dbeta1_part };
-    return result;
-}
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
-    m.doc() = "CUDA DropoutAddLayerNorm";
-    m.def("dropout_add_ln_fwd", &dropout_add_ln_fwd, "Run Dropout + Add + LayerNorm forward kernel",
-          py::arg("x0"), py::arg("residual"), py::arg("gamma"), py::arg("beta_"),
-          py::arg("rowscale_"), py::arg("colscale_"), py::arg("x0_subset_"), py::arg("z_subset_"),
-          py::arg("dropout_p"), py::arg("epsilon"), py::arg("rowscale_const"), py::arg("z_numrows"),
-          py::arg("gen_"), py::arg("residual_in_fp32")=false, py::arg("is_rms_norm")=false);
-    m.def("dropout_add_ln_bwd", &dropout_add_ln_bwd, "Run Dropout + Add + LayerNorm backward kernel",
-          py::arg("dz"), py::arg("dx_"), py::arg("x"), py::arg("x0_"), py::arg("dmask_"), py::arg("mu"),
-          py::arg("rsigma"), py::arg("gamma"), py::arg("rowscale_"), py::arg("colscale_"),
-          py::arg("x0_subset_"), py::arg("z_subset_"), py::arg("dropout_p"), py::arg("rowscale_const"),
-          py::arg("x0_numrows"), py::arg("has_residual"), py::arg("is_rms_norm")=false);
-    m.def("dropout_add_ln_parallel_residual_fwd", &dropout_add_ln_parallel_residual_fwd, "Run Dropout + Add + LayerNorm parallel residual forward kernel",
-          py::arg("x0"), py::arg("x1_"), py::arg("residual"), py::arg("gamma0"), py::arg("beta0_"),
-          py::arg("gamma1_"), py::arg("beta1_"), py::arg("dropout_p"), py::arg("epsilon"),
-          py::arg("gen_"), py::arg("residual_in_fp32")=false, py::arg("is_rms_norm")=false);
-    m.def("dropout_add_ln_parallel_residual_bwd", &dropout_add_ln_parallel_residual_bwd, "Run Dropout + Add + LayerNorm parallel residual backward kernel",
-          py::arg("dz0"), py::arg("dz1_"), py::arg("dx_"), py::arg("x"), py::arg("dmask0_"),
-          py::arg("dmask1_"), py::arg("mu"), py::arg("rsigma"), py::arg("gamma0"), py::arg("gamma1_"),
-          py::arg("dropout_p"), py::arg("has_x1"), py::arg("has_residual"), py::arg("is_rms_norm")=false);
-}
diff --git a/based/csrc/layer_norm/ln_bwd_1024.cu b/based/csrc/layer_norm/ln_bwd_1024.cu
deleted file mode 100644
index f7101f6..0000000
--- a/based/csrc/layer_norm/ln_bwd_1024.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_bwd_kernels.cuh"
-
-// Create backward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RTYPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG, BYTES_PER_LDG_FINAL
-
-REGISTER_BWD_LAUNCHER(  1024, fp32, fp32, fp32, fp32, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  1024, fp16, fp32, fp32, fp32, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  1024, fp32, fp16, fp32, fp16, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  1024, fp16, fp16, fp32, fp16, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  1024, fp32, fp16, fp16, fp16, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  1024, fp32, bf16, fp32, bf16, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  1024, bf16, bf16, fp32, bf16, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  1024, fp32, bf16, bf16, bf16, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  1024, fp16, fp16, fp16, fp16, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  1024, bf16, bf16, bf16, bf16, fp32, 1, 4, 1, 16, 4);
diff --git a/based/csrc/layer_norm/ln_bwd_1280.cu b/based/csrc/layer_norm/ln_bwd_1280.cu
deleted file mode 100644
index a80a576..0000000
--- a/based/csrc/layer_norm/ln_bwd_1280.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_bwd_kernels.cuh"
-
-// Create backward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RTYPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG, BYTES_PER_LDG_FINAL
-
-REGISTER_BWD_LAUNCHER(  1280, fp32, fp32, fp32, fp32, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  1280, fp16, fp32, fp32, fp32, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  1280, fp32, fp16, fp32, fp16, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  1280, fp16, fp16, fp32, fp16, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  1280, fp32, fp16, fp16, fp16, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  1280, fp32, bf16, fp32, bf16, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  1280, bf16, bf16, fp32, bf16, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  1280, fp32, bf16, bf16, bf16, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  1280, fp16, fp16, fp16, fp16, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  1280, bf16, bf16, bf16, bf16, fp32, 1, 4, 1, 16, 4);
diff --git a/based/csrc/layer_norm/ln_bwd_1536.cu b/based/csrc/layer_norm/ln_bwd_1536.cu
deleted file mode 100644
index 0c25c08..0000000
--- a/based/csrc/layer_norm/ln_bwd_1536.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_bwd_kernels.cuh"
-
-// Create backward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RTYPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG, BYTES_PER_LDG_FINAL
-
-REGISTER_BWD_LAUNCHER( 1536, fp32, fp32, fp32, fp32, fp32, 1, 1, 4, 16, 4);
-REGISTER_BWD_LAUNCHER( 1536, fp16, fp32, fp32, fp32, fp32, 1, 1, 4, 16, 4);
-REGISTER_BWD_LAUNCHER( 1536, fp32, fp16, fp32, fp16, fp32, 1, 1, 4,  8, 4);
-REGISTER_BWD_LAUNCHER( 1536, fp16, fp16, fp32, fp16, fp32, 1, 1, 4,  8, 4);
-REGISTER_BWD_LAUNCHER( 1536, fp32, fp16, fp16, fp16, fp32, 1, 1, 4,  8, 4);
-REGISTER_BWD_LAUNCHER( 1536, fp32, bf16, fp32, bf16, fp32, 1, 1, 4,  8, 4);
-REGISTER_BWD_LAUNCHER( 1536, bf16, bf16, fp32, bf16, fp32, 1, 1, 4,  8, 4);
-REGISTER_BWD_LAUNCHER( 1536, fp32, bf16, bf16, bf16, fp32, 1, 1, 4,  8, 4);
-REGISTER_BWD_LAUNCHER( 1536, fp16, fp16, fp16, fp16, fp32, 1, 1, 4,  8, 4);
-REGISTER_BWD_LAUNCHER( 1536, bf16, bf16, bf16, bf16, fp32, 1, 1, 4,  8, 4);
diff --git a/based/csrc/layer_norm/ln_bwd_2048.cu b/based/csrc/layer_norm/ln_bwd_2048.cu
deleted file mode 100644
index 06c0e60..0000000
--- a/based/csrc/layer_norm/ln_bwd_2048.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_bwd_kernels.cuh"
-
-// Create backward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RTYPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG, BYTES_PER_LDG_FINAL
-
-REGISTER_BWD_LAUNCHER( 2048, fp32, fp32, fp32, fp32, fp32, 1, 1, 4, 16, 4);
-REGISTER_BWD_LAUNCHER( 2048, fp16, fp32, fp32, fp32, fp32, 1, 1, 4, 16, 4);
-REGISTER_BWD_LAUNCHER( 2048, fp32, fp16, fp32, fp16, fp32, 1, 1, 4, 16, 4);
-REGISTER_BWD_LAUNCHER( 2048, fp16, fp16, fp32, fp16, fp32, 1, 1, 4, 16, 4);
-REGISTER_BWD_LAUNCHER( 2048, fp32, fp16, fp16, fp16, fp32, 1, 1, 4, 16, 4);
-REGISTER_BWD_LAUNCHER( 2048, fp32, bf16, fp32, bf16, fp32, 1, 1, 4, 16, 4);
-REGISTER_BWD_LAUNCHER( 2048, bf16, bf16, fp32, bf16, fp32, 1, 1, 4, 16, 4);
-REGISTER_BWD_LAUNCHER( 2048, fp32, bf16, bf16, bf16, fp32, 1, 1, 4, 16, 4);
-REGISTER_BWD_LAUNCHER( 2048, fp16, fp16, fp16, fp16, fp32, 1, 1, 4, 16, 4);
-REGISTER_BWD_LAUNCHER( 2048, bf16, bf16, bf16, bf16, fp32, 1, 1, 4, 16, 4);
\ No newline at end of file
diff --git a/based/csrc/layer_norm/ln_bwd_256.cu b/based/csrc/layer_norm/ln_bwd_256.cu
deleted file mode 100644
index 2094543..0000000
--- a/based/csrc/layer_norm/ln_bwd_256.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_bwd_kernels.cuh"
-
-// Create backward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RTYPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG, BYTES_PER_LDG_FINAL
-
-REGISTER_BWD_LAUNCHER(  256, fp32, fp32, fp32, fp32, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  256, fp16, fp32, fp32, fp32, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  256, fp32, fp16, fp32, fp16, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  256, fp16, fp16, fp32, fp16, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  256, fp32, fp16, fp16, fp16, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  256, fp32, bf16, fp32, bf16, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  256, bf16, bf16, fp32, bf16, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  256, fp32, bf16, bf16, bf16, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  256, fp16, fp16, fp16, fp16, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  256, bf16, bf16, bf16, bf16, fp32, 1, 4, 1, 16, 4);
diff --git a/based/csrc/layer_norm/ln_bwd_2560.cu b/based/csrc/layer_norm/ln_bwd_2560.cu
deleted file mode 100644
index 309184c..0000000
--- a/based/csrc/layer_norm/ln_bwd_2560.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_bwd_kernels.cuh"
-
-// Create backward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RTYPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG, BYTES_PER_LDG_FINAL
-
-REGISTER_BWD_LAUNCHER( 2560, fp32, fp32, fp32, fp32, fp32, 1, 1, 4, 16, 4);
-REGISTER_BWD_LAUNCHER( 2560, fp16, fp32, fp32, fp32, fp32, 1, 1, 4, 16, 4);
-REGISTER_BWD_LAUNCHER( 2560, fp32, fp16, fp32, fp16, fp32, 1, 1, 4,  8, 4);
-REGISTER_BWD_LAUNCHER( 2560, fp16, fp16, fp32, fp16, fp32, 1, 1, 4,  8, 4);
-REGISTER_BWD_LAUNCHER( 2560, fp32, fp16, fp16, fp16, fp32, 1, 1, 4,  8, 4);
-REGISTER_BWD_LAUNCHER( 2560, fp32, bf16, fp32, bf16, fp32, 1, 1, 4,  8, 4);
-REGISTER_BWD_LAUNCHER( 2560, bf16, bf16, fp32, bf16, fp32, 1, 1, 4,  8, 4);
-REGISTER_BWD_LAUNCHER( 2560, fp32, bf16, bf16, bf16, fp32, 1, 1, 4,  8, 4);
-REGISTER_BWD_LAUNCHER( 2560, fp16, fp16, fp16, fp16, fp32, 1, 1, 4,  8, 4);
-REGISTER_BWD_LAUNCHER( 2560, bf16, bf16, bf16, bf16, fp32, 1, 1, 4,  8, 4);
diff --git a/based/csrc/layer_norm/ln_bwd_3072.cu b/based/csrc/layer_norm/ln_bwd_3072.cu
deleted file mode 100644
index e156b11..0000000
--- a/based/csrc/layer_norm/ln_bwd_3072.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_bwd_kernels.cuh"
-
-// Create backward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RTYPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG, BYTES_PER_LDG_FINAL
-
-REGISTER_BWD_LAUNCHER( 3072, fp32, fp32, fp32, fp32, fp32, 1, 1, 4, 16, 4);
-REGISTER_BWD_LAUNCHER( 3072, fp16, fp32, fp32, fp32, fp32, 1, 1, 4, 16, 4);
-REGISTER_BWD_LAUNCHER( 3072, fp32, fp16, fp32, fp16, fp32, 1, 1, 4, 16, 4);
-REGISTER_BWD_LAUNCHER( 3072, fp16, fp16, fp32, fp16, fp32, 1, 1, 4, 16, 4);
-REGISTER_BWD_LAUNCHER( 3072, fp32, fp16, fp16, fp16, fp32, 1, 1, 4, 16, 4);
-REGISTER_BWD_LAUNCHER( 3072, fp32, bf16, fp32, bf16, fp32, 1, 1, 4, 16, 4);
-REGISTER_BWD_LAUNCHER( 3072, bf16, bf16, fp32, bf16, fp32, 1, 1, 4, 16, 4);
-REGISTER_BWD_LAUNCHER( 3072, fp32, bf16, bf16, bf16, fp32, 1, 1, 4, 16, 4);
-REGISTER_BWD_LAUNCHER( 3072, fp16, fp16, fp16, fp16, fp32, 1, 1, 4, 16, 4);
-REGISTER_BWD_LAUNCHER( 3072, bf16, bf16, bf16, bf16, fp32, 1, 1, 4, 16, 4);
\ No newline at end of file
diff --git a/based/csrc/layer_norm/ln_bwd_4096.cu b/based/csrc/layer_norm/ln_bwd_4096.cu
deleted file mode 100644
index b715b0e..0000000
--- a/based/csrc/layer_norm/ln_bwd_4096.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_bwd_kernels.cuh"
-
-// Create backward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RTYPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG, BYTES_PER_LDG_FINAL
-
-REGISTER_BWD_LAUNCHER( 4096, fp32, fp32, fp32, fp32, fp32, 1, 1, 4, 16, 4);
-REGISTER_BWD_LAUNCHER( 4096, fp16, fp32, fp32, fp32, fp32, 1, 1, 4, 16, 4);
-REGISTER_BWD_LAUNCHER( 4096, fp32, fp16, fp32, fp16, fp32, 1, 1, 4, 16, 4);
-REGISTER_BWD_LAUNCHER( 4096, fp16, fp16, fp32, fp16, fp32, 1, 1, 4, 16, 4);
-REGISTER_BWD_LAUNCHER( 4096, fp32, fp16, fp16, fp16, fp32, 1, 1, 4, 16, 4);
-REGISTER_BWD_LAUNCHER( 4096, fp32, bf16, fp32, bf16, fp32, 1, 1, 4, 16, 4);
-REGISTER_BWD_LAUNCHER( 4096, bf16, bf16, fp32, bf16, fp32, 1, 1, 4, 16, 4);
-REGISTER_BWD_LAUNCHER( 4096, fp32, bf16, bf16, bf16, fp32, 1, 1, 4, 16, 4);
-REGISTER_BWD_LAUNCHER( 4096, fp16, fp16, fp16, fp16, fp32, 1, 1, 4, 16, 4);
-REGISTER_BWD_LAUNCHER( 4096, bf16, bf16, bf16, bf16, fp32, 1, 1, 4, 16, 4);
\ No newline at end of file
diff --git a/based/csrc/layer_norm/ln_bwd_512.cu b/based/csrc/layer_norm/ln_bwd_512.cu
deleted file mode 100644
index 2b47211..0000000
--- a/based/csrc/layer_norm/ln_bwd_512.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_bwd_kernels.cuh"
-
-// Create backward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RTYPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG, BYTES_PER_LDG_FINAL
-
-REGISTER_BWD_LAUNCHER(  512, fp32, fp32, fp32, fp32, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  512, fp16, fp32, fp32, fp32, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  512, fp32, fp16, fp32, fp16, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  512, fp16, fp16, fp32, fp16, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  512, fp32, fp16, fp16, fp16, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  512, fp32, bf16, fp32, bf16, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  512, bf16, bf16, fp32, bf16, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  512, fp32, bf16, bf16, bf16, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  512, fp16, fp16, fp16, fp16, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  512, bf16, bf16, bf16, bf16, fp32, 1, 4, 1, 16, 4);
diff --git a/based/csrc/layer_norm/ln_bwd_5120.cu b/based/csrc/layer_norm/ln_bwd_5120.cu
deleted file mode 100644
index 38f3fbd..0000000
--- a/based/csrc/layer_norm/ln_bwd_5120.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_bwd_kernels.cuh"
-
-// Create backward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RTYPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG, BYTES_PER_LDG_FINAL
-
-REGISTER_BWD_LAUNCHER( 5120, fp32, fp32, fp32, fp32, fp32, 1, 1, 4, 16, 4);
-REGISTER_BWD_LAUNCHER( 5120, fp16, fp32, fp32, fp32, fp32, 1, 1, 4, 16, 4);
-REGISTER_BWD_LAUNCHER( 5120, fp32, fp16, fp32, fp16, fp32, 1, 1, 4, 16, 4);
-REGISTER_BWD_LAUNCHER( 5120, fp16, fp16, fp32, fp16, fp32, 1, 1, 4, 16, 4);
-REGISTER_BWD_LAUNCHER( 5120, fp32, fp16, fp16, fp16, fp32, 1, 1, 4, 16, 4);
-REGISTER_BWD_LAUNCHER( 5120, fp32, bf16, fp32, bf16, fp32, 1, 1, 4, 16, 4);
-REGISTER_BWD_LAUNCHER( 5120, bf16, bf16, fp32, bf16, fp32, 1, 1, 4, 16, 4);
-REGISTER_BWD_LAUNCHER( 5120, fp32, bf16, bf16, bf16, fp32, 1, 1, 4, 16, 4);
-REGISTER_BWD_LAUNCHER( 5120, fp16, fp16, fp16, fp16, fp32, 1, 1, 4, 16, 4);
-REGISTER_BWD_LAUNCHER( 5120, bf16, bf16, bf16, bf16, fp32, 1, 1, 4, 16, 4);
\ No newline at end of file
diff --git a/based/csrc/layer_norm/ln_bwd_6144.cu b/based/csrc/layer_norm/ln_bwd_6144.cu
deleted file mode 100644
index 469ed4b..0000000
--- a/based/csrc/layer_norm/ln_bwd_6144.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_bwd_kernels.cuh"
-
-// Create backward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RTYPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG, BYTES_PER_LDG_FINAL
-
-REGISTER_BWD_LAUNCHER( 6144, fp32, fp32, fp32, fp32, fp32, 1, 1, 8, 16, 4);
-REGISTER_BWD_LAUNCHER( 6144, fp16, fp32, fp32, fp32, fp32, 1, 1, 8, 16, 4);
-REGISTER_BWD_LAUNCHER( 6144, fp32, fp16, fp32, fp16, fp32, 1, 1, 8, 16, 4);
-REGISTER_BWD_LAUNCHER( 6144, fp16, fp16, fp32, fp16, fp32, 1, 1, 8, 16, 4);
-REGISTER_BWD_LAUNCHER( 6144, fp32, fp16, fp16, fp16, fp32, 1, 1, 8, 16, 4);
-REGISTER_BWD_LAUNCHER( 6144, fp32, bf16, fp32, bf16, fp32, 1, 1, 8, 16, 4);
-REGISTER_BWD_LAUNCHER( 6144, bf16, bf16, fp32, bf16, fp32, 1, 1, 8, 16, 4);
-REGISTER_BWD_LAUNCHER( 6144, fp32, bf16, bf16, bf16, fp32, 1, 1, 8, 16, 4);
-REGISTER_BWD_LAUNCHER( 6144, fp16, fp16, fp16, fp16, fp32, 1, 1, 8, 16, 4);
-REGISTER_BWD_LAUNCHER( 6144, bf16, bf16, bf16, bf16, fp32, 1, 1, 8, 16, 4);
\ No newline at end of file
diff --git a/based/csrc/layer_norm/ln_bwd_7168.cu b/based/csrc/layer_norm/ln_bwd_7168.cu
deleted file mode 100644
index 549eab1..0000000
--- a/based/csrc/layer_norm/ln_bwd_7168.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_bwd_kernels.cuh"
-
-// Create backward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RTYPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG, BYTES_PER_LDG_FINAL
-
-REGISTER_BWD_LAUNCHER( 7168, fp32, fp32, fp32, fp32, fp32, 1, 1, 8, 16, 4);
-REGISTER_BWD_LAUNCHER( 7168, fp16, fp32, fp32, fp32, fp32, 1, 1, 8, 16, 4);
-REGISTER_BWD_LAUNCHER( 7168, fp32, fp16, fp32, fp16, fp32, 1, 1, 8,  8, 4);
-REGISTER_BWD_LAUNCHER( 7168, fp16, fp16, fp32, fp16, fp32, 1, 1, 8,  8, 4);
-REGISTER_BWD_LAUNCHER( 7168, fp32, fp16, fp16, fp16, fp32, 1, 1, 8,  8, 4);
-REGISTER_BWD_LAUNCHER( 7168, fp32, bf16, fp32, bf16, fp32, 1, 1, 8,  8, 4);
-REGISTER_BWD_LAUNCHER( 7168, bf16, bf16, fp32, bf16, fp32, 1, 1, 8,  8, 4);
-REGISTER_BWD_LAUNCHER( 7168, fp32, bf16, bf16, bf16, fp32, 1, 1, 8,  8, 4);
-REGISTER_BWD_LAUNCHER( 7168, fp16, fp16, fp16, fp16, fp32, 1, 1, 8,  8, 4);
-REGISTER_BWD_LAUNCHER( 7168, bf16, bf16, bf16, bf16, fp32, 1, 1, 8,  8, 4);
\ No newline at end of file
diff --git a/based/csrc/layer_norm/ln_bwd_768.cu b/based/csrc/layer_norm/ln_bwd_768.cu
deleted file mode 100644
index 5db64d3..0000000
--- a/based/csrc/layer_norm/ln_bwd_768.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_bwd_kernels.cuh"
-
-// Create backward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RTYPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG, BYTES_PER_LDG_FINAL
-
-REGISTER_BWD_LAUNCHER(  768, fp32, fp32, fp32, fp32, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  768, fp16, fp32, fp32, fp32, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  768, fp32, fp16, fp32, fp16, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  768, fp16, fp16, fp32, fp16, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  768, fp32, fp16, fp16, fp16, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  768, fp32, bf16, fp32, bf16, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  768, bf16, bf16, fp32, bf16, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  768, fp32, bf16, bf16, bf16, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  768, fp16, fp16, fp16, fp16, fp32, 1, 4, 1, 16, 4);
-REGISTER_BWD_LAUNCHER(  768, bf16, bf16, bf16, bf16, fp32, 1, 4, 1, 16, 4);
diff --git a/based/csrc/layer_norm/ln_bwd_8192.cu b/based/csrc/layer_norm/ln_bwd_8192.cu
deleted file mode 100644
index e6514e6..0000000
--- a/based/csrc/layer_norm/ln_bwd_8192.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_bwd_kernels.cuh"
-
-// Create backward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RTYPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG, BYTES_PER_LDG_FINAL
-
-REGISTER_BWD_LAUNCHER( 8192, fp32, fp32, fp32, fp32, fp32, 1, 1, 8, 16, 4);
-REGISTER_BWD_LAUNCHER( 8192, fp16, fp32, fp32, fp32, fp32, 1, 1, 8, 16, 4);
-REGISTER_BWD_LAUNCHER( 8192, fp32, fp16, fp32, fp16, fp32, 1, 1, 8, 16, 4);
-REGISTER_BWD_LAUNCHER( 8192, fp16, fp16, fp32, fp16, fp32, 1, 1, 8, 16, 4);
-REGISTER_BWD_LAUNCHER( 8192, fp32, fp16, fp16, fp16, fp32, 1, 1, 8, 16, 4);
-REGISTER_BWD_LAUNCHER( 8192, fp32, bf16, fp32, bf16, fp32, 1, 1, 8, 16, 4);
-REGISTER_BWD_LAUNCHER( 8192, bf16, bf16, fp32, bf16, fp32, 1, 1, 8, 16, 4);
-REGISTER_BWD_LAUNCHER( 8192, fp32, bf16, bf16, bf16, fp32, 1, 1, 8, 16, 4);
-REGISTER_BWD_LAUNCHER( 8192, fp16, fp16, fp16, fp16, fp32, 1, 1, 8, 16, 4);
-REGISTER_BWD_LAUNCHER( 8192, bf16, bf16, bf16, bf16, fp32, 1, 1, 8, 16, 4);
\ No newline at end of file
diff --git a/based/csrc/layer_norm/ln_bwd_kernels.cuh b/based/csrc/layer_norm/ln_bwd_kernels.cuh
deleted file mode 100644
index c7261d2..0000000
--- a/based/csrc/layer_norm/ln_bwd_kernels.cuh
+++ /dev/null
@@ -1,534 +0,0 @@
-#pragma once
-
-#include "ln.h"
-#include "ln_utils.cuh"
-#include "ln_kernel_traits.h"
-#include "static_switch.h"
-
-namespace layer_norm {
-
-template<typename Ktraits, bool Is_dropout, bool Has_colscale, bool Has_subset, bool Is_even_cols>
-__global__ __launch_bounds__(Ktraits::THREADS_PER_CTA) 
-void ln_bwd_kernel(layer_norm::BwdParams params) {
-
-    enum { ROWS_PER_CTA = Ktraits::ROWS_PER_CTA };
-    enum { WARPS_M = Ktraits::WARPS_M };
-    enum { WARPS_N = Ktraits::WARPS_N };
-    enum { THREADS_PER_ROW = Ktraits::THREADS_PER_ROW };
-    enum { COLS = Ktraits::COLS };
-    enum { BYTES_PER_ROW = Ktraits::BYTES_PER_ROW };
-    enum { LDGS = Ktraits::LDGS };
-    enum { NUM_ELTS = Ktraits::ELTS_PER_LDG };
-    enum { THREADS_PER_WARP = Ktraits::THREADS_PER_WARP };
-    enum { CTAS_PER_ROW = Ktraits::CTAS_PER_ROW };
-
-    using input_t = typename Ktraits::input_t;
-    using compute_t = typename Ktraits::compute_t;
-    using index_t = typename Ktraits::index_t;
-    using mask_t = typename Ktraits::mask_t;
-    using Ivec = typename Ktraits::Ivec;
-    using Rvec = typename Ktraits::Rvec;
-    using Ovec = typename Ktraits::Ovec;
-    using Wvec = typename Ktraits::Wvec;
-    using Cvec = typename Ktraits::Cvec;
-    using Mvec = typename Ktraits::Mvec;
-    using Reducer = typename Ktraits::Reducer;
-    using reduce_t = typename Reducer::Type;
-
-    extern __shared__ char smem_[];
-
-    const bool has_residual = params.dresidual != nullptr;
-    const bool prenorm = params.dx != nullptr;
-
-    const index_t tidx = threadIdx.x;
-    const index_t bidn = blockIdx.x % CTAS_PER_ROW;
-    const index_t bidm = blockIdx.x / CTAS_PER_ROW;
-    const index_t lane = tidx % THREADS_PER_WARP;
-    const index_t warp = tidx / THREADS_PER_WARP;
-    const index_t warp_m = warp / Ktraits::WARPS_N;
-    const index_t warp_n = warp % Ktraits::WARPS_N;
-    const index_t tid_r = warp_n * THREADS_PER_WARP + lane;
-
-    const index_t r = bidm * Ktraits::ROWS_PER_CTA + warp_m;
-    const index_t c = bidn * THREADS_PER_ROW + warp_n * THREADS_PER_WARP + lane;
-
-    static_assert(COLS == THREADS_PER_ROW * LDGS * NUM_ELTS * CTAS_PER_ROW);
-
-    const input_t *rowscale = static_cast<input_t *>(params.rowscale);
-    const index_t *x0_subset = static_cast<index_t *>(params.x0_subset);
-    const index_t *z_subset = static_cast<index_t *>(params.z_subset);
-
-    Cvec dzy_sum[LDGS];
-    Cvec dz_sum[LDGS];
-    Cvec dcolscale_sum[LDGS];
-
-    memset(dzy_sum, 0, sizeof(dzy_sum));
-    memset(dz_sum, 0, sizeof(dz_sum));
-    if (Has_colscale) { memset(dcolscale_sum, 0, sizeof(dcolscale_sum)); }
-
-    compute_t * smem_wgrad = reinterpret_cast<compute_t*>(smem_);
-    char *smem_dgrad = smem_ + Ktraits::SMEM_BYTES_WGRAD;
-
-    Reducer reducer(params, bidm, bidn, warp_m, warp_n, lane, smem_dgrad);
-
-    Sum<reduce_t> sum;
-
-    const index_t num_valid_ldgs =
-        ((params.cols / Ktraits::ELTS_PER_LDG) - 1 - c + Ktraits::VEC_COLS_PER_LDG) / Ktraits::VEC_COLS_PER_LDG;
-
-    Wvec gamma[LDGS];
-    Wvec colscale[LDGS];
-    index_t idx = c;
-    #pragma unroll
-    for( int it = 0; it < LDGS; it++ ) {
-        if (Is_even_cols || (it < num_valid_ldgs)) {
-            gamma[it].load_from(params.gamma, idx);
-            if (Has_colscale) { colscale[it].load_from(params.colscale, idx); }
-            idx += Ktraits::VEC_COLS_PER_LDG;
-        }
-    }
-    // TODO if ROWS_PER_CTA does not divide rows, we might get divergence in the
-    // last blocks with syncthreads!
-    // grid stride over rows
-    #pragma unroll 1
-    for( int row = r; row < params.rows; row += params.ctas_per_col * ROWS_PER_CTA ) {
-        const compute_t mu_r = static_cast<const compute_t *>(params.mu)[row];
-        const compute_t rs_r = static_cast<const compute_t *>(params.rs)[row];
-        const compute_t rowscale_val = !Has_subset ? (params.rowscale == nullptr ? 1.0f : compute_t(rowscale[row])) : params.rowscale_const;
-        const int row_z = !Has_subset ? row + 1 : z_subset[row];
-        const int row_x0 = !Has_subset ? row + 1 : x0_subset[row];
-        const bool load_dz = !Has_subset || row_z > 0;
-        const bool save_dx0 = !Has_subset || row_x0 > 0;
-        Mvec dmask[LDGS];
-        Rvec dx[LDGS];
-        compute_t dy[LDGS * NUM_ELTS];
-        compute_t y[LDGS * NUM_ELTS];
-        compute_t mdy_local = 0.f;
-        compute_t mdyy_local = 0.f;
-        // If dz is not loaded, then dy should be 0 and we don't care about the value of y.
-        if (load_dz) {
-            index_t idx_x = row * params.cols / Ktraits::ELTS_PER_LDG + c;
-            index_t idx_z = !Has_subset ? idx_x : (load_dz ? (row_z - 1) * params.cols / Ktraits::ELTS_PER_LDG + c : 0);
-            index_t idx_x0 = !Has_subset ? idx_x : (save_dx0 ? (row_x0 - 1) * params.cols / Ktraits::ELTS_PER_LDG + c : 0);
-            #pragma unroll
-            for( int it = 0; it < LDGS; it++ ) {
-                if (Is_even_cols || (it < num_valid_ldgs)) {
-                    Rvec x;
-                    Ovec dz;
-                    dz.load_from(params.dz, !Has_subset ? idx_x : idx_z);
-                    if (prenorm) { dx[it].load_from(params.dx, idx_x); }
-                    x.load_from(params.x, idx_x);
-                    if (Is_dropout) { dmask[it].load_from(params.dmask, !Has_subset ? idx_x : idx_x0); }
-                    idx_x += Ktraits::VEC_COLS_PER_LDG;
-                    idx_z += Ktraits::VEC_COLS_PER_LDG;
-                    idx_x0 += Ktraits::VEC_COLS_PER_LDG;
-                    #pragma unroll
-                    for( int jt = 0; jt < NUM_ELTS; jt++ ) {
-                        compute_t x_tmp = x.data.elt[jt];
-                        compute_t y_tmp = rs_r * (x_tmp - (!params.is_rms_norm ? mu_r : 0.f));
-                        compute_t dy_tmp = compute_t(gamma[it].data.elt[jt]) * compute_t(dz.data.elt[jt]);
-                        compute_t dz_tmp = dz.data.elt[jt];
-
-                        mdy_local += dy_tmp;
-                        mdyy_local += dy_tmp * y_tmp;
-
-                        dy[it * NUM_ELTS + jt] = dy_tmp;
-                        y[it * NUM_ELTS + jt] = y_tmp;
-
-                        dzy_sum[it].data.elt[jt] += dz_tmp * y_tmp;
-                        dz_sum[it].data.elt[jt] += dz_tmp;
-                    }
-                }
-            }
-        } else {
-            index_t idx_x = row * params.cols / Ktraits::ELTS_PER_LDG + c;
-            index_t idx_x0 = !Has_subset ? idx_x : (save_dx0 ? (row_x0 - 1) * params.cols / Ktraits::ELTS_PER_LDG + c : 0);
-            #pragma unroll
-            for( int it = 0; it < LDGS; it++ ) {
-                if (Is_even_cols || (it < num_valid_ldgs)) {
-                    if (prenorm) { dx[it].load_from(params.dx, idx_x); }
-                    if (Is_dropout) { dmask[it].load_from(params.dmask, !Has_subset ? idx_x : idx_x0); }
-                    idx_x += Ktraits::VEC_COLS_PER_LDG;
-                    idx_x0 += Ktraits::VEC_COLS_PER_LDG;
-                }
-            }
-        }
-
-        reduce_t result = reducer.allreduce({mdy_local, mdyy_local}, sum);
-        mdy_local = layer_norm::Get<0>::of<reduce_t, compute_t>(result) * params.inverse_cols;
-        mdyy_local = layer_norm::Get<1>::of<reduce_t, compute_t>(result) * params.inverse_cols;
-
-        index_t idx_x = row * params.cols / Ktraits::ELTS_PER_LDG + c;
-        index_t idx_x0 = !Has_subset ? idx_x : (save_dx0 ? (row_x0 - 1) * params.cols / Ktraits::ELTS_PER_LDG + c : 0);
-        #pragma unroll
-        for( int it = 0; it < LDGS; it++ ) {
-            if (Is_even_cols || (it < num_valid_ldgs)) {
-                Ivec dx0;
-                Rvec dresidual;
-                Ivec x0;
-                if (Has_colscale && save_dx0) { x0.load_from(params.x0, !Has_subset ? idx_x : idx_x0); }
-                #pragma unroll
-                for( int jt = 0; jt < NUM_ELTS; jt++ ) {
-                    compute_t dx_tmp_res;
-                    if (load_dz) {
-                        compute_t dy_tmp = dy[it * NUM_ELTS + jt];
-                        compute_t y_tmp = y[it * NUM_ELTS + jt];
-                        compute_t dx_tmp = rs_r * (dy_tmp - (mdyy_local * y_tmp + (!params.is_rms_norm ? mdy_local : 0.f)));
-                        dx_tmp_res = prenorm ? dx_tmp + compute_t(dx[it].data.elt[jt]) : dx_tmp;
-                    } else {
-                        dx_tmp_res = prenorm ? compute_t(dx[it].data.elt[jt]) : 0.f;
-                    }
-                    if (has_residual) { dresidual.data.elt[jt] = dx_tmp_res; }
-                    if (save_dx0) {
-                        compute_t dx0_tmp_res = dx_tmp_res * rowscale_val;
-                        if (Is_dropout) {
-                            dx0_tmp_res *= params.dropout_scale;
-                            if (Has_colscale) {
-                                dcolscale_sum[it].data.elt[jt] += dmask[it].data.elt[jt] ? dx0_tmp_res * compute_t(x0.data.elt[jt]) : 0.f;
-                                dx0.data.elt[jt] = dmask[it].data.elt[jt] ? dx0_tmp_res * compute_t(colscale[it].data.elt[jt]) : 0.f;
-                            } else {
-                                dx0.data.elt[jt] = dmask[it].data.elt[jt] ? dx0_tmp_res : 0.f;
-                            }
-                        } else {
-                            if (Has_colscale) {
-                                dcolscale_sum[it].data.elt[jt] += dx0_tmp_res * compute_t(x0.data.elt[jt]);
-                                dx0.data.elt[jt] = dx0_tmp_res * compute_t(colscale[it].data.elt[jt]);
-                            } else {
-                                dx0.data.elt[jt] = dx0_tmp_res;
-                            }
-                        }
-                    }
-                }
-                if (has_residual) { dresidual.store_to(params.dresidual, idx_x); }
-                if (save_dx0) { dx0.store_to(params.dx0, !Has_subset ? idx_x : idx_x0); }
-                idx_x += Ktraits::VEC_COLS_PER_LDG;
-                idx_x0 += Ktraits::VEC_COLS_PER_LDG;
-            }
-        }
-
-    }  // end: grid stride loop
-
-    if( WARPS_M == 1 ) {
-        idx = r * params.cols / Ktraits::ELTS_PER_LDG + c;
-        #pragma unroll
-        for( int it = 0; it < LDGS; it++ ) {
-            if (Is_even_cols || (it < num_valid_ldgs)) {
-                dz_sum[it].store_to(params.dbeta_part, idx);
-                dzy_sum[it].store_to(params.dgamma_part, idx);
-                if (Has_colscale) { dcolscale_sum[it].store_to(params.dcolscale_part, idx); }
-                idx += Ktraits::VEC_COLS_PER_LDG;
-            }
-        }
-    } else {
-        static_assert(WARPS_M == 1 || Ktraits::CTAS_PER_ROW == 1, "Multiple rows per CTA not supported for Multi-CTA.");
-        // Finalize reduction of part dgamma and dbeta for this CTA
-        // by reducing over the rows held across the WARPS_M warps
-
-        // Assumption: blockSize divides hidden size.
-        enum { NUM_RES = COLS / Ktraits::THREADS_PER_CTA };
-        static_assert(NUM_RES * Ktraits::THREADS_PER_CTA == COLS, "");
-
-        idx = warp_m * Ktraits::VEC_COLS + tid_r;
-        #pragma unroll
-        for( int it = 0; it < LDGS; it++ ) {
-            dz_sum[it].store_to(smem_wgrad, idx);
-            idx += THREADS_PER_ROW;
-        }
-        __syncthreads();
-        compute_t cta_dz_sum[NUM_RES];
-        memset(cta_dz_sum, 0, sizeof(compute_t) * NUM_RES);
-        for( int it = 0; it < ROWS_PER_CTA; it++ ) {
-            for( int jt = 0; jt < NUM_RES; jt++ ) {
-                cta_dz_sum[jt] += smem_wgrad[it * COLS + tidx + jt * Ktraits::THREADS_PER_CTA];
-            }
-        }
-        __syncthreads();
-
-        idx = warp_m * Ktraits::VEC_COLS + tid_r;
-        #pragma unroll
-        for( int it = 0; it < LDGS; it++ ) {
-            dzy_sum[it].store_to(smem_wgrad, idx);
-            idx += THREADS_PER_ROW;
-        }
-        __syncthreads();
-        compute_t cta_dzy_sum[NUM_RES];
-        memset(cta_dzy_sum, 0, sizeof(compute_t) * NUM_RES);
-        for( int it = 0; it < ROWS_PER_CTA; it++ ) {
-            for( int jt = 0; jt < NUM_RES; jt++ ) {
-                cta_dzy_sum[jt] += smem_wgrad[it * COLS + tidx + jt * Ktraits::THREADS_PER_CTA];
-            }
-        }
-
-        compute_t cta_dcolscale_sum[NUM_RES];
-        if (Has_colscale) {
-            __syncthreads();
-            idx = warp_m * Ktraits::VEC_COLS + tid_r;
-            #pragma unroll
-            for( int it = 0; it < LDGS; it++ ) {
-                dcolscale_sum[it].store_to(smem_wgrad, idx);
-                idx += THREADS_PER_ROW;
-            }
-            __syncthreads();
-            memset(cta_dcolscale_sum, 0, sizeof(compute_t) * NUM_RES);
-            for( int it = 0; it < ROWS_PER_CTA; it++ ) {
-                for( int jt = 0; jt < NUM_RES; jt++ ) {
-                    cta_dcolscale_sum[jt] += smem_wgrad[it * COLS + tidx + jt * Ktraits::THREADS_PER_CTA];
-                }
-            }
-        }
-
-        const index_t num_valid_writes
-            = (params.cols - 1 - tidx + Ktraits::THREADS_PER_CTA) / Ktraits::THREADS_PER_CTA;
-        compute_t *dgamma_part = static_cast<compute_t *>(params.dgamma_part) + bidm * params.cols + tidx;
-        compute_t *dbeta_part = static_cast<compute_t *>(params.dbeta_part) + bidm * params.cols + tidx;
-        compute_t *dcolscale_part = Has_colscale ? static_cast<compute_t *>(params.dcolscale_part) + bidm * params.cols + tidx : nullptr;
-        for( int jt = 0; jt < NUM_RES; jt++ ) {
-            if (Is_even_cols || (jt < num_valid_writes)) {
-                *dgamma_part = cta_dzy_sum[jt];
-                dgamma_part += Ktraits::THREADS_PER_CTA;
-                *dbeta_part = cta_dz_sum[jt];
-                dbeta_part += Ktraits::THREADS_PER_CTA;
-                if (Has_colscale) {
-                    *dcolscale_part = cta_dcolscale_sum[jt];
-                    dcolscale_part += Ktraits::THREADS_PER_CTA;
-                }
-            }
-        }
-
-    }
-}
-
-template<typename Kernel_traits, bool Has_colscale, bool Is_even_cols>
-__global__ __launch_bounds__(Kernel_traits::THREADS_PER_CTA)
-void ln_bwd_finalize_kernel(BwdParams params)
-{
-
-    using compute_t = typename Kernel_traits::compute_t;
-    using weight_t = typename Kernel_traits::weight_t;
-    using index_t = typename Kernel_traits::index_t;
-    using Reducer = typename Kernel_traits::Reducer;
-    using reduce_t = typename Reducer::Type;
-
-    Sum<reduce_t> sum;
-    enum { NUM_ELT = Kernel_traits::ELTS_PER_LDG };
-    enum { THREADS_PER_WARP = Kernel_traits::THREADS_PER_WARP };
-
-    __shared__ char smem_[Kernel_traits::SMEM_BYTES_PER_CTA];
-
-    constexpr uint32_t bidm = 0;
-
-    const uint32_t bidn = blockIdx.x;
-    const uint32_t tidx = threadIdx.x;
-    const uint32_t warp = tidx / THREADS_PER_WARP;
-    const uint32_t lane = tidx % THREADS_PER_WARP;
-
-    Reducer reducer(params, bidm, bidn, 0, 0, lane, smem_);
-
-    const uint32_t c = bidn * THREADS_PER_WARP + lane;
-    const uint32_t c_out = bidn * THREADS_PER_WARP / 2 + lane;
-    constexpr uint32_t COL_STRIDE = Kernel_traits::CTAS * THREADS_PER_WARP;
-    for( uint32_t col = c, col_out = c_out; col < Kernel_traits::COLS; col += COL_STRIDE, col_out += COL_STRIDE / 2 ) {
-        // Each thread sums over NUM_ELT columns.
-        Vec<compute_t, NUM_ELT> dbeta_local, dgamma_local, dcolscale_local;
-        memset(&dgamma_local, 0, sizeof(dgamma_local));
-        memset(&dbeta_local, 0, sizeof(dbeta_local));
-        if (Has_colscale) { memset(&dcolscale_local, 0, sizeof(dcolscale_local)); }
-        if (Is_even_cols || col < params.cols) {
-            for( uint32_t row = warp; row < params.ctas_per_col; row += Kernel_traits::ROWS_PER_CTA ) {
-                index_t idx = row * params.cols + col;
-
-                Vec<compute_t, NUM_ELT> dbeta_part, dgamma_part, dcolscale_part;
-                dbeta_part.load_from(params.dbeta_part, idx);
-                dgamma_part.load_from(params.dgamma_part, idx);
-                if (Has_colscale) { dcolscale_part.load_from(params.dcolscale_part, idx); }
-                #pragma unroll
-                for( int it = 0; it < NUM_ELT; it++ ) {
-                    dgamma_local.data.elt[it] += dgamma_part.data.elt[it];
-                    dbeta_local.data.elt[it] += dbeta_part.data.elt[it];
-                    if (Has_colscale) { dcolscale_local.data.elt[it] += dcolscale_part.data.elt[it]; }
-                }
-            }
-        }
-        void * smem_gamma = smem_;
-        void * smem_beta = &smem_[Kernel_traits::SMEM_BYTES_TRANSPOSE];
-        void * smem_colscale = &smem_[2 * Kernel_traits::SMEM_BYTES_TRANSPOSE];
-
-        const int write_row = warp;
-        const int write_col = lane ^ write_row;
-        const int write_idx = write_row * THREADS_PER_WARP + write_col;
-
-        dgamma_local.store_to(smem_gamma, write_idx);
-        dbeta_local.store_to(smem_beta, write_idx);
-        if (Has_colscale) { dcolscale_local.store_to(smem_colscale, write_idx); }
-
-        __syncthreads();
-
-        // It would be probably safe to reuse the first row of smem_beta and smem_gamma
-        void * smem_gamma_out = &smem_[Kernel_traits::NUM_FACTORS * Kernel_traits::SMEM_BYTES_TRANSPOSE];
-        void * smem_beta_out = &smem_[Kernel_traits::NUM_FACTORS * Kernel_traits::SMEM_BYTES_TRANSPOSE + Kernel_traits::SMEM_BYTES_OUTPUT];
-        void * smem_colscale_out = &smem_[Kernel_traits::NUM_FACTORS * Kernel_traits::SMEM_BYTES_TRANSPOSE + 2 * Kernel_traits::SMEM_BYTES_OUTPUT];
-
-
-        // More than one iter iff ROWS_PER_CTA < 32.
-        for( int w = warp; w < THREADS_PER_WARP; w += Kernel_traits::ROWS_PER_CTA ) {
-            const int read_row = lane;
-            const int read_col = w ^ read_row;
-            const int read_idx = read_row * THREADS_PER_WARP + read_col;
-
-            memset(&dbeta_local, 0, sizeof(dbeta_local));
-            memset(&dgamma_local, 0, sizeof(dgamma_local));
-            if (Has_colscale) { memset(&dcolscale_local, 0, sizeof(dcolscale_local)); }
-
-            // Load beta and gamma transposed 
-            if(read_row < Kernel_traits::ROWS_PER_CTA){
-                dbeta_local.load_from(smem_beta, read_idx);
-                dgamma_local.load_from(smem_gamma, read_idx);
-                if (Has_colscale) { dcolscale_local.load_from(smem_colscale, read_idx); }
-            }
-
-            // Call reducer on the loaded value(s) and convert.
-            #pragma unroll
-            for( int it = 0; it < NUM_ELT; it++ ) {
-                compute_t b_i = dbeta_local.data.elt[it];
-                compute_t g_i = dgamma_local.data.elt[it];
-                b_i = reducer.allreduce(b_i, sum);
-                g_i = reducer.allreduce(g_i, sum);
-
-                dgamma_local.data.elt[it] = g_i;
-                dbeta_local.data.elt[it] = b_i;
-                if (Has_colscale) {
-                    compute_t cs_i = dcolscale_local.data.elt[it];
-                    cs_i = reducer.allreduce(cs_i, sum);
-                    dcolscale_local.data.elt[it] = cs_i;
-                }
-            }
-
-            // Leader stores the result at the current column.
-            if(lane == 0){
-                dgamma_local.store_to(smem_gamma_out, w);
-                dbeta_local.store_to(smem_beta_out, w);
-                if (Has_colscale) { dcolscale_local.store_to(smem_colscale_out, w); }
-            }
-
-        }
-
-        // All writes done.
-        __syncthreads();
-
-        // Pack and store: 2-wide stores with half the threads.
-        if (Is_even_cols || col_out * 2 < params.cols) {
-            if( warp == Kernel_traits::ROWS_PER_CTA - 1 && lane < THREADS_PER_WARP / 2 ) {
-
-                using src_t = typename TypeToVec2<compute_t>::Type;
-                using dst_t = typename TypeToVec2<weight_t>::Type;
-                Vec<src_t, NUM_ELT> dbeta_vec2, dgamma_vec2, dcolscale_vec2;
-                Vec<dst_t, NUM_ELT> dbeta_out2, dgamma_out2, dcolscale_out2;
-
-                dgamma_vec2.load_from(smem_gamma_out, lane);
-                dbeta_vec2.load_from(smem_beta_out, lane);
-                if (Has_colscale) { dcolscale_vec2.load_from(smem_colscale_out, lane); }
-                #pragma unroll
-                for( int it = 0; it < NUM_ELT; it++ ) {
-                    dgamma_out2.data.elt[it] = Converter<src_t,dst_t>::convert(dgamma_vec2.data.elt[it]);
-                    dbeta_out2.data.elt[it] = Converter<src_t,dst_t>::convert(dbeta_vec2.data.elt[it]);
-                    if (Has_colscale) { dcolscale_out2.data.elt[it] = Converter<src_t,dst_t>::convert(dcolscale_vec2.data.elt[it]); }
-                }
-                dgamma_out2.store_to(params.dgamma, col_out);
-                dbeta_out2.store_to(params.dbeta, col_out);
-                if (Has_colscale) { dcolscale_out2.store_to(params.dcolscale, col_out); }
-            }
-        }
-    }
-}
-}  // namespace layer_norm
-
-using namespace layer_norm;
-
-template<
-    typename weight_t,
-    typename input_t,
-    typename residual_t,
-    typename output_t,
-    typename compute_t,
-    typename index_t,
-    int HIDDEN_SIZE,
-    int CTAS_PER_ROW,
-    int WARPS_M,
-    int WARPS_N,
-    int BYTES_PER_LDG_MAIN,
-    int BYTES_PER_LDG_FINAL
->
-void launch_(LaunchParams<BwdParams> &launch_params, const bool configure_params){
-
-    using Kernel_traits = Kernel_traits<weight_t,
-                                        input_t,
-                                        residual_t,
-                                        output_t,
-                                        compute_t,
-                                        index_t,
-                                        HIDDEN_SIZE,
-                                        CTAS_PER_ROW,
-                                        WARPS_M,
-                                        WARPS_N,
-                                        BYTES_PER_LDG_MAIN
-                                        >;
-    bool is_dropout = launch_params.params.dropout_keep_p < 1.f;
-    bool has_colscale = launch_params.params.colscale != nullptr;
-    bool has_subset = launch_params.params.x0_subset != nullptr;
-    bool is_even_cols = launch_params.params.cols == HIDDEN_SIZE;
-    BOOL_SWITCH(is_dropout, IsDropoutConst, [&] {
-        BOOL_SWITCH(has_colscale, HasColscaleConst, [&] {
-            BOOL_SWITCH(has_subset, HasSubsetConst, [&] {
-                BOOL_SWITCH(is_even_cols, IsEvenColsConst, [&] {
-                    auto kernel = &ln_bwd_kernel<Kernel_traits, IsDropoutConst, HasColscaleConst, HasSubsetConst, IsEvenColsConst>;
-                    if( configure_params ) {
-                        int ctas_per_sm;
-                        CHECK_CUDA(cudaOccupancyMaxActiveBlocksPerMultiprocessor(
-                            &ctas_per_sm, kernel, Kernel_traits::THREADS_PER_CTA, Kernel_traits::SMEM_BYTES));
-                        launch_params.params.ctas_per_col = launch_params.props->multiProcessorCount * ctas_per_sm / Kernel_traits::CTAS_PER_ROW;
-                        launch_params.barrier_size = 0;
-                        launch_params.workspace_bytes = 0;
-                        if(Kernel_traits::CTAS_PER_ROW > 1) {
-                            launch_params.barrier_size = 2 * launch_params.params.ctas_per_col;
-                            launch_params.workspace_bytes = launch_params.params.ctas_per_col
-                                                          * Kernel_traits::WARPS_M
-                                                          * Kernel_traits::CTAS_PER_ROW
-                                                          * sizeof(typename Kernel_traits::reduce_t)
-                                                          * 2;
-                        }
-                        return;
-                    }
-
-                    if( Kernel_traits::SMEM_BYTES >= 48 * 1024 ) {
-                        CHECK_CUDA(cudaFuncSetAttribute(kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, Kernel_traits::SMEM_BYTES));
-                    }
-                    auto stream = launch_params.stream;
-                    auto ctas_per_col = launch_params.params.ctas_per_col;
-
-                    if( Kernel_traits::CTAS_PER_ROW == 1 ) {
-                        kernel<<<ctas_per_col, Kernel_traits::THREADS_PER_CTA, Kernel_traits::SMEM_BYTES, stream>>>(launch_params.params);
-                    } else {
-                        dim3 grid(Kernel_traits::CTAS_PER_ROW * ctas_per_col);
-                        dim3 block(Kernel_traits::THREADS_PER_CTA);
-                        void *params_ = (void *)&launch_params.params;
-                        cudaLaunchCooperativeKernel((void *)kernel, grid, block, (void **)&params_, Kernel_traits::SMEM_BYTES, stream);
-                    }
-
-                    using Kernel_traits_f = layer_norm::Kernel_traits_finalize<HIDDEN_SIZE,
-                                                                              weight_t,
-                                                                              input_t,
-                                                                              residual_t,
-                                                                              output_t,
-                                                                              compute_t,
-                                                                              index_t,
-                                                                              HasColscaleConst,
-                                                                              32 * 32,  // THREADS_PER_CTA
-                                                                              BYTES_PER_LDG_FINAL>;
-
-                    auto kernel_f = &layer_norm::ln_bwd_finalize_kernel<Kernel_traits_f, HasColscaleConst, IsEvenColsConst>;
-                    kernel_f<<<Kernel_traits_f::CTAS, Kernel_traits_f::THREADS_PER_CTA, 0, stream>>>(launch_params.params);
-                });
-            });
-        });
-    });
-}
diff --git a/based/csrc/layer_norm/ln_fwd_1024.cu b/based/csrc/layer_norm/ln_fwd_1024.cu
deleted file mode 100644
index 824d86e..0000000
--- a/based/csrc/layer_norm/ln_fwd_1024.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_fwd_kernels.cuh"
-
-// Create forward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RYTPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG
-
-REGISTER_FWD_LAUNCHER( 1024, fp32, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 1024, fp16, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 1024, fp32, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 1024, fp16, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 1024, fp32, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 1024, fp32, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 1024, bf16, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 1024, fp32, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 1024, fp16, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 1024, bf16, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
diff --git a/based/csrc/layer_norm/ln_fwd_1280.cu b/based/csrc/layer_norm/ln_fwd_1280.cu
deleted file mode 100644
index 1ff58cb..0000000
--- a/based/csrc/layer_norm/ln_fwd_1280.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_fwd_kernels.cuh"
-
-// Create forward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RYTPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG
-
-REGISTER_FWD_LAUNCHER( 1280, fp32, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 1280, fp16, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 1280, fp32, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 1280, fp16, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 1280, fp32, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 1280, fp32, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 1280, bf16, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 1280, fp32, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 1280, fp16, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 1280, bf16, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
diff --git a/based/csrc/layer_norm/ln_fwd_1536.cu b/based/csrc/layer_norm/ln_fwd_1536.cu
deleted file mode 100644
index a8e19d4..0000000
--- a/based/csrc/layer_norm/ln_fwd_1536.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_fwd_kernels.cuh"
-
-// Create forward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RYTPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG
-
-REGISTER_FWD_LAUNCHER( 1536, fp32, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 1536, fp16, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 1536, fp32, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 1536, fp16, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 1536, fp32, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 1536, fp32, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 1536, bf16, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 1536, fp32, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 1536, fp16, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 1536, bf16, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
diff --git a/based/csrc/layer_norm/ln_fwd_2048.cu b/based/csrc/layer_norm/ln_fwd_2048.cu
deleted file mode 100644
index 6f9794c..0000000
--- a/based/csrc/layer_norm/ln_fwd_2048.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_fwd_kernels.cuh"
-
-// Create forward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RYTPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG
-
-REGISTER_FWD_LAUNCHER( 2048, fp32, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 2048, fp16, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 2048, fp32, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 2048, fp16, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 2048, fp32, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 2048, fp32, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 2048, bf16, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 2048, fp32, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 2048, fp16, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 2048, bf16, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
diff --git a/based/csrc/layer_norm/ln_fwd_256.cu b/based/csrc/layer_norm/ln_fwd_256.cu
deleted file mode 100644
index f3a541c..0000000
--- a/based/csrc/layer_norm/ln_fwd_256.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_fwd_kernels.cuh"
-
-// Create forward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RYTPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG
-
-REGISTER_FWD_LAUNCHER(  256, fp32, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER(  256, fp16, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER(  256, fp32, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER(  256, fp16, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER(  256, fp32, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER(  256, fp32, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER(  256, bf16, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER(  256, fp32, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER(  256, fp16, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER(  256, bf16, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
diff --git a/based/csrc/layer_norm/ln_fwd_2560.cu b/based/csrc/layer_norm/ln_fwd_2560.cu
deleted file mode 100644
index 1650671..0000000
--- a/based/csrc/layer_norm/ln_fwd_2560.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_fwd_kernels.cuh"
-
-// Create forward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RYTPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG
-
-REGISTER_FWD_LAUNCHER( 2560, fp32, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 2560, fp16, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 2560, fp32, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 2560, fp16, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 2560, fp32, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 2560, fp32, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 2560, bf16, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 2560, fp32, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 2560, fp16, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER( 2560, bf16, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
diff --git a/based/csrc/layer_norm/ln_fwd_3072.cu b/based/csrc/layer_norm/ln_fwd_3072.cu
deleted file mode 100644
index 25bb869..0000000
--- a/based/csrc/layer_norm/ln_fwd_3072.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_fwd_kernels.cuh"
-
-// Create forward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RYTPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG
-
-REGISTER_FWD_LAUNCHER( 3072, fp32, fp32, fp32, fp32, fp32, 1, 1, 4, 16);
-REGISTER_FWD_LAUNCHER( 3072, fp16, fp32, fp32, fp32, fp32, 1, 1, 4, 16);
-REGISTER_FWD_LAUNCHER( 3072, fp32, fp16, fp32, fp16, fp32, 1, 1, 4, 16);
-REGISTER_FWD_LAUNCHER( 3072, fp16, fp16, fp32, fp16, fp32, 1, 1, 4, 16);
-REGISTER_FWD_LAUNCHER( 3072, fp32, fp16, fp16, fp16, fp32, 1, 1, 4, 16);
-REGISTER_FWD_LAUNCHER( 3072, fp32, bf16, fp32, bf16, fp32, 1, 1, 4, 16);
-REGISTER_FWD_LAUNCHER( 3072, bf16, bf16, fp32, bf16, fp32, 1, 1, 4, 16);
-REGISTER_FWD_LAUNCHER( 3072, fp32, bf16, bf16, bf16, fp32, 1, 1, 4, 16);
-REGISTER_FWD_LAUNCHER( 3072, fp16, fp16, fp16, fp16, fp32, 1, 1, 4, 16);
-REGISTER_FWD_LAUNCHER( 3072, bf16, bf16, bf16, bf16, fp32, 1, 1, 4, 16);
diff --git a/based/csrc/layer_norm/ln_fwd_4096.cu b/based/csrc/layer_norm/ln_fwd_4096.cu
deleted file mode 100644
index b2bffb5..0000000
--- a/based/csrc/layer_norm/ln_fwd_4096.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_fwd_kernels.cuh"
-
-// Create forward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RYTPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG
-
-REGISTER_FWD_LAUNCHER( 4096, fp32, fp32, fp32, fp32, fp32, 1, 1, 4, 16);
-REGISTER_FWD_LAUNCHER( 4096, fp16, fp32, fp32, fp32, fp32, 1, 1, 4, 16);
-REGISTER_FWD_LAUNCHER( 4096, fp32, fp16, fp32, fp16, fp32, 1, 1, 4, 16);
-REGISTER_FWD_LAUNCHER( 4096, fp16, fp16, fp32, fp16, fp32, 1, 1, 4, 16);
-REGISTER_FWD_LAUNCHER( 4096, fp32, fp16, fp16, fp16, fp32, 1, 1, 4, 16);
-REGISTER_FWD_LAUNCHER( 4096, fp32, bf16, fp32, bf16, fp32, 1, 1, 4, 16);
-REGISTER_FWD_LAUNCHER( 4096, bf16, bf16, fp32, bf16, fp32, 1, 1, 4, 16);
-REGISTER_FWD_LAUNCHER( 4096, fp32, bf16, bf16, bf16, fp32, 1, 1, 4, 16);
-REGISTER_FWD_LAUNCHER( 4096, fp16, fp16, fp16, fp16, fp32, 1, 1, 4, 16);
-REGISTER_FWD_LAUNCHER( 4096, bf16, bf16, bf16, bf16, fp32, 1, 1, 4, 16);
diff --git a/based/csrc/layer_norm/ln_fwd_512.cu b/based/csrc/layer_norm/ln_fwd_512.cu
deleted file mode 100644
index a08fe34..0000000
--- a/based/csrc/layer_norm/ln_fwd_512.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_fwd_kernels.cuh"
-
-// Create forward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RYTPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG
-
-REGISTER_FWD_LAUNCHER(  512, fp32, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER(  512, fp16, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER(  512, fp32, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER(  512, fp16, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER(  512, fp32, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER(  512, fp32, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER(  512, bf16, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER(  512, fp32, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER(  512, fp16, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER(  512, bf16, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
diff --git a/based/csrc/layer_norm/ln_fwd_5120.cu b/based/csrc/layer_norm/ln_fwd_5120.cu
deleted file mode 100644
index bebbd69..0000000
--- a/based/csrc/layer_norm/ln_fwd_5120.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_fwd_kernels.cuh"
-
-// Create forward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RYTPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG
-
-REGISTER_FWD_LAUNCHER( 5120, fp32, fp32, fp32, fp32, fp32, 1, 1, 4, 16);
-REGISTER_FWD_LAUNCHER( 5120, fp16, fp32, fp32, fp32, fp32, 1, 1, 4, 16);
-REGISTER_FWD_LAUNCHER( 5120, fp32, fp16, fp32, fp16, fp32, 1, 1, 4, 16);
-REGISTER_FWD_LAUNCHER( 5120, fp16, fp16, fp32, fp16, fp32, 1, 1, 4, 16);
-REGISTER_FWD_LAUNCHER( 5120, fp32, fp16, fp16, fp16, fp32, 1, 1, 4, 16);
-REGISTER_FWD_LAUNCHER( 5120, fp32, bf16, fp32, bf16, fp32, 1, 1, 4, 16);
-REGISTER_FWD_LAUNCHER( 5120, bf16, bf16, fp32, bf16, fp32, 1, 1, 4, 16);
-REGISTER_FWD_LAUNCHER( 5120, fp32, bf16, bf16, bf16, fp32, 1, 1, 4, 16);
-REGISTER_FWD_LAUNCHER( 5120, fp16, fp16, fp16, fp16, fp32, 1, 1, 4, 16);
-REGISTER_FWD_LAUNCHER( 5120, bf16, bf16, bf16, bf16, fp32, 1, 1, 4, 16);
diff --git a/based/csrc/layer_norm/ln_fwd_6144.cu b/based/csrc/layer_norm/ln_fwd_6144.cu
deleted file mode 100644
index 4df01ea..0000000
--- a/based/csrc/layer_norm/ln_fwd_6144.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_fwd_kernels.cuh"
-
-// Create forward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RYTPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG
-
-REGISTER_FWD_LAUNCHER( 6144, fp32, fp32, fp32, fp32, fp32, 1, 1, 8, 16);
-REGISTER_FWD_LAUNCHER( 6144, fp16, fp32, fp32, fp32, fp32, 1, 1, 8, 16);
-REGISTER_FWD_LAUNCHER( 6144, fp32, fp16, fp32, fp16, fp32, 1, 1, 8, 16);
-REGISTER_FWD_LAUNCHER( 6144, fp16, fp16, fp32, fp16, fp32, 1, 1, 8, 16);
-REGISTER_FWD_LAUNCHER( 6144, fp32, fp16, fp16, fp16, fp32, 1, 1, 8, 16);
-REGISTER_FWD_LAUNCHER( 6144, fp32, bf16, fp32, bf16, fp32, 1, 1, 8, 16);
-REGISTER_FWD_LAUNCHER( 6144, bf16, bf16, fp32, bf16, fp32, 1, 1, 8, 16);
-REGISTER_FWD_LAUNCHER( 6144, fp32, bf16, bf16, bf16, fp32, 1, 1, 8, 16);
-REGISTER_FWD_LAUNCHER( 6144, fp16, fp16, fp16, fp16, fp32, 1, 1, 8, 16);
-REGISTER_FWD_LAUNCHER( 6144, bf16, bf16, bf16, bf16, fp32, 1, 1, 8, 16);
diff --git a/based/csrc/layer_norm/ln_fwd_7168.cu b/based/csrc/layer_norm/ln_fwd_7168.cu
deleted file mode 100644
index 8343666..0000000
--- a/based/csrc/layer_norm/ln_fwd_7168.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_fwd_kernels.cuh"
-
-// Create forward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RYTPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG
-
-REGISTER_FWD_LAUNCHER( 7168, fp32, fp32, fp32, fp32, fp32, 1, 1, 8, 16);
-REGISTER_FWD_LAUNCHER( 7168, fp16, fp32, fp32, fp32, fp32, 1, 1, 8, 16);
-REGISTER_FWD_LAUNCHER( 7168, fp32, fp16, fp32, fp16, fp32, 1, 1, 4, 16);
-REGISTER_FWD_LAUNCHER( 7168, fp16, fp16, fp32, fp16, fp32, 1, 1, 4, 16);
-REGISTER_FWD_LAUNCHER( 7168, fp32, fp16, fp16, fp16, fp32, 1, 1, 4, 16);
-REGISTER_FWD_LAUNCHER( 7168, fp32, bf16, fp32, bf16, fp32, 1, 1, 4, 16);
-REGISTER_FWD_LAUNCHER( 7168, bf16, bf16, fp32, bf16, fp32, 1, 1, 4, 16);
-REGISTER_FWD_LAUNCHER( 7168, fp32, bf16, bf16, bf16, fp32, 1, 1, 4, 16);
-REGISTER_FWD_LAUNCHER( 7168, fp16, fp16, fp16, fp16, fp32, 1, 1, 4, 16);
-REGISTER_FWD_LAUNCHER( 7168, bf16, bf16, bf16, bf16, fp32, 1, 1, 4, 16);
diff --git a/based/csrc/layer_norm/ln_fwd_768.cu b/based/csrc/layer_norm/ln_fwd_768.cu
deleted file mode 100644
index 06d5a3b..0000000
--- a/based/csrc/layer_norm/ln_fwd_768.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_fwd_kernels.cuh"
-
-// Create forward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RYTPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG
-
-REGISTER_FWD_LAUNCHER(  768, fp32, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER(  768, fp16, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER(  768, fp32, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER(  768, fp16, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER(  768, fp32, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER(  768, fp32, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER(  768, bf16, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER(  768, fp32, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER(  768, fp16, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
-REGISTER_FWD_LAUNCHER(  768, bf16, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
diff --git a/based/csrc/layer_norm/ln_fwd_8192.cu b/based/csrc/layer_norm/ln_fwd_8192.cu
deleted file mode 100644
index bf7cb40..0000000
--- a/based/csrc/layer_norm/ln_fwd_8192.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_fwd_kernels.cuh"
-
-// Create forward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RYTPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG
-
-REGISTER_FWD_LAUNCHER( 8192, fp32, fp32, fp32, fp32, fp32, 1, 1, 8, 16);
-REGISTER_FWD_LAUNCHER( 8192, fp16, fp32, fp32, fp32, fp32, 1, 1, 8, 16);
-REGISTER_FWD_LAUNCHER( 8192, fp32, fp16, fp32, fp16, fp32, 1, 1, 8, 16);
-REGISTER_FWD_LAUNCHER( 8192, fp16, fp16, fp32, fp16, fp32, 1, 1, 8, 16);
-REGISTER_FWD_LAUNCHER( 8192, fp32, fp16, fp16, fp16, fp32, 1, 1, 8, 16);
-REGISTER_FWD_LAUNCHER( 8192, fp32, bf16, fp32, bf16, fp32, 1, 1, 8, 16);
-REGISTER_FWD_LAUNCHER( 8192, bf16, bf16, fp32, bf16, fp32, 1, 1, 8, 16);
-REGISTER_FWD_LAUNCHER( 8192, fp32, bf16, bf16, bf16, fp32, 1, 1, 8, 16);
-REGISTER_FWD_LAUNCHER( 8192, fp16, fp16, fp16, fp16, fp32, 1, 1, 8, 16);
-REGISTER_FWD_LAUNCHER( 8192, bf16, bf16, bf16, bf16, fp32, 1, 1, 8, 16);
diff --git a/based/csrc/layer_norm/ln_fwd_kernels.cuh b/based/csrc/layer_norm/ln_fwd_kernels.cuh
deleted file mode 100644
index f6bccb8..0000000
--- a/based/csrc/layer_norm/ln_fwd_kernels.cuh
+++ /dev/null
@@ -1,272 +0,0 @@
-#pragma once
-
-#ifdef OLD_GENERATOR_PATH
-#include <ATen/CUDAGeneratorImpl.h>
-#else
-#include <ATen/cuda/CUDAGeneratorImpl.h>
-#endif
-
-#include <ATen/cuda/detail/UnpackRaw.cuh>  // For at::cuda::philox::unpack
-#include <curand_kernel.h>
-
-#include "ln.h"
-#include "ln_utils.cuh"
-#include "ln_kernel_traits.h"
-#include "static_switch.h"
-
-namespace layer_norm {
-
-template<typename Ktraits, bool Is_dropout, bool Has_colscale, bool Has_subset, bool Is_even_cols>
-__global__ __launch_bounds__(Ktraits::THREADS_PER_CTA) 
-void ln_fwd_kernel(FwdParams params) {
-
-    enum { ROWS_PER_CTA = Ktraits::ROWS_PER_CTA };
-    enum { WARPS_N = Ktraits::WARPS_N };
-    enum { WARPS_M = Ktraits::WARPS_M };
-    enum { THREADS_PER_ROW = Ktraits::THREADS_PER_ROW };
-    enum { VEC_COLS_PER_LDG = Ktraits::VEC_COLS_PER_LDG };
-    enum { BYTES_PER_ROW = Ktraits::BYTES_PER_ROW };
-    enum { LDGS = Ktraits::LDGS };
-    enum { NUM_ELTS = Ktraits::NUM_ELTS };
-    enum { CTAS_PER_ROW = Ktraits::CTAS_PER_ROW };
-
-    using input_t = typename Ktraits::input_t;
-    using residual_t = typename Ktraits::residual_t;
-    using output_t = typename Ktraits::output_t;
-    using index_t = typename Ktraits::index_t;
-    using compute_t = typename Ktraits::compute_t;
-    using mask_t = typename Ktraits::mask_t;
-    using Ivec = typename Ktraits::Ivec;
-    using Rvec = typename Ktraits::Rvec;
-    using Ovec = typename Ktraits::Ovec;
-    using Wvec = typename Ktraits::Wvec;
-    using Cvec = typename Ktraits::Cvec;
-    using Mvec = typename Ktraits::Mvec;
-
-    using Stats = typename Ktraits::Stats;
-    using stats_t = typename Stats::stats_t;
-
-    const bool has_residual = params.residual != nullptr;
-    const bool save_x = has_residual || Is_dropout || Has_colscale || (params.rowscale != nullptr) || Has_subset || !(std::is_same<input_t, residual_t>::value);
-
-    extern __shared__ char smem_[];
-
-    const index_t tidx = threadIdx.x;
-    const index_t bidn = blockIdx.x % CTAS_PER_ROW;
-    const index_t bidm = blockIdx.x / CTAS_PER_ROW;
-    const index_t lane = tidx % THREADS_PER_WARP;
-    const index_t warp = tidx / THREADS_PER_WARP;
-    const index_t warp_m = warp / WARPS_N;
-    const index_t warp_n = warp % WARPS_N;
-
-    const index_t r = bidm * ROWS_PER_CTA + warp_m;
-    const index_t c = bidn * THREADS_PER_ROW + warp_n * THREADS_PER_WARP + lane;
-
-    Stats stats(params, bidm, bidn, warp_m, warp_n, lane, smem_);
-
-    compute_t *mu_ptr = static_cast<compute_t *>(params.mu);
-    compute_t *rs_ptr = static_cast<compute_t *>(params.rs);
-
-    const input_t *rowscale = static_cast<input_t *>(params.rowscale);
-    const index_t *x0_subset = static_cast<index_t *>(params.x0_subset);
-    const index_t *z_subset = static_cast<index_t *>(params.z_subset);
-
-    // https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/Dropout.cu
-    curandStatePhilox4_32_10_t state;
-    if (Is_dropout) {
-        auto seeds = at::cuda::philox::unpack(params.philox_args);
-        const index_t tidx_global = blockIdx.x * blockDim.x + threadIdx.x;
-        curand_init(std::get<0>(seeds), tidx_global, std::get<1>(seeds), &state);
-    }
-
-    const index_t num_valid_ldgs = ((params.cols / Ktraits::ELTS_PER_LDG) - 1 - c + VEC_COLS_PER_LDG) / VEC_COLS_PER_LDG;
-
-    Wvec gamma[LDGS];
-    Wvec beta[LDGS];
-    Wvec colscale[LDGS];
-    index_t idx = c;
-    #pragma unroll
-    for( int it = 0; it < LDGS; it++ ) {
-        if (Is_even_cols || (it < num_valid_ldgs)) {
-            gamma[it].load_from(params.gamma, idx);
-            if (params.beta != nullptr) {
-                beta[it].load_from(params.beta, idx);
-            } else {
-                beta[it].zero_();
-            }
-            if (Has_colscale) { colscale[it].load_from(params.colscale, idx); }
-            idx += VEC_COLS_PER_LDG;
-        }
-    }
-
-    for( int row = r; row < params.rows; row += params.ctas_per_col * ROWS_PER_CTA ) {
-        const compute_t rowscale_val = !Has_subset ? (params.rowscale == nullptr ? 1.0f : compute_t(rowscale[row])) : params.rowscale_const;
-        const int row_x0 = !Has_subset ? row + 1 : x0_subset[row];
-        const int row_z = !Has_subset ? row + 1 : z_subset[row];
-        const bool load_x0 = !Has_subset || row_x0 > 0;
-        index_t idx_x = row * params.cols / Ktraits::ELTS_PER_LDG + c;
-        index_t idx_x0 = !Has_subset ? idx_x : (load_x0 ? (row_x0 - 1) * params.cols / Ktraits::ELTS_PER_LDG + c : 0);
-        compute_t xf[LDGS * NUM_ELTS];
-        #pragma unroll
-        for( int it = 0; it < LDGS; it++ ) {
-            if (Is_even_cols || (it < num_valid_ldgs)) {
-                Ivec x0;
-                Rvec residual;
-                Rvec x;
-                Mvec dmask;
-                if (load_x0) { x0.load_from(params.x0, !Has_subset ? idx_x : idx_x0); }
-                if (has_residual) { residual.load_from(params.residual, idx_x); }
-                #pragma unroll
-                for( int jt = 0; jt < NUM_ELTS; jt++ ) {
-                    // TD [2022-04-22]: We're memory bound, not compute bound, so we don't need to use
-                    // the more efficient curand_uniform4.
-                    compute_t x_ij;
-                    if (load_x0) {
-                        mask_t keep = !Is_dropout ? true : curand_uniform(&state) <= params.dropout_keep_p;
-                        if (Is_dropout) { dmask.data.elt[jt] = keep; }
-                        compute_t x0_ij = compute_t(x0.data.elt[jt]) * rowscale_val;
-                        x0_ij = keep ? (Is_dropout ? x0_ij * params.dropout_scale : x0_ij) : 0.0f;
-                        if (Has_colscale) { x0_ij *= compute_t(colscale[it].data.elt[jt]); }
-                        x_ij = has_residual ? x0_ij + compute_t(residual.data.elt[jt]) : x0_ij;
-                    } else {
-                        x_ij = has_residual ? compute_t(residual.data.elt[jt]) : 0.f;
-                    }
-                    if (save_x) { x.data.elt[jt] = x_ij; }
-                    xf[it * NUM_ELTS + jt] = x_ij;
-                }
-                if (save_x) { x.store_to(params.x, idx_x); }
-                if (Is_dropout && load_x0) { dmask.store_to(params.dmask, !Has_subset ? idx_x : idx_x0); }
-                idx_x += VEC_COLS_PER_LDG;
-                idx_x0 += VEC_COLS_PER_LDG;
-            }
-        }
-
-        static_assert(CTAS_PER_ROW == 1, "Don't support multiple CTAs per row for now");
-        const index_t num_vecs = params.cols / Ktraits::ELTS_PER_LDG;
-        const index_t num_full_ldgs = num_vecs / Ktraits::VEC_COLS_PER_LDG;
-        const index_t remaining_vecs = num_vecs % Ktraits::VEC_COLS_PER_LDG;
-        auto valid_elts_in_warp_fn = [num_full_ldgs, remaining_vecs] (int warp_n) -> int {
-            // Need to convert to int, otherwise the subtraction will wrap around.
-            const index_t valid_partial_vecs_in_warp =
-                std::min(std::max(int(remaining_vecs) - int(warp_n * THREADS_PER_WARP), int(0)),
-                        int(THREADS_PER_WARP));
-            return (num_full_ldgs * THREADS_PER_WARP + valid_partial_vecs_in_warp) * NUM_ELTS;
-        };
-        stats_t s = stats.template compute<Is_even_cols>(
-            xf, params.inverse_cols, valid_elts_in_warp_fn, num_valid_ldgs * NUM_ELTS
-        );
-
-        compute_t mu = layer_norm::Get<0>::of<stats_t, compute_t>(s);
-        compute_t m2 = layer_norm::Get<1>::of<stats_t, compute_t>(s);
-
-        if( bidn == 0 && warp_n == 0 && lane == 0 ) {
-            mu_ptr[row] = mu;
-        }
-
-        compute_t rs = rsqrtf(m2 * params.inverse_cols + params.epsilon + (!params.is_rms_norm ? 0.f : mu * mu));
-
-        if( bidn == 0 && warp_n == 0 && lane == 0 ) {
-            rs_ptr[row] = rs;
-        }
-
-        const bool save_z = !Has_subset || row_z > 0;
-        if (save_z) {
-            index_t idx_z = (!Has_subset ? row : (row_z - 1)) * params.cols / Ktraits::ELTS_PER_LDG + c;
-            #pragma unroll
-            for( int it = 0; it < LDGS; it++ ) {
-                if (Is_even_cols || (it < num_valid_ldgs)) {
-                    Ovec z;
-                    #pragma unroll
-                    for( int jt = 0; jt < NUM_ELTS; jt++ ) {
-                        compute_t y_ij = compute_t(rs * (xf[it * NUM_ELTS + jt] - (!params.is_rms_norm ? mu : 0.f)));
-                        compute_t g_ij = gamma[it].data.elt[jt];
-                        compute_t b_ij = beta[it].data.elt[jt];
-                        z.data.elt[jt] = output_t(g_ij * y_ij + b_ij);
-                    }
-                    z.store_to(params.z, idx_z);
-                    idx_z += VEC_COLS_PER_LDG;
-                }
-            }
-        }
-
-    }
-}
-
-}  // namespace layer_norm
-
-using namespace layer_norm;
-
-template<
-    typename weight_t,
-    typename input_t,
-    typename residual_t,
-    typename output_t,
-    typename compute_t,
-    typename index_t,
-    int HIDDEN_SIZE,
-    int CTAS_PER_ROW,
-    int WARPS_M,
-    int WARPS_N,
-    int BYTES_PER_LDG
->
-void launch_(LaunchParams<FwdParams> &launch_params, const bool configure_params){
-
-    using Kernel_traits = Kernel_traits<weight_t,
-                                        input_t,
-                                        residual_t,
-                                        output_t,
-                                        compute_t,
-                                        index_t,
-                                        HIDDEN_SIZE,
-                                        CTAS_PER_ROW,
-                                        WARPS_M,
-                                        WARPS_N,
-                                        BYTES_PER_LDG
-                                        >;
-    bool has_colscale = launch_params.params.colscale != nullptr;
-    bool has_subset = launch_params.params.x0_subset != nullptr;
-    bool is_even_cols = launch_params.params.cols == HIDDEN_SIZE;
-    BOOL_SWITCH(launch_params.params.dropout_keep_p < 1.f, IsDropoutConst, [&] {
-        BOOL_SWITCH(has_colscale, HasColscaleConst, [&] {
-            BOOL_SWITCH(has_subset, HasSubsetConst, [&] {
-                    BOOL_SWITCH(is_even_cols, IsEvenColsConst, [&] {
-                        auto kernel = &ln_fwd_kernel<Kernel_traits, IsDropoutConst, HasColscaleConst, HasSubsetConst, IsEvenColsConst>;
-                    if( configure_params ) {
-                        int ctas_per_sm;
-                        CHECK_CUDA(cudaOccupancyMaxActiveBlocksPerMultiprocessor(
-                            &ctas_per_sm, kernel, Kernel_traits::THREADS_PER_CTA, Kernel_traits::SMEM_BYTES_FWD));
-                        launch_params.params.ctas_per_col = launch_params.props->multiProcessorCount * ctas_per_sm / Kernel_traits::CTAS_PER_ROW;
-                        const size_t rows_per_loop = launch_params.params.ctas_per_col * Kernel_traits::ROWS_PER_CTA;
-                        launch_params.elts_per_thread = (launch_params.params.rows + rows_per_loop - 1) / rows_per_loop * Kernel_traits::LDGS * Kernel_traits::NUM_ELTS;
-                        launch_params.barrier_size = 0;
-                        launch_params.workspace_bytes = 0;
-                        if(Kernel_traits::CTAS_PER_ROW > 1) {
-                            launch_params.barrier_size = 2 * launch_params.params.ctas_per_col;
-                            launch_params.workspace_bytes = launch_params.params.ctas_per_col
-                                                          * Kernel_traits::WARPS_M
-                                                          * Kernel_traits::CTAS_PER_ROW
-                                                          * sizeof(typename Kernel_traits::Stats::stats_t)
-                                                          * 2;
-                        }
-                        return;
-                    }
-
-                    if( Kernel_traits::SMEM_BYTES_FWD >= 48 * 1024 ) {
-                        CHECK_CUDA(cudaFuncSetAttribute(kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, Kernel_traits::SMEM_BYTES_FWD));
-                    }
-                    auto stream = launch_params.stream;
-                    auto ctas_per_col = launch_params.params.ctas_per_col;
-
-                    if( Kernel_traits::CTAS_PER_ROW == 1 ) {
-                        kernel<<<ctas_per_col, Kernel_traits::THREADS_PER_CTA, Kernel_traits::SMEM_BYTES_FWD, stream>>>(launch_params.params);
-                    } else {
-                        dim3 grid(Kernel_traits::CTAS_PER_ROW * ctas_per_col);
-                        dim3 block(Kernel_traits::THREADS_PER_CTA);
-                        void *params_ = (void *)&launch_params.params;
-                        cudaLaunchCooperativeKernel((void *)kernel, grid, block, (void **)&params_, Kernel_traits::SMEM_BYTES_FWD, stream);
-                    }
-                });
-            });
-        });
-    });
-}
diff --git a/based/csrc/layer_norm/ln_kernel_traits.h b/based/csrc/layer_norm/ln_kernel_traits.h
deleted file mode 100644
index 77de6bf..0000000
--- a/based/csrc/layer_norm/ln_kernel_traits.h
+++ /dev/null
@@ -1,172 +0,0 @@
-#pragma once
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-namespace layer_norm {
-template<
-    uint32_t HIDDEN_SIZE_,
-    typename weight_t_,
-    typename input_t_,
-    typename residual_t_,
-    typename output_t_,
-    typename compute_t_,
-    typename index_t_,
-    uint32_t THREADS_PER_CTA_
->
-struct Kernel_traits_base {
-
-    using weight_t = weight_t_;
-    using input_t = input_t_;
-    using residual_t = residual_t_;
-    using output_t = output_t_;
-    using compute_t = compute_t_;
-    using index_t = index_t_;
-
-    enum { HIDDEN_SIZE = HIDDEN_SIZE_ };
-    enum { THREADS_PER_CTA = THREADS_PER_CTA_ };
-    enum { THREADS_PER_WARP = 32 };
-
-};
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-template<
-    uint32_t HIDDEN_SIZE_,
-    typename weight_t_,
-    typename input_t_,
-    typename residual_t_,
-    typename output_t_,
-    typename compute_t_,
-    typename index_t_,
-    bool Has_colscale,
-    uint32_t THREADS_PER_CTA_,
-    uint32_t BYTES_PER_LDG_,
-    typename Base = Kernel_traits_base<HIDDEN_SIZE_,
-                                        weight_t_,
-                                        input_t_,
-                                        residual_t_,
-                                        output_t_,
-                                        compute_t_,
-                                        index_t_,
-                                        THREADS_PER_CTA_>
->
-struct Kernel_traits_finalize : public Base {
-    enum { ROWS_PER_CTA = Base::THREADS_PER_CTA / Base::THREADS_PER_WARP };
-    static_assert((int) ROWS_PER_CTA <= (int) Base::THREADS_PER_WARP);
-    // Bytes per global load from the input. 
-    enum { BYTES_PER_LDG = BYTES_PER_LDG_ };
-    // Number of elements fetched by a global load.
-    enum { ELTS_PER_LDG = BYTES_PER_LDG / sizeof(compute_t_) };
-    // Bytes per global store of the weights.
-    enum { BYTES_PER_STG = ELTS_PER_LDG * sizeof(weight_t_) };
-    static_assert(sizeof(BYTES_PER_LDG) == 4, "Conflict-free smem transpose only implemented for 4B compute type!");
-    static_assert(Base::THREADS_PER_CTA == ROWS_PER_CTA * Base::THREADS_PER_WARP, "We assume one warp per row!");
-    // The total number of BYTES_PER_LDG-wide words in a hidden vector.
-    enum { COLS = HIDDEN_SIZE_ * sizeof(compute_t_) / BYTES_PER_LDG };
-    static_assert(COLS * BYTES_PER_LDG == HIDDEN_SIZE_ * sizeof(compute_t_));
-
-    // Shared memory size to transpose the CTA result.
-    enum { SMEM_BYTES_TRANSPOSE = Base::THREADS_PER_CTA * BYTES_PER_LDG };
-    // Shared memory size to coalsece the CTA result.
-    enum { SMEM_BYTES_OUTPUT = Base::THREADS_PER_WARP * BYTES_PER_LDG };
-    // Shared memory requirement per CTA. 
-    static constexpr int NUM_FACTORS = Has_colscale ? 3 : 2;
-    enum { SMEM_BYTES_PER_CTA = NUM_FACTORS * SMEM_BYTES_TRANSPOSE + NUM_FACTORS * SMEM_BYTES_OUTPUT };
-
-    // The type of the reducer.
-    using Reducer = layer_norm::Reducer<compute_t_, 1, 1, 1>;
-
-    // Condition for the whole CTA to participate in syncthreads.
-    static_assert(COLS % Base::THREADS_PER_WARP == 0);
-    enum { CTAS = COLS / Base::THREADS_PER_WARP };
-}; 
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-
-template<
-    typename weight_t_,
-    typename input_t_,
-    typename residual_t_,
-    typename output_t_,
-    typename compute_t_,
-    typename index_t_,
-    uint32_t HIDDEN_SIZE_, 
-    uint32_t CTAS_PER_ROW_, 
-    uint32_t WARPS_M_, 
-    uint32_t WARPS_N_, 
-    uint32_t BYTES_PER_LDG_ = 16,
-    typename Base = Kernel_traits_base<
-        HIDDEN_SIZE_,
-        weight_t_, 
-        input_t_,
-        residual_t_,
-        output_t_, 
-        compute_t_, 
-        index_t_, 
-        WARPS_M_*WARPS_N_*THREADS_PER_WARP
-        >
->
-struct Kernel_traits : public Base {
-
-    using input_t = typename Base::input_t;
-    using residual_t = typename Base::residual_t;
-    using weight_t = typename Base::weight_t;
-    using compute_t = typename Base::compute_t;
-    using output_t = typename Base::output_t;
-    using index_t = typename Base::index_t;
-    // using mask_t = unsigned char;
-    using mask_t = bool;
-
-    enum { CTAS_PER_ROW = CTAS_PER_ROW_ };
-    enum { WARPS_M = WARPS_M_ };
-    enum { WARPS_N = WARPS_N_ };
-    enum { COLS = HIDDEN_SIZE_ };
-    enum { HIDDEN_SIZE = HIDDEN_SIZE_ };
-    enum { BYTES_PER_LDG = BYTES_PER_LDG_ };
-    enum { NUM_ELTS = BYTES_PER_LDG / sizeof(input_t) };
-
-    enum { THREADS_PER_ROW = WARPS_N * THREADS_PER_WARP };
-    enum { THREADS_PER_CTA = WARPS_M * THREADS_PER_ROW };
-    enum { ROWS_PER_CTA = WARPS_M };
-
-    enum { BYTES_PER_ROW = COLS * sizeof(input_t) };
-    enum { BYTES_PER_ROW_PER_CTA = THREADS_PER_ROW * BYTES_PER_LDG };
-    // Multi-row per CTA not supported for multi-CTA => no smem for WGRAD needed
-    enum { SMEM_BYTES_WGRAD = CTAS_PER_ROW > 1 ? 0 : ROWS_PER_CTA * COLS * sizeof(compute_t) };
-    static_assert(WARPS_M == 1 || CTAS_PER_ROW == 1);
-
-    using reduce_t = typename layer_norm::TypeToVec2<compute_t>::Type;
-    using Reducer = layer_norm::Reducer<reduce_t, CTAS_PER_ROW, WARPS_M, WARPS_N>; 
-
-    enum { SMEM_BYTES_DGRAD = Reducer::SMEM_BYTES };
-    enum { SMEM_BYTES = SMEM_BYTES_DGRAD  + SMEM_BYTES_WGRAD };
-
-    using Ivec = layer_norm::Vec<input_t, NUM_ELTS>;
-    using Rvec = layer_norm::Vec<residual_t, NUM_ELTS>;
-    using Ovec = layer_norm::Vec<output_t, NUM_ELTS>;
-    using Wvec = layer_norm::Vec<weight_t, NUM_ELTS>;
-    using Cvec = layer_norm::Vec<compute_t, NUM_ELTS>;
-    using Mvec = layer_norm::Vec<mask_t, NUM_ELTS>;
-    enum { ELTS_PER_LDG = BYTES_PER_LDG / sizeof(input_t) };
-
-    // Assume that each thread can handle the same number of elements in the output and weights as in the input.
-    static_assert(sizeof(input_t) == sizeof(output_t));
-    static_assert(sizeof(input_t) <= sizeof(residual_t));
-    // The number of columns fetched per load from input: one per thread.
-    enum { VEC_COLS_PER_LDG =  CTAS_PER_ROW * THREADS_PER_ROW };
-    // The total number of vectorized loads/stores per hidden vector.
-    enum { VEC_COLS = COLS / ELTS_PER_LDG };
-    // The number of loads per thread for the input.
-    enum { LDGS = VEC_COLS / VEC_COLS_PER_LDG };
-    static_assert(LDGS * VEC_COLS_PER_LDG  == VEC_COLS);
-    //static_assert(LDGS * BYTES_PER_ROW_PER_CTA * CTAS_PER_ROW == BYTES_PER_ROW, "");
-
-    using Stats = layer_norm::Stats<compute_t, CTAS_PER_ROW, WARPS_M, WARPS_N>;
-    enum { SMEM_BYTES_FWD = Stats::SMEM_BYTES };
-
-};
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-}  // namespace layer_norm
diff --git a/based/csrc/layer_norm/ln_parallel_bwd_1024.cu b/based/csrc/layer_norm/ln_parallel_bwd_1024.cu
deleted file mode 100644
index 6f4e774..0000000
--- a/based/csrc/layer_norm/ln_parallel_bwd_1024.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_parallel_residual_bwd_kernels.cuh"
-
-// Create backward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RTYPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG, BYTES_PER_LDG_FINAL
-
-REGISTER_PARALLEL_BWD_LAUNCHER(  1024, fp32, fp32, fp32, fp32, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  1024, fp16, fp32, fp32, fp32, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  1024, fp32, fp16, fp32, fp16, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  1024, fp16, fp16, fp32, fp16, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  1024, fp32, fp16, fp16, fp16, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  1024, fp32, bf16, fp32, bf16, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  1024, bf16, bf16, fp32, bf16, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  1024, fp32, bf16, bf16, bf16, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  1024, fp16, fp16, fp16, fp16, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  1024, bf16, bf16, bf16, bf16, fp32, 1, 4, 1, 16, 4);
diff --git a/based/csrc/layer_norm/ln_parallel_bwd_1280.cu b/based/csrc/layer_norm/ln_parallel_bwd_1280.cu
deleted file mode 100644
index 2dba3be..0000000
--- a/based/csrc/layer_norm/ln_parallel_bwd_1280.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_parallel_residual_bwd_kernels.cuh"
-
-// Create backward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RTYPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG, BYTES_PER_LDG_FINAL
-
-REGISTER_PARALLEL_BWD_LAUNCHER(  1280, fp32, fp32, fp32, fp32, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  1280, fp16, fp32, fp32, fp32, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  1280, fp32, fp16, fp32, fp16, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  1280, fp16, fp16, fp32, fp16, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  1280, fp32, fp16, fp16, fp16, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  1280, fp32, bf16, fp32, bf16, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  1280, bf16, bf16, fp32, bf16, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  1280, fp32, bf16, bf16, bf16, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  1280, fp16, fp16, fp16, fp16, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  1280, bf16, bf16, bf16, bf16, fp32, 1, 4, 1, 16, 4);
diff --git a/based/csrc/layer_norm/ln_parallel_bwd_1536.cu b/based/csrc/layer_norm/ln_parallel_bwd_1536.cu
deleted file mode 100644
index c2ac4b1..0000000
--- a/based/csrc/layer_norm/ln_parallel_bwd_1536.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_parallel_residual_bwd_kernels.cuh"
-
-// Create backward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RTYPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG, BYTES_PER_LDG_FINAL
-
-REGISTER_PARALLEL_BWD_LAUNCHER( 1536, fp32, fp32, fp32, fp32, fp32, 1, 1, 4, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 1536, fp16, fp32, fp32, fp32, fp32, 1, 1, 4, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 1536, fp32, fp16, fp32, fp16, fp32, 1, 1, 4,  8, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 1536, fp16, fp16, fp32, fp16, fp32, 1, 1, 4,  8, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 1536, fp32, fp16, fp16, fp16, fp32, 1, 1, 4,  8, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 1536, fp32, bf16, fp32, bf16, fp32, 1, 1, 4,  8, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 1536, bf16, bf16, fp32, bf16, fp32, 1, 1, 4,  8, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 1536, fp32, bf16, bf16, bf16, fp32, 1, 1, 4,  8, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 1536, fp16, fp16, fp16, fp16, fp32, 1, 1, 4,  8, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 1536, bf16, bf16, bf16, bf16, fp32, 1, 1, 4,  8, 4);
diff --git a/based/csrc/layer_norm/ln_parallel_bwd_2048.cu b/based/csrc/layer_norm/ln_parallel_bwd_2048.cu
deleted file mode 100644
index f7f959e..0000000
--- a/based/csrc/layer_norm/ln_parallel_bwd_2048.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_parallel_residual_bwd_kernels.cuh"
-
-// Create backward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RTYPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG, BYTES_PER_LDG_FINAL
-
-REGISTER_PARALLEL_BWD_LAUNCHER( 2048, fp32, fp32, fp32, fp32, fp32, 1, 1, 4, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 2048, fp16, fp32, fp32, fp32, fp32, 1, 1, 4, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 2048, fp32, fp16, fp32, fp16, fp32, 1, 1, 4, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 2048, fp16, fp16, fp32, fp16, fp32, 1, 1, 4, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 2048, fp32, fp16, fp16, fp16, fp32, 1, 1, 4, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 2048, fp32, bf16, fp32, bf16, fp32, 1, 1, 4, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 2048, bf16, bf16, fp32, bf16, fp32, 1, 1, 4, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 2048, fp32, bf16, bf16, bf16, fp32, 1, 1, 4, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 2048, fp16, fp16, fp16, fp16, fp32, 1, 1, 4, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 2048, bf16, bf16, bf16, bf16, fp32, 1, 1, 4, 16, 4);
\ No newline at end of file
diff --git a/based/csrc/layer_norm/ln_parallel_bwd_256.cu b/based/csrc/layer_norm/ln_parallel_bwd_256.cu
deleted file mode 100644
index fa613cf..0000000
--- a/based/csrc/layer_norm/ln_parallel_bwd_256.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_parallel_residual_bwd_kernels.cuh"
-
-// Create backward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RTYPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG, BYTES_PER_LDG_FINAL
-
-REGISTER_PARALLEL_BWD_LAUNCHER(  256, fp32, fp32, fp32, fp32, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  256, fp16, fp32, fp32, fp32, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  256, fp32, fp16, fp32, fp16, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  256, fp16, fp16, fp32, fp16, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  256, fp32, fp16, fp16, fp16, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  256, fp32, bf16, fp32, bf16, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  256, bf16, bf16, fp32, bf16, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  256, fp32, bf16, bf16, bf16, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  256, fp16, fp16, fp16, fp16, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  256, bf16, bf16, bf16, bf16, fp32, 1, 4, 1, 16, 4);
diff --git a/based/csrc/layer_norm/ln_parallel_bwd_2560.cu b/based/csrc/layer_norm/ln_parallel_bwd_2560.cu
deleted file mode 100644
index 5f57076..0000000
--- a/based/csrc/layer_norm/ln_parallel_bwd_2560.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_parallel_residual_bwd_kernels.cuh"
-
-// Create backward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RTYPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG, BYTES_PER_LDG_FINAL
-
-REGISTER_PARALLEL_BWD_LAUNCHER( 2560, fp32, fp32, fp32, fp32, fp32, 1, 1, 4, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 2560, fp16, fp32, fp32, fp32, fp32, 1, 1, 4, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 2560, fp32, fp16, fp32, fp16, fp32, 1, 1, 4,  8, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 2560, fp16, fp16, fp32, fp16, fp32, 1, 1, 4,  8, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 2560, fp32, fp16, fp16, fp16, fp32, 1, 1, 4,  8, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 2560, fp32, bf16, fp32, bf16, fp32, 1, 1, 4,  8, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 2560, bf16, bf16, fp32, bf16, fp32, 1, 1, 4,  8, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 2560, fp32, bf16, bf16, bf16, fp32, 1, 1, 4,  8, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 2560, fp16, fp16, fp16, fp16, fp32, 1, 1, 4,  8, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 2560, bf16, bf16, bf16, bf16, fp32, 1, 1, 4,  8, 4);
diff --git a/based/csrc/layer_norm/ln_parallel_bwd_3072.cu b/based/csrc/layer_norm/ln_parallel_bwd_3072.cu
deleted file mode 100644
index 8fdcb8f..0000000
--- a/based/csrc/layer_norm/ln_parallel_bwd_3072.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_parallel_residual_bwd_kernels.cuh"
-
-// Create backward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RTYPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG, BYTES_PER_LDG_FINAL
-
-REGISTER_PARALLEL_BWD_LAUNCHER( 3072, fp32, fp32, fp32, fp32, fp32, 1, 1, 4, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 3072, fp16, fp32, fp32, fp32, fp32, 1, 1, 4, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 3072, fp32, fp16, fp32, fp16, fp32, 1, 1, 4, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 3072, fp16, fp16, fp32, fp16, fp32, 1, 1, 4, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 3072, fp32, fp16, fp16, fp16, fp32, 1, 1, 4, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 3072, fp32, bf16, fp32, bf16, fp32, 1, 1, 4, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 3072, bf16, bf16, fp32, bf16, fp32, 1, 1, 4, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 3072, fp32, bf16, bf16, bf16, fp32, 1, 1, 4, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 3072, fp16, fp16, fp16, fp16, fp32, 1, 1, 4, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 3072, bf16, bf16, bf16, bf16, fp32, 1, 1, 4, 16, 4);
\ No newline at end of file
diff --git a/based/csrc/layer_norm/ln_parallel_bwd_4096.cu b/based/csrc/layer_norm/ln_parallel_bwd_4096.cu
deleted file mode 100644
index 8decfb0..0000000
--- a/based/csrc/layer_norm/ln_parallel_bwd_4096.cu
+++ /dev/null
@@ -1,17 +0,0 @@
-#include "ln_parallel_residual_bwd_kernels.cuh"
-
-// Create backward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RTYPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG, BYTES_PER_LDG_FINAL
-
-// Use 8 warps otherwise there's a lot of register spilling
-
-REGISTER_PARALLEL_BWD_LAUNCHER( 4096, fp32, fp32, fp32, fp32, fp32, 1, 1, 8, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 4096, fp16, fp32, fp32, fp32, fp32, 1, 1, 8, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 4096, fp32, fp16, fp32, fp16, fp32, 1, 1, 8, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 4096, fp16, fp16, fp32, fp16, fp32, 1, 1, 8, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 4096, fp32, fp16, fp16, fp16, fp32, 1, 1, 8, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 4096, fp32, bf16, fp32, bf16, fp32, 1, 1, 8, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 4096, bf16, bf16, fp32, bf16, fp32, 1, 1, 8, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 4096, fp32, bf16, bf16, bf16, fp32, 1, 1, 8, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 4096, fp16, fp16, fp16, fp16, fp32, 1, 1, 8, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 4096, bf16, bf16, bf16, bf16, fp32, 1, 1, 8, 16, 4);
\ No newline at end of file
diff --git a/based/csrc/layer_norm/ln_parallel_bwd_512.cu b/based/csrc/layer_norm/ln_parallel_bwd_512.cu
deleted file mode 100644
index 178453d..0000000
--- a/based/csrc/layer_norm/ln_parallel_bwd_512.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_parallel_residual_bwd_kernels.cuh"
-
-// Create backward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RTYPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG, BYTES_PER_LDG_FINAL
-
-REGISTER_PARALLEL_BWD_LAUNCHER(  512, fp32, fp32, fp32, fp32, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  512, fp16, fp32, fp32, fp32, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  512, fp32, fp16, fp32, fp16, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  512, fp16, fp16, fp32, fp16, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  512, fp32, fp16, fp16, fp16, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  512, fp32, bf16, fp32, bf16, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  512, bf16, bf16, fp32, bf16, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  512, fp32, bf16, bf16, bf16, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  512, fp16, fp16, fp16, fp16, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  512, bf16, bf16, bf16, bf16, fp32, 1, 4, 1, 16, 4);
diff --git a/based/csrc/layer_norm/ln_parallel_bwd_5120.cu b/based/csrc/layer_norm/ln_parallel_bwd_5120.cu
deleted file mode 100644
index 8155219..0000000
--- a/based/csrc/layer_norm/ln_parallel_bwd_5120.cu
+++ /dev/null
@@ -1,17 +0,0 @@
-#include "ln_parallel_residual_bwd_kernels.cuh"
-
-// Create backward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RTYPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG, BYTES_PER_LDG_FINAL
-
-// Use 8 warps otherwise there's a lot of register spilling
-
-REGISTER_PARALLEL_BWD_LAUNCHER( 5120, fp32, fp32, fp32, fp32, fp32, 1, 1, 8, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 5120, fp16, fp32, fp32, fp32, fp32, 1, 1, 8, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 5120, fp32, fp16, fp32, fp16, fp32, 1, 1, 8,  8, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 5120, fp16, fp16, fp32, fp16, fp32, 1, 1, 8,  8, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 5120, fp32, fp16, fp16, fp16, fp32, 1, 1, 8,  8, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 5120, fp32, bf16, fp32, bf16, fp32, 1, 1, 8,  8, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 5120, bf16, bf16, fp32, bf16, fp32, 1, 1, 8,  8, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 5120, fp32, bf16, bf16, bf16, fp32, 1, 1, 8,  8, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 5120, fp16, fp16, fp16, fp16, fp32, 1, 1, 8,  8, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 5120, bf16, bf16, bf16, bf16, fp32, 1, 1, 8,  8, 4);
\ No newline at end of file
diff --git a/based/csrc/layer_norm/ln_parallel_bwd_6144.cu b/based/csrc/layer_norm/ln_parallel_bwd_6144.cu
deleted file mode 100644
index eb8668d..0000000
--- a/based/csrc/layer_norm/ln_parallel_bwd_6144.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_parallel_residual_bwd_kernels.cuh"
-
-// Create backward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RTYPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG, BYTES_PER_LDG_FINAL
-
-REGISTER_PARALLEL_BWD_LAUNCHER( 6144, fp32, fp32, fp32, fp32, fp32, 1, 1, 8, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 6144, fp16, fp32, fp32, fp32, fp32, 1, 1, 8, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 6144, fp32, fp16, fp32, fp16, fp32, 1, 1, 8, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 6144, fp16, fp16, fp32, fp16, fp32, 1, 1, 8, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 6144, fp32, fp16, fp16, fp16, fp32, 1, 1, 8, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 6144, fp32, bf16, fp32, bf16, fp32, 1, 1, 8, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 6144, bf16, bf16, fp32, bf16, fp32, 1, 1, 8, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 6144, fp32, bf16, bf16, bf16, fp32, 1, 1, 8, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 6144, fp16, fp16, fp16, fp16, fp32, 1, 1, 8, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 6144, bf16, bf16, bf16, bf16, fp32, 1, 1, 8, 16, 4);
\ No newline at end of file
diff --git a/based/csrc/layer_norm/ln_parallel_bwd_7168.cu b/based/csrc/layer_norm/ln_parallel_bwd_7168.cu
deleted file mode 100644
index 0c12dc4..0000000
--- a/based/csrc/layer_norm/ln_parallel_bwd_7168.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_parallel_residual_bwd_kernels.cuh"
-
-// Create backward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RTYPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG, BYTES_PER_LDG_FINAL
-
-REGISTER_PARALLEL_BWD_LAUNCHER( 7168, fp32, fp32, fp32, fp32, fp32, 1, 1, 8, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 7168, fp16, fp32, fp32, fp32, fp32, 1, 1, 8, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 7168, fp32, fp16, fp32, fp16, fp32, 1, 1, 8,  8, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 7168, fp16, fp16, fp32, fp16, fp32, 1, 1, 8,  8, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 7168, fp32, fp16, fp16, fp16, fp32, 1, 1, 8,  8, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 7168, fp32, bf16, fp32, bf16, fp32, 1, 1, 8,  8, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 7168, bf16, bf16, fp32, bf16, fp32, 1, 1, 8,  8, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 7168, fp32, bf16, bf16, bf16, fp32, 1, 1, 8,  8, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 7168, fp16, fp16, fp16, fp16, fp32, 1, 1, 8,  8, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 7168, bf16, bf16, bf16, bf16, fp32, 1, 1, 8,  8, 4);
\ No newline at end of file
diff --git a/based/csrc/layer_norm/ln_parallel_bwd_768.cu b/based/csrc/layer_norm/ln_parallel_bwd_768.cu
deleted file mode 100644
index 8beece8..0000000
--- a/based/csrc/layer_norm/ln_parallel_bwd_768.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_parallel_residual_bwd_kernels.cuh"
-
-// Create backward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RTYPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG, BYTES_PER_LDG_FINAL
-
-REGISTER_PARALLEL_BWD_LAUNCHER(  768, fp32, fp32, fp32, fp32, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  768, fp16, fp32, fp32, fp32, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  768, fp32, fp16, fp32, fp16, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  768, fp16, fp16, fp32, fp16, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  768, fp32, fp16, fp16, fp16, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  768, fp32, bf16, fp32, bf16, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  768, bf16, bf16, fp32, bf16, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  768, fp32, bf16, bf16, bf16, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  768, fp16, fp16, fp16, fp16, fp32, 1, 4, 1, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER(  768, bf16, bf16, bf16, bf16, fp32, 1, 4, 1, 16, 4);
diff --git a/based/csrc/layer_norm/ln_parallel_bwd_8192.cu b/based/csrc/layer_norm/ln_parallel_bwd_8192.cu
deleted file mode 100644
index 5ad47c9..0000000
--- a/based/csrc/layer_norm/ln_parallel_bwd_8192.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_parallel_residual_bwd_kernels.cuh"
-
-// Create backward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RTYPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG, BYTES_PER_LDG_FINAL
-
-REGISTER_PARALLEL_BWD_LAUNCHER( 8192, fp32, fp32, fp32, fp32, fp32, 1, 1, 8, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 8192, fp16, fp32, fp32, fp32, fp32, 1, 1, 8, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 8192, fp32, fp16, fp32, fp16, fp32, 1, 1, 8, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 8192, fp16, fp16, fp32, fp16, fp32, 1, 1, 8, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 8192, fp32, fp16, fp16, fp16, fp32, 1, 1, 8, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 8192, fp32, bf16, fp32, bf16, fp32, 1, 1, 8, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 8192, bf16, bf16, fp32, bf16, fp32, 1, 1, 8, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 8192, fp32, bf16, bf16, bf16, fp32, 1, 1, 8, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 8192, fp16, fp16, fp16, fp16, fp32, 1, 1, 8, 16, 4);
-REGISTER_PARALLEL_BWD_LAUNCHER( 8192, bf16, bf16, bf16, bf16, fp32, 1, 1, 8, 16, 4);
\ No newline at end of file
diff --git a/based/csrc/layer_norm/ln_parallel_fwd_1024.cu b/based/csrc/layer_norm/ln_parallel_fwd_1024.cu
deleted file mode 100644
index 3c64e16..0000000
--- a/based/csrc/layer_norm/ln_parallel_fwd_1024.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_parallel_residual_fwd_kernels.cuh"
-
-// Create forward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RYTPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG
-
-REGISTER_PARALLEL_FWD_LAUNCHER( 1024, fp32, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 1024, fp16, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 1024, fp32, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 1024, fp16, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 1024, fp32, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 1024, fp32, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 1024, bf16, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 1024, fp32, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 1024, fp16, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 1024, bf16, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
diff --git a/based/csrc/layer_norm/ln_parallel_fwd_1280.cu b/based/csrc/layer_norm/ln_parallel_fwd_1280.cu
deleted file mode 100644
index 9bbfce5..0000000
--- a/based/csrc/layer_norm/ln_parallel_fwd_1280.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_parallel_residual_fwd_kernels.cuh"
-
-// Create forward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RYTPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG
-
-REGISTER_PARALLEL_FWD_LAUNCHER( 1280, fp32, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 1280, fp16, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 1280, fp32, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 1280, fp16, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 1280, fp32, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 1280, fp32, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 1280, bf16, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 1280, fp32, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 1280, fp16, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 1280, bf16, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
diff --git a/based/csrc/layer_norm/ln_parallel_fwd_1536.cu b/based/csrc/layer_norm/ln_parallel_fwd_1536.cu
deleted file mode 100644
index b57f5ed..0000000
--- a/based/csrc/layer_norm/ln_parallel_fwd_1536.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_parallel_residual_fwd_kernels.cuh"
-
-// Create forward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RYTPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG
-
-REGISTER_PARALLEL_FWD_LAUNCHER( 1536, fp32, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 1536, fp16, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 1536, fp32, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 1536, fp16, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 1536, fp32, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 1536, fp32, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 1536, bf16, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 1536, fp32, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 1536, fp16, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 1536, bf16, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
diff --git a/based/csrc/layer_norm/ln_parallel_fwd_2048.cu b/based/csrc/layer_norm/ln_parallel_fwd_2048.cu
deleted file mode 100644
index 6fa322d..0000000
--- a/based/csrc/layer_norm/ln_parallel_fwd_2048.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_parallel_residual_fwd_kernels.cuh"
-
-// Create forward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RYTPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG
-
-REGISTER_PARALLEL_FWD_LAUNCHER( 2048, fp32, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 2048, fp16, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 2048, fp32, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 2048, fp16, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 2048, fp32, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 2048, fp32, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 2048, bf16, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 2048, fp32, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 2048, fp16, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 2048, bf16, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
diff --git a/based/csrc/layer_norm/ln_parallel_fwd_256.cu b/based/csrc/layer_norm/ln_parallel_fwd_256.cu
deleted file mode 100644
index 27445a6..0000000
--- a/based/csrc/layer_norm/ln_parallel_fwd_256.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_parallel_residual_fwd_kernels.cuh"
-
-// Create forward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RYTPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG
-
-REGISTER_PARALLEL_FWD_LAUNCHER(  256, fp32, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER(  256, fp16, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER(  256, fp32, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER(  256, fp16, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER(  256, fp32, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER(  256, fp32, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER(  256, bf16, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER(  256, fp32, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER(  256, fp16, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER(  256, bf16, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
\ No newline at end of file
diff --git a/based/csrc/layer_norm/ln_parallel_fwd_2560.cu b/based/csrc/layer_norm/ln_parallel_fwd_2560.cu
deleted file mode 100644
index fdde470..0000000
--- a/based/csrc/layer_norm/ln_parallel_fwd_2560.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_parallel_residual_fwd_kernels.cuh"
-
-// Create forward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RYTPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG
-
-REGISTER_PARALLEL_FWD_LAUNCHER( 2560, fp32, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 2560, fp16, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 2560, fp32, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 2560, fp16, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 2560, fp32, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 2560, fp32, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 2560, bf16, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 2560, fp32, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 2560, fp16, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 2560, bf16, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
diff --git a/based/csrc/layer_norm/ln_parallel_fwd_3072.cu b/based/csrc/layer_norm/ln_parallel_fwd_3072.cu
deleted file mode 100644
index 992f710..0000000
--- a/based/csrc/layer_norm/ln_parallel_fwd_3072.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_parallel_residual_fwd_kernels.cuh"
-
-// Create forward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RYTPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG
-
-REGISTER_PARALLEL_FWD_LAUNCHER( 3072, fp32, fp32, fp32, fp32, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 3072, fp16, fp32, fp32, fp32, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 3072, fp32, fp16, fp32, fp16, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 3072, fp16, fp16, fp32, fp16, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 3072, fp32, fp16, fp16, fp16, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 3072, fp32, bf16, fp32, bf16, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 3072, bf16, bf16, fp32, bf16, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 3072, fp32, bf16, bf16, bf16, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 3072, fp16, fp16, fp16, fp16, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 3072, bf16, bf16, bf16, bf16, fp32, 1, 1, 4, 16);
diff --git a/based/csrc/layer_norm/ln_parallel_fwd_4096.cu b/based/csrc/layer_norm/ln_parallel_fwd_4096.cu
deleted file mode 100644
index 381837e..0000000
--- a/based/csrc/layer_norm/ln_parallel_fwd_4096.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_parallel_residual_fwd_kernels.cuh"
-
-// Create forward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RYTPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG
-
-REGISTER_PARALLEL_FWD_LAUNCHER( 4096, fp32, fp32, fp32, fp32, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 4096, fp16, fp32, fp32, fp32, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 4096, fp32, fp16, fp32, fp16, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 4096, fp16, fp16, fp32, fp16, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 4096, fp32, fp16, fp16, fp16, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 4096, fp32, bf16, fp32, bf16, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 4096, bf16, bf16, fp32, bf16, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 4096, fp32, bf16, bf16, bf16, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 4096, fp16, fp16, fp16, fp16, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 4096, bf16, bf16, bf16, bf16, fp32, 1, 1, 4, 16);
diff --git a/based/csrc/layer_norm/ln_parallel_fwd_512.cu b/based/csrc/layer_norm/ln_parallel_fwd_512.cu
deleted file mode 100644
index 4ba478b..0000000
--- a/based/csrc/layer_norm/ln_parallel_fwd_512.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_parallel_residual_fwd_kernels.cuh"
-
-// Create forward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RYTPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG
-
-REGISTER_PARALLEL_FWD_LAUNCHER(  512, fp32, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER(  512, fp16, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER(  512, fp32, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER(  512, fp16, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER(  512, fp32, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER(  512, fp32, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER(  512, bf16, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER(  512, fp32, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER(  512, fp16, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER(  512, bf16, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
diff --git a/based/csrc/layer_norm/ln_parallel_fwd_5120.cu b/based/csrc/layer_norm/ln_parallel_fwd_5120.cu
deleted file mode 100644
index 7ada352..0000000
--- a/based/csrc/layer_norm/ln_parallel_fwd_5120.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_parallel_residual_fwd_kernels.cuh"
-
-// Create forward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RYTPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG
-
-REGISTER_PARALLEL_FWD_LAUNCHER( 5120, fp32, fp32, fp32, fp32, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 5120, fp16, fp32, fp32, fp32, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 5120, fp32, fp16, fp32, fp16, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 5120, fp16, fp16, fp32, fp16, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 5120, fp32, fp16, fp16, fp16, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 5120, fp32, bf16, fp32, bf16, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 5120, bf16, bf16, fp32, bf16, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 5120, fp32, bf16, bf16, bf16, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 5120, fp16, fp16, fp16, fp16, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 5120, bf16, bf16, bf16, bf16, fp32, 1, 1, 4, 16);
diff --git a/based/csrc/layer_norm/ln_parallel_fwd_6144.cu b/based/csrc/layer_norm/ln_parallel_fwd_6144.cu
deleted file mode 100644
index 6f531c8..0000000
--- a/based/csrc/layer_norm/ln_parallel_fwd_6144.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_parallel_residual_fwd_kernels.cuh"
-
-// Create forward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RYTPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG
-
-REGISTER_PARALLEL_FWD_LAUNCHER( 6144, fp32, fp32, fp32, fp32, fp32, 1, 1, 8, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 6144, fp16, fp32, fp32, fp32, fp32, 1, 1, 8, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 6144, fp32, fp16, fp32, fp16, fp32, 1, 1, 8, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 6144, fp16, fp16, fp32, fp16, fp32, 1, 1, 8, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 6144, fp32, fp16, fp16, fp16, fp32, 1, 1, 8, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 6144, fp32, bf16, fp32, bf16, fp32, 1, 1, 8, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 6144, bf16, bf16, fp32, bf16, fp32, 1, 1, 8, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 6144, fp32, bf16, bf16, bf16, fp32, 1, 1, 8, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 6144, fp16, fp16, fp16, fp16, fp32, 1, 1, 8, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 6144, bf16, bf16, bf16, bf16, fp32, 1, 1, 8, 16);
diff --git a/based/csrc/layer_norm/ln_parallel_fwd_7168.cu b/based/csrc/layer_norm/ln_parallel_fwd_7168.cu
deleted file mode 100644
index c99e752..0000000
--- a/based/csrc/layer_norm/ln_parallel_fwd_7168.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_parallel_residual_fwd_kernels.cuh"
-
-// Create forward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RYTPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG
-
-REGISTER_PARALLEL_FWD_LAUNCHER( 7168, fp32, fp32, fp32, fp32, fp32, 1, 1, 8, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 7168, fp16, fp32, fp32, fp32, fp32, 1, 1, 8, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 7168, fp32, fp16, fp32, fp16, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 7168, fp16, fp16, fp32, fp16, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 7168, fp32, fp16, fp16, fp16, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 7168, fp32, bf16, fp32, bf16, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 7168, bf16, bf16, fp32, bf16, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 7168, fp32, bf16, bf16, bf16, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 7168, fp16, fp16, fp16, fp16, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 7168, bf16, bf16, bf16, bf16, fp32, 1, 1, 4, 16);
diff --git a/based/csrc/layer_norm/ln_parallel_fwd_768.cu b/based/csrc/layer_norm/ln_parallel_fwd_768.cu
deleted file mode 100644
index f33f519..0000000
--- a/based/csrc/layer_norm/ln_parallel_fwd_768.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_parallel_residual_fwd_kernels.cuh"
-
-// Create forward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RYTPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG
-
-REGISTER_PARALLEL_FWD_LAUNCHER(  768, fp32, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER(  768, fp16, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER(  768, fp32, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER(  768, fp16, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER(  768, fp32, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER(  768, fp32, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER(  768, bf16, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER(  768, fp32, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER(  768, fp16, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER(  768, bf16, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
diff --git a/based/csrc/layer_norm/ln_parallel_fwd_8192.cu b/based/csrc/layer_norm/ln_parallel_fwd_8192.cu
deleted file mode 100644
index 360e6d4..0000000
--- a/based/csrc/layer_norm/ln_parallel_fwd_8192.cu
+++ /dev/null
@@ -1,15 +0,0 @@
-#include "ln_parallel_residual_fwd_kernels.cuh"
-
-// Create forward launch function and register. Macro signature:
-//  HIDDEN_SIZE, WTYPE, ITYPE, RYTPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG
-
-REGISTER_PARALLEL_FWD_LAUNCHER( 8192, fp32, fp32, fp32, fp32, fp32, 1, 1, 8, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 8192, fp16, fp32, fp32, fp32, fp32, 1, 1, 8, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 8192, fp32, fp16, fp32, fp16, fp32, 1, 1, 8, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 8192, fp16, fp16, fp32, fp16, fp32, 1, 1, 8, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 8192, fp32, fp16, fp16, fp16, fp32, 1, 1, 8, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 8192, fp32, bf16, fp32, bf16, fp32, 1, 1, 8, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 8192, bf16, bf16, fp32, bf16, fp32, 1, 1, 8, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 8192, fp32, bf16, bf16, bf16, fp32, 1, 1, 8, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 8192, fp16, fp16, fp16, fp16, fp32, 1, 1, 8, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 8192, bf16, bf16, bf16, bf16, fp32, 1, 1, 8, 16);
diff --git a/based/csrc/layer_norm/ln_parallel_residual_bwd_kernels.cuh b/based/csrc/layer_norm/ln_parallel_residual_bwd_kernels.cuh
deleted file mode 100644
index 5214957..0000000
--- a/based/csrc/layer_norm/ln_parallel_residual_bwd_kernels.cuh
+++ /dev/null
@@ -1,540 +0,0 @@
-#pragma once
-
-#include "ln.h"
-#include "ln_utils.cuh"
-#include "ln_kernel_traits.h"
-#include "static_switch.h"
-#include "ln_bwd_kernels.cuh"
-
-namespace layer_norm {
-
-template<typename Ktraits, bool Is_dropout, bool Tied_norm, bool Is_even_cols>
-__global__ __launch_bounds__(Ktraits::THREADS_PER_CTA) 
-void ln_parallel_residual_bwd_kernel(layer_norm::BwdParams params) {
-
-    enum { ROWS_PER_CTA = Ktraits::ROWS_PER_CTA };
-    enum { WARPS_M = Ktraits::WARPS_M };
-    enum { WARPS_N = Ktraits::WARPS_N };
-    enum { THREADS_PER_ROW = Ktraits::THREADS_PER_ROW };
-    enum { COLS = Ktraits::COLS };
-    enum { BYTES_PER_ROW = Ktraits::BYTES_PER_ROW };
-    enum { LDGS = Ktraits::LDGS };
-    enum { NUM_ELTS = Ktraits::ELTS_PER_LDG };
-    enum { THREADS_PER_WARP = Ktraits::THREADS_PER_WARP };
-    enum { CTAS_PER_ROW = Ktraits::CTAS_PER_ROW };
-
-    using input_t = typename Ktraits::input_t;
-    using compute_t = typename Ktraits::compute_t;
-    using index_t = typename Ktraits::index_t;
-    using mask_t = typename Ktraits::mask_t;
-    using Ivec = typename Ktraits::Ivec;
-    using Rvec = typename Ktraits::Rvec;
-    using Ovec = typename Ktraits::Ovec;
-    using Wvec = typename Ktraits::Wvec;
-    using Cvec = typename Ktraits::Cvec;
-    using Mvec = typename Ktraits::Mvec;
-    using Reducer = typename Ktraits::Reducer;
-    using reduce_t = typename Reducer::Type;
-
-    extern __shared__ char smem_[];
-
-    const bool has_residual = params.dresidual != nullptr;
-    const bool has_x1 = params.dx1 != nullptr;
-    const bool prenorm = params.dx != nullptr;
-
-    const index_t tidx = threadIdx.x;
-    const index_t bidn = blockIdx.x % CTAS_PER_ROW;
-    const index_t bidm = blockIdx.x / CTAS_PER_ROW;
-    const index_t lane = tidx % THREADS_PER_WARP;
-    const index_t warp = tidx / THREADS_PER_WARP;
-    const index_t warp_m = warp / Ktraits::WARPS_N;
-    const index_t warp_n = warp % Ktraits::WARPS_N;
-    const index_t tid_r = warp_n * THREADS_PER_WARP + lane;
-
-    const index_t r = bidm * Ktraits::ROWS_PER_CTA + warp_m;
-    const index_t c = bidn * THREADS_PER_ROW + warp_n * THREADS_PER_WARP + lane;
-
-    static_assert(COLS == THREADS_PER_ROW * LDGS * NUM_ELTS * CTAS_PER_ROW);
-
-    Cvec dz0y_sum[LDGS];
-    Cvec dz0_sum[LDGS];
-    Cvec dz1y_sum[LDGS];
-    Cvec dz1_sum[LDGS];
-
-    memset(dz0y_sum, 0, sizeof(dz0y_sum));
-    memset(dz0_sum, 0, sizeof(dz0_sum));
-    if (!Tied_norm) {
-        memset(dz1y_sum, 0, sizeof(dz1y_sum));
-        memset(dz1_sum, 0, sizeof(dz1_sum));
-    }
-
-    compute_t * smem_wgrad = reinterpret_cast<compute_t*>(smem_);
-    char *smem_dgrad = smem_ + Ktraits::SMEM_BYTES_WGRAD;
-
-    Reducer reducer(params, bidm, bidn, warp_m, warp_n, lane, smem_dgrad);
-
-    Sum<reduce_t> sum;
-
-    const index_t num_valid_ldgs =
-        ((params.cols / Ktraits::ELTS_PER_LDG) - 1 - c + Ktraits::VEC_COLS_PER_LDG) / Ktraits::VEC_COLS_PER_LDG;
-
-    Wvec gamma0[LDGS];
-    Wvec gamma1[LDGS];
-    index_t idx = c;
-    #pragma unroll
-    for( int it = 0; it < LDGS; it++ ) {
-        if (Is_even_cols || (it < num_valid_ldgs)) {
-            gamma0[it].load_from(params.gamma, idx);
-            if (!Tied_norm) { gamma1[it].load_from(params.gamma1, idx); }
-            idx += Ktraits::VEC_COLS_PER_LDG;
-        }
-    }
-    // TODO if ROWS_PER_CTA does not divide rows, we might get divergence in the
-    // last blocks with syncthreads!
-    // grid stride over rows
-    #pragma unroll 1
-    for( int row = r; row < params.rows; row += params.ctas_per_col * ROWS_PER_CTA ) {
-        const compute_t mu_r = static_cast<const compute_t *>(params.mu)[row];
-        const compute_t rs_r = static_cast<const compute_t *>(params.rs)[row];
-        Mvec dmask0[LDGS], dmask1[LDGS];
-        Rvec dx[LDGS];
-        compute_t dy[LDGS * NUM_ELTS];
-        compute_t y[LDGS * NUM_ELTS];
-        compute_t mdy_local = 0.f;
-        compute_t mdyy_local = 0.f;
-        index_t idx = row * params.cols / Ktraits::ELTS_PER_LDG + c;
-        #pragma unroll
-        for( int it = 0; it < LDGS; it++ ) {
-            if (Is_even_cols || (it < num_valid_ldgs)) {
-                Rvec x;
-                Ovec dz0, dz1;
-                dz0.load_from(params.dz, idx);
-                if (!Tied_norm) { dz1.load_from(params.dz1, idx); }
-                if (prenorm) { dx[it].load_from(params.dx, idx); }
-                x.load_from(params.x, idx);
-                if (Is_dropout) {
-                    dmask0[it].load_from(params.dmask, idx);
-                    if (has_x1) { dmask1[it].load_from(params.dmask1, idx); }
-                }
-                idx += Ktraits::VEC_COLS_PER_LDG;
-                #pragma unroll
-                for( int jt = 0; jt < NUM_ELTS; jt++ ) {
-                    compute_t x_tmp = x.data.elt[jt];
-                    compute_t y_tmp = rs_r * (x_tmp - (!params.is_rms_norm ? mu_r : 0.f));
-                    compute_t dy_tmp = compute_t(gamma0[it].data.elt[jt]) * compute_t(dz0.data.elt[jt]);
-                    if (!Tied_norm) {
-                        dy_tmp += compute_t(gamma1[it].data.elt[jt]) * compute_t(dz1.data.elt[jt]);
-                    }
-                    compute_t dz0_tmp = dz0.data.elt[jt];
-                    compute_t dz1_tmp;
-                    if (!Tied_norm) { dz1_tmp = dz1.data.elt[jt]; }
-
-                    mdy_local += dy_tmp;
-                    mdyy_local += dy_tmp * y_tmp;
-
-                    dy[it * NUM_ELTS + jt] = dy_tmp;
-                    y[it * NUM_ELTS + jt] = y_tmp;
-
-                    dz0y_sum[it].data.elt[jt] += dz0_tmp * y_tmp;
-                    dz0_sum[it].data.elt[jt] += dz0_tmp;
-                    if (!Tied_norm) {
-                        dz1y_sum[it].data.elt[jt] += dz1_tmp * y_tmp;
-                        dz1_sum[it].data.elt[jt] += dz1_tmp;
-                    }
-                }
-            }
-        }
-
-        reduce_t result = reducer.allreduce({mdy_local, mdyy_local}, sum);
-        mdy_local = layer_norm::Get<0>::of<reduce_t, compute_t>(result) * params.inverse_cols;
-        mdyy_local = layer_norm::Get<1>::of<reduce_t, compute_t>(result) * params.inverse_cols;
-
-        idx = row * params.cols / Ktraits::ELTS_PER_LDG + c;
-        #pragma unroll
-        for( int it = 0; it < LDGS; it++ ) {
-            if (Is_even_cols || (it < num_valid_ldgs)) {
-                Ivec dx0, dx1;
-                Rvec dresidual;
-                #pragma unroll
-                for( int jt = 0; jt < NUM_ELTS; jt++ ) {
-                    compute_t dx_tmp_res;
-                    compute_t dy_tmp = dy[it * NUM_ELTS + jt];
-                    compute_t y_tmp = y[it * NUM_ELTS + jt];
-                    compute_t dx_tmp = rs_r * (dy_tmp - (mdyy_local * y_tmp + (!params.is_rms_norm ? mdy_local : 0.f)));
-                    dx_tmp_res = prenorm ? dx_tmp + compute_t(dx[it].data.elt[jt]) : dx_tmp;
-                    if (has_residual) { dresidual.data.elt[jt] = dx_tmp_res; }
-                    if (Is_dropout) {
-                        dx0.data.elt[jt] = dmask0[it].data.elt[jt] ? dx_tmp_res * params.dropout_scale : 0.f;
-                        if (has_x1) { dx1.data.elt[jt] = dmask1[it].data.elt[jt] ? dx_tmp_res * params.dropout_scale : 0.f; }
-                    } else {
-                        dx0.data.elt[jt] = dx_tmp_res;
-                        if (has_x1) { dx1.data.elt[jt] = dx_tmp_res; }
-                    }
-                }
-                if (has_residual) { dresidual.store_to(params.dresidual, idx); }
-                dx0.store_to(params.dx0, idx);
-                if (has_x1) { dx1.store_to(params.dx1, idx); }
-                idx += Ktraits::VEC_COLS_PER_LDG;
-            }
-        }
-
-    }  // end: grid stride loop
-
-    if( WARPS_M == 1 ) {
-        idx = r * params.cols / Ktraits::ELTS_PER_LDG + c;
-        #pragma unroll
-        for( int it = 0; it < LDGS; it++ ) {
-            if (Is_even_cols || (it < num_valid_ldgs)) {
-                dz0_sum[it].store_to(params.dbeta_part, idx);
-                dz0y_sum[it].store_to(params.dgamma_part, idx);
-                if (!Tied_norm) {
-                    dz1_sum[it].store_to(params.dbeta1_part, idx);
-                    dz1y_sum[it].store_to(params.dgamma1_part, idx);
-                }
-                idx += Ktraits::VEC_COLS_PER_LDG;
-            }
-        }
-    } else {
-        static_assert(WARPS_M == 1 || Ktraits::CTAS_PER_ROW == 1, "Multiple rows per CTA not supported for Multi-CTA.");
-        // Finalize reduction of part dgamma and dbeta for this CTA
-        // by reducing over the rows held across the WARPS_M warps
-
-        // Assumption: blockSize divides hidden size.
-        enum { NUM_RES = COLS / Ktraits::THREADS_PER_CTA };
-        static_assert(NUM_RES * Ktraits::THREADS_PER_CTA == COLS, "");
-
-        idx = warp_m * Ktraits::VEC_COLS + tid_r;
-        #pragma unroll
-        for( int it = 0; it < LDGS; it++ ) {
-            dz0_sum[it].store_to(smem_wgrad, idx);
-            idx += THREADS_PER_ROW;
-        }
-        __syncthreads();
-        compute_t cta_dz0_sum[NUM_RES];
-        memset(cta_dz0_sum, 0, sizeof(compute_t) * NUM_RES);
-        for( int it = 0; it < ROWS_PER_CTA; it++ ) {
-            for( int jt = 0; jt < NUM_RES; jt++ ) {
-                cta_dz0_sum[jt] += smem_wgrad[it * COLS + tidx + jt * Ktraits::THREADS_PER_CTA];
-            }
-        }
-        __syncthreads();
-
-        idx = warp_m * Ktraits::VEC_COLS + tid_r;
-        #pragma unroll
-        for( int it = 0; it < LDGS; it++ ) {
-            dz0y_sum[it].store_to(smem_wgrad, idx);
-            idx += THREADS_PER_ROW;
-        }
-        __syncthreads();
-        compute_t cta_dz0y_sum[NUM_RES];
-        memset(cta_dz0y_sum, 0, sizeof(compute_t) * NUM_RES);
-        for( int it = 0; it < ROWS_PER_CTA; it++ ) {
-            for( int jt = 0; jt < NUM_RES; jt++ ) {
-                cta_dz0y_sum[jt] += smem_wgrad[it * COLS + tidx + jt * Ktraits::THREADS_PER_CTA];
-            }
-        }
-
-        compute_t cta_dz1_sum[NUM_RES], cta_dz1y_sum[NUM_RES];
-        if (!Tied_norm) {
-            __syncthreads();
-            idx = warp_m * Ktraits::VEC_COLS + tid_r;
-            #pragma unroll
-            for( int it = 0; it < LDGS; it++ ) {
-                dz1_sum[it].store_to(smem_wgrad, idx);
-                idx += THREADS_PER_ROW;
-            }
-            __syncthreads();
-            memset(cta_dz1_sum, 0, sizeof(compute_t) * NUM_RES);
-            for( int it = 0; it < ROWS_PER_CTA; it++ ) {
-                for( int jt = 0; jt < NUM_RES; jt++ ) {
-                    cta_dz1_sum[jt] += smem_wgrad[it * COLS + tidx + jt * Ktraits::THREADS_PER_CTA];
-                }
-            }
-            __syncthreads();
-            idx = warp_m * Ktraits::VEC_COLS + tid_r;
-            #pragma unroll
-            for( int it = 0; it < LDGS; it++ ) {
-                dz1y_sum[it].store_to(smem_wgrad, idx);
-                idx += THREADS_PER_ROW;
-            }
-            __syncthreads();
-            memset(cta_dz1y_sum, 0, sizeof(compute_t) * NUM_RES);
-            for( int it = 0; it < ROWS_PER_CTA; it++ ) {
-                for( int jt = 0; jt < NUM_RES; jt++ ) {
-                    cta_dz1y_sum[jt] += smem_wgrad[it * COLS + tidx + jt * Ktraits::THREADS_PER_CTA];
-                }
-            }
-        }
-
-        const index_t num_valid_writes
-            = (params.cols - 1 - tidx + Ktraits::THREADS_PER_CTA) / Ktraits::THREADS_PER_CTA;
-        compute_t *dgamma0_part = static_cast<compute_t *>(params.dgamma_part) + bidm * params.cols + tidx;
-        compute_t *dbeta0_part = static_cast<compute_t *>(params.dbeta_part) + bidm * params.cols + tidx;
-        compute_t *dgamma1_part = !Tied_norm ? static_cast<compute_t *>(params.dgamma1_part) + bidm * params.cols + tidx : nullptr;
-        compute_t *dbeta1_part = !Tied_norm ? static_cast<compute_t *>(params.dbeta1_part) + bidm * params.cols + tidx : nullptr;
-        for( int jt = 0; jt < NUM_RES; jt++ ) {
-            if (Is_even_cols || (jt < num_valid_writes)) {
-                *dgamma0_part = cta_dz0y_sum[jt];
-                dgamma0_part += Ktraits::THREADS_PER_CTA;
-                *dbeta0_part = cta_dz0_sum[jt];
-                dbeta0_part += Ktraits::THREADS_PER_CTA;
-                if (!Tied_norm) {
-                    *dgamma1_part = cta_dz1y_sum[jt];
-                    dgamma1_part += Ktraits::THREADS_PER_CTA;
-                    *dbeta1_part = cta_dz1_sum[jt];
-                    dbeta1_part += Ktraits::THREADS_PER_CTA;
-                }
-            }
-        }
-
-    }
-}
-
-template<typename Kernel_traits, bool Is_even_cols>
-__global__ __launch_bounds__(Kernel_traits::THREADS_PER_CTA)
-void ln_parallel_residual_bwd_finalize_kernel(BwdParams params)
-{
-
-    using compute_t = typename Kernel_traits::compute_t;
-    using weight_t = typename Kernel_traits::weight_t;
-    using index_t = typename Kernel_traits::index_t;
-    using Reducer = typename Kernel_traits::Reducer;
-    using reduce_t = typename Reducer::Type;
-
-    Sum<reduce_t> sum;
-    enum { NUM_ELT = Kernel_traits::ELTS_PER_LDG };
-    enum { THREADS_PER_WARP = Kernel_traits::THREADS_PER_WARP };
-
-    // Multiplying by 2 since we have both gamma0 and gamma1
-    __shared__ char smem_[2 * Kernel_traits::SMEM_BYTES_PER_CTA];
-
-    constexpr uint32_t bidm = 0;
-
-    const uint32_t bidn = blockIdx.x;
-    const uint32_t tidx = threadIdx.x;
-    const uint32_t warp = tidx / THREADS_PER_WARP;
-    const uint32_t lane = tidx % THREADS_PER_WARP;
-
-    Reducer reducer(params, bidm, bidn, 0, 0, lane, smem_);
-
-    const uint32_t c = bidn * THREADS_PER_WARP + lane;
-    const uint32_t c_out = bidn * THREADS_PER_WARP / 2 + lane;
-    constexpr uint32_t COL_STRIDE = Kernel_traits::CTAS * THREADS_PER_WARP;
-    for( uint32_t col = c, col_out = c_out; col < Kernel_traits::COLS; col += COL_STRIDE, col_out += COL_STRIDE / 2 ) {
-        // Each thread sums over NUM_ELT columns.
-        Vec<compute_t, NUM_ELT> dbeta0_local, dgamma0_local, dbeta1_local, dgamma1_local;
-        memset(&dgamma0_local, 0, sizeof(dgamma0_local));
-        memset(&dbeta0_local, 0, sizeof(dbeta0_local));
-        memset(&dgamma1_local, 0, sizeof(dgamma1_local));
-        memset(&dbeta1_local, 0, sizeof(dbeta1_local));
-        if (Is_even_cols || col < params.cols) {
-            for( uint32_t row = warp; row < params.ctas_per_col; row += Kernel_traits::ROWS_PER_CTA ) {
-                index_t idx = row * params.cols + col;
-
-                Vec<compute_t, NUM_ELT> dbeta0_part, dgamma0_part, dbeta1_part, dgamma1_part;
-                dbeta0_part.load_from(params.dbeta_part, idx);
-                dgamma0_part.load_from(params.dgamma_part, idx);
-                dbeta1_part.load_from(params.dbeta1_part, idx);
-                dgamma1_part.load_from(params.dgamma1_part, idx);
-                #pragma unroll
-                for( int it = 0; it < NUM_ELT; it++ ) {
-                    dgamma0_local.data.elt[it] += dgamma0_part.data.elt[it];
-                    dbeta0_local.data.elt[it] += dbeta0_part.data.elt[it];
-                    dgamma1_local.data.elt[it] += dgamma1_part.data.elt[it];
-                    dbeta1_local.data.elt[it] += dbeta1_part.data.elt[it];
-                }
-            }
-        }
-        void * smem_gamma0 = smem_;
-        void * smem_beta0 = &smem_[Kernel_traits::SMEM_BYTES_TRANSPOSE];
-        void * smem_gamma1 = &smem_[2 * Kernel_traits::SMEM_BYTES_TRANSPOSE];
-        void * smem_beta1 = &smem_[3 * Kernel_traits::SMEM_BYTES_TRANSPOSE];
-
-        const int write_row = warp;
-        const int write_col = lane ^ write_row;
-        const int write_idx = write_row * THREADS_PER_WARP + write_col;
-
-        dgamma0_local.store_to(smem_gamma0, write_idx);
-        dbeta0_local.store_to(smem_beta0, write_idx);
-        dgamma1_local.store_to(smem_gamma1, write_idx);
-        dbeta1_local.store_to(smem_beta1, write_idx);
-
-        __syncthreads();
-
-        // It would be probably safe to reuse the first row of smem_beta0 and smem_gamma0
-        void * smem_gamma0_out = &smem_[4 * Kernel_traits::SMEM_BYTES_TRANSPOSE];
-        void * smem_beta0_out = &smem_[4 * Kernel_traits::SMEM_BYTES_TRANSPOSE + Kernel_traits::SMEM_BYTES_OUTPUT];
-        void * smem_gamma1_out = &smem_[4 * Kernel_traits::SMEM_BYTES_TRANSPOSE + 2 * Kernel_traits::SMEM_BYTES_OUTPUT];
-        void * smem_beta1_out = &smem_[4 * Kernel_traits::SMEM_BYTES_TRANSPOSE + 3 * Kernel_traits::SMEM_BYTES_OUTPUT];
-
-        // More than one iter iff ROWS_PER_CTA < 32.
-        for( int w = warp; w < THREADS_PER_WARP; w += Kernel_traits::ROWS_PER_CTA ) {
-            const int read_row = lane;
-            const int read_col = w ^ read_row;
-            const int read_idx = read_row * THREADS_PER_WARP + read_col;
-
-            memset(&dbeta0_local, 0, sizeof(dbeta0_local));
-            memset(&dgamma0_local, 0, sizeof(dgamma0_local));
-            memset(&dbeta1_local, 0, sizeof(dbeta1_local));
-            memset(&dgamma1_local, 0, sizeof(dgamma1_local));
-
-            // Load beta and gamma transposed
-            if(read_row < Kernel_traits::ROWS_PER_CTA){
-                dbeta0_local.load_from(smem_beta0, read_idx);
-                dgamma0_local.load_from(smem_gamma0, read_idx);
-                dbeta1_local.load_from(smem_beta1, read_idx);
-                dgamma1_local.load_from(smem_gamma1, read_idx);
-            }
-
-            // Call reducer on the loaded value(s) and convert.
-            #pragma unroll
-            for( int it = 0; it < NUM_ELT; it++ ) {
-                compute_t b0_i = dbeta0_local.data.elt[it];
-                compute_t g0_i = dgamma0_local.data.elt[it];
-                compute_t b1_i = dbeta1_local.data.elt[it];
-                compute_t g1_i = dgamma1_local.data.elt[it];
-                b0_i = reducer.allreduce(b0_i, sum);
-                g0_i = reducer.allreduce(g0_i, sum);
-                b1_i = reducer.allreduce(b1_i, sum);
-                g1_i = reducer.allreduce(g1_i, sum);
-
-                dgamma0_local.data.elt[it] = g0_i;
-                dbeta0_local.data.elt[it] = b0_i;
-                dgamma1_local.data.elt[it] = g1_i;
-                dbeta1_local.data.elt[it] = b1_i;
-            }
-
-            // Leader stores the result at the current column.
-            if(lane == 0){
-                dgamma0_local.store_to(smem_gamma0_out, w);
-                dbeta0_local.store_to(smem_beta0_out, w);
-                dgamma1_local.store_to(smem_gamma1_out, w);
-                dbeta1_local.store_to(smem_beta1_out, w);
-            }
-
-        }
-
-        // All writes done.
-        __syncthreads();
-
-        // Pack and store: 2-wide stores with half the threads.
-        if (Is_even_cols || col_out * 2 < params.cols) {
-            if( warp == Kernel_traits::ROWS_PER_CTA - 1 && lane < THREADS_PER_WARP / 2 ) {
-
-                using src_t = typename TypeToVec2<compute_t>::Type;
-                using dst_t = typename TypeToVec2<weight_t>::Type;
-                Vec<src_t, NUM_ELT> dbeta0_vec2, dgamma0_vec2, dbeta1_vec2, dgamma1_vec2;
-                Vec<dst_t, NUM_ELT> dbeta0_out2, dgamma0_out2, dbeta1_out2, dgamma1_out2;
-
-                dgamma0_vec2.load_from(smem_gamma0_out, lane);
-                dbeta0_vec2.load_from(smem_beta0_out, lane);
-                dgamma1_vec2.load_from(smem_gamma1_out, lane);
-                dbeta1_vec2.load_from(smem_beta1_out, lane);
-                #pragma unroll
-                for( int it = 0; it < NUM_ELT; it++ ) {
-                    dgamma0_out2.data.elt[it] = Converter<src_t,dst_t>::convert(dgamma0_vec2.data.elt[it]);
-                    dbeta0_out2.data.elt[it] = Converter<src_t,dst_t>::convert(dbeta0_vec2.data.elt[it]);
-                    dgamma1_out2.data.elt[it] = Converter<src_t,dst_t>::convert(dgamma1_vec2.data.elt[it]);
-                    dbeta1_out2.data.elt[it] = Converter<src_t,dst_t>::convert(dbeta1_vec2.data.elt[it]);
-                }
-                dgamma0_out2.store_to(params.dgamma, col_out);
-                dbeta0_out2.store_to(params.dbeta, col_out);
-                dgamma1_out2.store_to(params.dgamma1, col_out);
-                dbeta1_out2.store_to(params.dbeta1, col_out);
-            }
-        }
-    }
-}
-
-}  // namespace layer_norm
-
-using namespace layer_norm;
-
-template<
-    typename weight_t,
-    typename input_t,
-    typename residual_t,
-    typename output_t,
-    typename compute_t,
-    typename index_t,
-    int HIDDEN_SIZE,
-    int CTAS_PER_ROW,
-    int WARPS_M,
-    int WARPS_N,
-    int BYTES_PER_LDG_MAIN,
-    int BYTES_PER_LDG_FINAL
->
-void launch_parallel_residual_(LaunchParams<BwdParams> &launch_params, const bool configure_params){
-
-    using Kernel_traits = Kernel_traits<weight_t,
-                                        input_t,
-                                        residual_t,
-                                        output_t,
-                                        compute_t,
-                                        index_t,
-                                        HIDDEN_SIZE,
-                                        CTAS_PER_ROW,
-                                        WARPS_M,
-                                        WARPS_N,
-                                        BYTES_PER_LDG_MAIN
-                                        >;
-    bool is_dropout = launch_params.params.dropout_keep_p < 1.f;
-    bool tied_norm = launch_params.params.gamma1 == nullptr;
-    bool is_even_cols = launch_params.params.cols == HIDDEN_SIZE;
-    BOOL_SWITCH(is_dropout, IsDropoutConst, [&] {
-        BOOL_SWITCH(tied_norm, TiedNormConst, [&] {
-            BOOL_SWITCH(is_even_cols, IsEvenColsConst, [&] {
-                auto kernel = &ln_parallel_residual_bwd_kernel<Kernel_traits, IsDropoutConst, TiedNormConst, IsEvenColsConst>;
-                if( configure_params ) {
-                    int ctas_per_sm;
-                    CHECK_CUDA(cudaOccupancyMaxActiveBlocksPerMultiprocessor(
-                        &ctas_per_sm, kernel, Kernel_traits::THREADS_PER_CTA, Kernel_traits::SMEM_BYTES));
-                    launch_params.params.ctas_per_col = launch_params.props->multiProcessorCount * ctas_per_sm / Kernel_traits::CTAS_PER_ROW;
-                    launch_params.barrier_size = 0;
-                    launch_params.workspace_bytes = 0;
-                    if(Kernel_traits::CTAS_PER_ROW > 1) {
-                        launch_params.barrier_size = 2 * launch_params.params.ctas_per_col;
-                        launch_params.workspace_bytes = launch_params.params.ctas_per_col
-                                                      * Kernel_traits::WARPS_M
-                                                      * Kernel_traits::CTAS_PER_ROW
-                                                      * sizeof(typename Kernel_traits::reduce_t)
-                                                      * 2;
-                    }
-                    return;
-                }
-
-                if( Kernel_traits::SMEM_BYTES >= 48 * 1024 ) {
-                    CHECK_CUDA(cudaFuncSetAttribute(kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, Kernel_traits::SMEM_BYTES));
-                }
-                auto stream = launch_params.stream;
-                auto ctas_per_col = launch_params.params.ctas_per_col;
-
-                if( Kernel_traits::CTAS_PER_ROW == 1 ) {
-                    kernel<<<ctas_per_col, Kernel_traits::THREADS_PER_CTA, Kernel_traits::SMEM_BYTES, stream>>>(launch_params.params);
-                } else {
-                    dim3 grid(Kernel_traits::CTAS_PER_ROW * ctas_per_col);
-                    dim3 block(Kernel_traits::THREADS_PER_CTA);
-                    void *params_ = (void *)&launch_params.params;
-                    cudaLaunchCooperativeKernel((void *)kernel, grid, block, (void **)&params_, Kernel_traits::SMEM_BYTES, stream);
-                }
-
-                using Kernel_traits_f = layer_norm::Kernel_traits_finalize<HIDDEN_SIZE,
-                                                                          weight_t,
-                                                                          input_t,
-                                                                          residual_t,
-                                                                          output_t,
-                                                                          compute_t,
-                                                                          index_t,
-                                                                          /*HasColscaleConst=*/false,
-                                                                          32 * 32,  // THREADS_PER_CTA
-                                                                          BYTES_PER_LDG_FINAL>;
-
-                auto kernel_f = !TiedNormConst
-                    ? &layer_norm::ln_parallel_residual_bwd_finalize_kernel<Kernel_traits_f, IsEvenColsConst>
-                    : &layer_norm::ln_bwd_finalize_kernel<Kernel_traits_f, /*HasColscaleConst=*/false, IsEvenColsConst>;
-                kernel_f<<<Kernel_traits_f::CTAS, Kernel_traits_f::THREADS_PER_CTA, 0, stream>>>(launch_params.params);
-
-            });
-        });
-    });
-}
diff --git a/based/csrc/layer_norm/ln_parallel_residual_fwd_kernels.cuh b/based/csrc/layer_norm/ln_parallel_residual_fwd_kernels.cuh
deleted file mode 100644
index 0e55cb4..0000000
--- a/based/csrc/layer_norm/ln_parallel_residual_fwd_kernels.cuh
+++ /dev/null
@@ -1,281 +0,0 @@
-#pragma once
-
-#ifdef OLD_GENERATOR_PATH
-#include <ATen/CUDAGeneratorImpl.h>
-#else
-#include <ATen/cuda/CUDAGeneratorImpl.h>
-#endif
-
-#include <ATen/cuda/detail/UnpackRaw.cuh>  // For at::cuda::philox::unpack
-#include <curand_kernel.h>
-
-#include "ln.h"
-#include "ln_utils.cuh"
-#include "ln_kernel_traits.h"
-#include "static_switch.h"
-
-namespace layer_norm {
-
-template<typename Ktraits, bool Is_dropout, bool Tied_norm, bool Is_even_cols>
-__global__ __launch_bounds__(Ktraits::THREADS_PER_CTA) 
-void ln_parallel_residual_fwd_kernel(FwdParams params) {
-
-    enum { ROWS_PER_CTA = Ktraits::ROWS_PER_CTA };
-    enum { WARPS_N = Ktraits::WARPS_N };
-    enum { WARPS_M = Ktraits::WARPS_M };
-    enum { THREADS_PER_ROW = Ktraits::THREADS_PER_ROW };
-    enum { VEC_COLS_PER_LDG = Ktraits::VEC_COLS_PER_LDG };
-    enum { BYTES_PER_ROW = Ktraits::BYTES_PER_ROW };
-    enum { LDGS = Ktraits::LDGS };
-    enum { NUM_ELTS = Ktraits::NUM_ELTS };
-    enum { CTAS_PER_ROW = Ktraits::CTAS_PER_ROW };
-
-    using input_t = typename Ktraits::input_t;
-    using residual_t = typename Ktraits::residual_t;
-    using output_t = typename Ktraits::output_t;
-    using index_t = typename Ktraits::index_t;
-    using compute_t = typename Ktraits::compute_t;
-    using mask_t = typename Ktraits::mask_t;
-    using Ivec = typename Ktraits::Ivec;
-    using Rvec = typename Ktraits::Rvec;
-    using Ovec = typename Ktraits::Ovec;
-    using Wvec = typename Ktraits::Wvec;
-    using Cvec = typename Ktraits::Cvec;
-    using Mvec = typename Ktraits::Mvec;
-
-    using Stats = typename Ktraits::Stats;
-    using stats_t = typename Stats::stats_t;
-
-    const bool has_residual = params.residual != nullptr;
-    const bool has_x1 = params.x1 != nullptr;
-    const bool save_x = has_residual || has_x1 || Is_dropout || !(std::is_same<input_t, residual_t>::value);
-
-    extern __shared__ char smem_[];
-
-    const index_t tidx = threadIdx.x;
-    const index_t bidn = blockIdx.x % CTAS_PER_ROW;
-    const index_t bidm = blockIdx.x / CTAS_PER_ROW;
-    const index_t lane = tidx % THREADS_PER_WARP;
-    const index_t warp = tidx / THREADS_PER_WARP;
-    const index_t warp_m = warp / WARPS_N;
-    const index_t warp_n = warp % WARPS_N;
-
-    const index_t r = bidm * ROWS_PER_CTA + warp_m;
-    const index_t c = bidn * THREADS_PER_ROW + warp_n * THREADS_PER_WARP + lane;
-
-    Stats stats(params, bidm, bidn, warp_m, warp_n, lane, smem_);
-
-    compute_t *mu_ptr = static_cast<compute_t *>(params.mu);
-    compute_t *rs_ptr = static_cast<compute_t *>(params.rs);
-
-    // https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/Dropout.cu
-    curandStatePhilox4_32_10_t state;
-    if (Is_dropout) {
-        auto seeds = at::cuda::philox::unpack(params.philox_args);
-        const index_t tidx_global = blockIdx.x * blockDim.x + threadIdx.x;
-        curand_init(std::get<0>(seeds), tidx_global, std::get<1>(seeds), &state);
-    }
-
-    const index_t num_valid_ldgs = ((params.cols / Ktraits::ELTS_PER_LDG) - 1 - c + VEC_COLS_PER_LDG) / VEC_COLS_PER_LDG;
-
-    Wvec gamma0[LDGS];
-    Wvec beta0[LDGS];
-    Wvec gamma1[LDGS];
-    Wvec beta1[LDGS];
-    index_t idx = c;
-    #pragma unroll
-    for( int it = 0; it < LDGS; it++ ) {
-        if (Is_even_cols || (it < num_valid_ldgs)) {
-            gamma0[it].load_from(params.gamma, idx);
-            if (params.beta != nullptr) {
-                beta0[it].load_from(params.beta, idx);
-            } else {
-                beta0[it].zero_();
-            }
-            if (!Tied_norm) {
-                gamma1[it].load_from(params.gamma1, idx);
-                if (params.beta1 != nullptr) {
-                    beta1[it].load_from(params.beta1, idx);
-                } else {
-                    beta1[it].zero_();
-                }
-            }
-            idx += VEC_COLS_PER_LDG;
-        }
-    }
-
-    for( int row = r; row < params.rows; row += params.ctas_per_col * ROWS_PER_CTA ) {
-        index_t idx = row * params.cols / Ktraits::ELTS_PER_LDG + c;
-        compute_t xf[LDGS * NUM_ELTS];
-        #pragma unroll
-        for( int it = 0; it < LDGS; it++ ) {
-            if (Is_even_cols || (it < num_valid_ldgs)) {
-                Ivec x0;
-                Ivec x1;
-                Rvec residual;
-                Rvec x;
-                Mvec dmask0;
-                Mvec dmask1;
-                x0.load_from(params.x0, idx);
-                if (has_x1) { x1.load_from(params.x1, idx); }
-                if (has_residual) { residual.load_from(params.residual, idx); }
-                #pragma unroll
-                for( int jt = 0; jt < NUM_ELTS; jt++ ) {
-                    // TD [2022-04-22]: We're memory bound, not compute bound, so we don't need to use
-                    // the more efficient curand_uniform4.
-                    compute_t x_ij;
-                    mask_t keep0 = !Is_dropout ? true : curand_uniform(&state) <= params.dropout_keep_p;
-                    if (Is_dropout) { dmask0.data.elt[jt] = keep0; }
-                    compute_t x0_ij = compute_t(x0.data.elt[jt]);
-                    x0_ij = keep0 ? (Is_dropout ? x0_ij * params.dropout_scale : x0_ij) : 0.0f;
-                    if (has_x1) {
-                        mask_t keep1 = !Is_dropout ? true : curand_uniform(&state) <= params.dropout_keep_p;
-                        if (Is_dropout) { dmask1.data.elt[jt] = keep1; }
-                        compute_t x1_ij = compute_t(x1.data.elt[jt]);
-                        x1_ij = keep1 ? (Is_dropout ? x1_ij * params.dropout_scale : x1_ij) : 0.0f;
-                        x_ij = has_residual ? x0_ij + x1_ij + compute_t(residual.data.elt[jt]) : x0_ij + x1_ij;
-                    } else {
-                        x_ij = has_residual ? x0_ij + compute_t(residual.data.elt[jt]) : x0_ij;
-                    }
-                    if (save_x) { x.data.elt[jt] = x_ij; }
-                    xf[it * NUM_ELTS + jt] = x_ij;
-                }
-                if (save_x) { x.store_to(params.x, idx); }
-                if (Is_dropout) {
-                    dmask0.store_to(params.dmask, idx);
-                    if (has_x1) { dmask1.store_to(params.dmask1, idx); }
-                }
-                idx += VEC_COLS_PER_LDG;
-            }
-        }
-
-        static_assert(CTAS_PER_ROW == 1, "Don't support multiple CTAs per row for now");
-        const index_t num_vecs = params.cols / Ktraits::ELTS_PER_LDG;
-        const index_t num_full_ldgs = num_vecs / Ktraits::VEC_COLS_PER_LDG;
-        const index_t remaining_vecs = num_vecs % Ktraits::VEC_COLS_PER_LDG;
-        auto valid_elts_in_warp_fn = [num_full_ldgs, remaining_vecs] (int warp_n) -> int {
-            // Need to convert to int, otherwise the subtraction will wrap around.
-            const index_t valid_partial_vecs_in_warp =
-                std::min(std::max(int(remaining_vecs) - int(warp_n * THREADS_PER_WARP), int(0)),
-                        int(THREADS_PER_WARP));
-            return (num_full_ldgs * THREADS_PER_WARP + valid_partial_vecs_in_warp) * NUM_ELTS;
-        };
-        stats_t s = stats.template compute<Is_even_cols>(
-            xf, params.inverse_cols, valid_elts_in_warp_fn, num_valid_ldgs * NUM_ELTS
-        );
-
-        compute_t mu = layer_norm::Get<0>::of<stats_t, compute_t>(s);
-        compute_t m2 = layer_norm::Get<1>::of<stats_t, compute_t>(s);
-
-        if( bidn == 0 && warp_n == 0 && lane == 0 ) {
-            mu_ptr[row] = mu;
-        }
-
-        compute_t rs = rsqrtf(m2 * params.inverse_cols + params.epsilon + (!params.is_rms_norm ? 0.f : mu * mu));
-
-        if( bidn == 0 && warp_n == 0 && lane == 0 ) {
-            rs_ptr[row] = rs;
-        }
-
-        idx = row * params.cols / Ktraits::ELTS_PER_LDG + c;
-        #pragma unroll
-        for( int it = 0; it < LDGS; it++ ) {
-            if (Is_even_cols || (it < num_valid_ldgs)) {
-                Ovec z0;
-                Ovec z1;
-                #pragma unroll
-                for( int jt = 0; jt < NUM_ELTS; jt++ ) {
-                    compute_t y_ij = compute_t(rs * (xf[it * NUM_ELTS + jt] - (!params.is_rms_norm ? mu : 0.f)));
-                    compute_t g0_ij = gamma0[it].data.elt[jt];
-                    compute_t b0_ij = beta0[it].data.elt[jt];
-                    z0.data.elt[jt] = output_t(g0_ij * y_ij + b0_ij);
-                    if (!Tied_norm) {
-                        compute_t g1_ij = gamma1[it].data.elt[jt];
-                        compute_t b1_ij = beta1[it].data.elt[jt];
-                        z1.data.elt[jt] = output_t(g1_ij * y_ij + b1_ij);
-                    }
-                }
-                z0.store_to(params.z, idx);
-                if (!Tied_norm) { z1.store_to(params.z1, idx); }
-                idx += VEC_COLS_PER_LDG;
-            }
-        }
-
-    }
-}
-
-}  // namespace layer_norm
-
-using namespace layer_norm;
-
-template<
-    typename weight_t,
-    typename input_t,
-    typename residual_t,
-    typename output_t,
-    typename compute_t,
-    typename index_t,
-    int HIDDEN_SIZE,
-    int CTAS_PER_ROW,
-    int WARPS_M,
-    int WARPS_N,
-    int BYTES_PER_LDG
->
-void launch_parallel_residual_(LaunchParams<FwdParams> &launch_params, const bool configure_params){
-
-    using Kernel_traits = Kernel_traits<weight_t,
-                                        input_t,
-                                        residual_t,
-                                        output_t,
-                                        compute_t,
-                                        index_t,
-                                        HIDDEN_SIZE,
-                                        CTAS_PER_ROW,
-                                        WARPS_M,
-                                        WARPS_N,
-                                        BYTES_PER_LDG
-                                        >;
-    bool is_even_cols = launch_params.params.cols == HIDDEN_SIZE;
-    bool tied_norm = launch_params.params.gamma1 == nullptr;
-    BOOL_SWITCH(launch_params.params.dropout_keep_p < 1.f, IsDropoutConst, [&] {
-        BOOL_SWITCH(tied_norm, TiedNormConst, [&] {
-            BOOL_SWITCH(is_even_cols, IsEvenColsConst, [&] {
-                auto kernel = &ln_parallel_residual_fwd_kernel<Kernel_traits, IsDropoutConst, TiedNormConst, IsEvenColsConst>;
-                if( configure_params ) {
-                    int ctas_per_sm;
-                    CHECK_CUDA(cudaOccupancyMaxActiveBlocksPerMultiprocessor(
-                        &ctas_per_sm, kernel, Kernel_traits::THREADS_PER_CTA, Kernel_traits::SMEM_BYTES_FWD));
-                    launch_params.params.ctas_per_col = launch_params.props->multiProcessorCount * ctas_per_sm / Kernel_traits::CTAS_PER_ROW;
-                    const size_t rows_per_loop = launch_params.params.ctas_per_col * Kernel_traits::ROWS_PER_CTA;
-                    launch_params.elts_per_thread = (launch_params.params.rows + rows_per_loop - 1) / rows_per_loop * Kernel_traits::LDGS * Kernel_traits::NUM_ELTS;
-                    launch_params.barrier_size = 0;
-                    launch_params.workspace_bytes = 0;
-                    if(Kernel_traits::CTAS_PER_ROW > 1) {
-                        launch_params.barrier_size = 2 * launch_params.params.ctas_per_col;
-                        launch_params.workspace_bytes = launch_params.params.ctas_per_col
-                                                      * Kernel_traits::WARPS_M
-                                                      * Kernel_traits::CTAS_PER_ROW
-                                                      * sizeof(typename Kernel_traits::Stats::stats_t)
-                                                      * 2;
-                    }
-                    return;
-                }
-
-                if( Kernel_traits::SMEM_BYTES_FWD >= 48 * 1024 ) {
-                    CHECK_CUDA(cudaFuncSetAttribute(kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, Kernel_traits::SMEM_BYTES_FWD));
-                }
-                auto stream = launch_params.stream;
-                auto ctas_per_col = launch_params.params.ctas_per_col;
-
-                if( Kernel_traits::CTAS_PER_ROW == 1 ) {
-                    kernel<<<ctas_per_col, Kernel_traits::THREADS_PER_CTA, Kernel_traits::SMEM_BYTES_FWD, stream>>>(launch_params.params);
-                } else {
-                    dim3 grid(Kernel_traits::CTAS_PER_ROW * ctas_per_col);
-                    dim3 block(Kernel_traits::THREADS_PER_CTA);
-                    void *params_ = (void *)&launch_params.params;
-                    cudaLaunchCooperativeKernel((void *)kernel, grid, block, (void **)&params_, Kernel_traits::SMEM_BYTES_FWD, stream);
-                }
-            });
-        });
-    });
-}
diff --git a/based/csrc/layer_norm/ln_utils.cuh b/based/csrc/layer_norm/ln_utils.cuh
deleted file mode 100644
index 178d6fd..0000000
--- a/based/csrc/layer_norm/ln_utils.cuh
+++ /dev/null
@@ -1,783 +0,0 @@
-#pragma once
-
-#include <cassert>
-
-#include <cuda_bf16.h>
-#include <cuda_fp16.h>
-
-#include "ln.h"
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-constexpr uint32_t THREADS_PER_WARP = 32;
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-inline void check_cuda_(cudaError_t status, const char *file, int line) {
-    if( status != cudaSuccess ) {
-        fprintf(stderr, "CUDA Error: %s %s %d\n", cudaGetErrorString(status), file, line);
-        exit(status);
-    }
-}
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-#define CHECK_CUDA(ans)                                                                                                        \
-    { check_cuda_((ans), __FILE__, __LINE__); }
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-#define DIVUP(x, y) (((x) + ((y)-1)) / (y))
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-#define REGISTER_FWD_LAUNCHER(HIDDEN_SIZE, WTYPE, ITYPE, RTYPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG)                 \
-    void ln_fwd_##HIDDEN_SIZE##_##WTYPE##_##ITYPE##_##RTYPE##_##OTYPE##_##CTYPE(LaunchParams<FwdParams> &launch_params,                      \
-                                                                                const bool configure_params) {                               \
-        launch_<WTYPE, ITYPE, RTYPE, OTYPE, CTYPE, uint32_t, HIDDEN_SIZE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG>(                    \
-            launch_params, configure_params);                                                                                                \
-    }                                                                                                                                        \
-    static FwdRegistrar<WTYPE, ITYPE, RTYPE, OTYPE, CTYPE, HIDDEN_SIZE> reg_##HIDDEN_SIZE##_##WTYPE##_##ITYPE##_##RTYPE##_##OTYPE##_##CTYPE( \
-        ln_fwd_##HIDDEN_SIZE##_##WTYPE##_##ITYPE##_##RTYPE##_##OTYPE##_##CTYPE)
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-#define REGISTER_BWD_LAUNCHER(                                                                                                                  \
-    HIDDEN_SIZE, WTYPE, ITYPE, RTYPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG, BYTES_PER_LDG_FINALIZE)                      \
-    void ln_bwd_##HIDDEN_SIZE##_##WTYPE##_##ITYPE##_##RTYPE##_##OTYPE##_##CTYPE(LaunchParams<BwdParams> &launch_params,                         \
-                                                                                const bool configure_params) {                                  \
-        launch_<WTYPE,                                                                                                                          \
-                ITYPE,                                                                                                                          \
-                RTYPE,                                                                                                                          \
-                OTYPE,                                                                                                                          \
-                CTYPE,                                                                                                                          \
-                uint32_t,                                                                                                                       \
-                HIDDEN_SIZE,                                                                                                                    \
-                CTAS_PER_ROW,                                                                                                                   \
-                WARPS_M,                                                                                                                        \
-                WARPS_N,                                                                                                                        \
-                BYTES_PER_LDG,                                                                                                                  \
-                BYTES_PER_LDG_FINALIZE>(launch_params, configure_params);                                                                       \
-    }                                                                                                                                           \
-    static BwdRegistrar<WTYPE, ITYPE, RTYPE, OTYPE, CTYPE, HIDDEN_SIZE> reg_##HIDDEN_SIZE##_##WTYPE##_##ITYPE##_##RTYPE##_##OTYPE##_##CTYPE(    \
-        ln_bwd_##HIDDEN_SIZE##_##WTYPE##_##ITYPE##_##RTYPE##_##OTYPE##_##CTYPE)
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-#define REGISTER_PARALLEL_FWD_LAUNCHER(HIDDEN_SIZE, WTYPE, ITYPE, RTYPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG)                \
-    void ln_parallel_residual_fwd_##HIDDEN_SIZE##_##WTYPE##_##ITYPE##_##RTYPE##_##OTYPE##_##CTYPE(LaunchParams<FwdParams> &launch_params,            \
-                                                                                const bool configure_params) {                                       \
-        launch_parallel_residual_<WTYPE, ITYPE, RTYPE, OTYPE, CTYPE, uint32_t, HIDDEN_SIZE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG>(          \
-            launch_params, configure_params);                                                                                                        \
-    }                                                                                                                                                \
-    static FwdParallelRegistrar<WTYPE, ITYPE, RTYPE, OTYPE, CTYPE, HIDDEN_SIZE> reg_##HIDDEN_SIZE##_##WTYPE##_##ITYPE##_##RTYPE##_##OTYPE##_##CTYPE( \
-        ln_parallel_residual_fwd_##HIDDEN_SIZE##_##WTYPE##_##ITYPE##_##RTYPE##_##OTYPE##_##CTYPE)
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-#define REGISTER_PARALLEL_BWD_LAUNCHER(                                                                                                              \
-    HIDDEN_SIZE, WTYPE, ITYPE, RTYPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG, BYTES_PER_LDG_FINALIZE)                           \
-    void ln_parallel_residual_bwd_##HIDDEN_SIZE##_##WTYPE##_##ITYPE##_##RTYPE##_##OTYPE##_##CTYPE(LaunchParams<BwdParams> &launch_params,            \
-                                                                                const bool configure_params) {                                       \
-        launch_parallel_residual_<WTYPE,                                                                                                             \
-                ITYPE,                                                                                                                               \
-                RTYPE,                                                                                                                               \
-                OTYPE,                                                                                                                               \
-                CTYPE,                                                                                                                               \
-                uint32_t,                                                                                                                            \
-                HIDDEN_SIZE,                                                                                                                         \
-                CTAS_PER_ROW,                                                                                                                        \
-                WARPS_M,                                                                                                                             \
-                WARPS_N,                                                                                                                             \
-                BYTES_PER_LDG,                                                                                                                       \
-                BYTES_PER_LDG_FINALIZE>(launch_params, configure_params);                                                                            \
-    }                                                                                                                                                \
-    static BwdParallelRegistrar<WTYPE, ITYPE, RTYPE, OTYPE, CTYPE, HIDDEN_SIZE> reg_##HIDDEN_SIZE##_##WTYPE##_##ITYPE##_##RTYPE##_##OTYPE##_##CTYPE( \
-        ln_parallel_residual_bwd_##HIDDEN_SIZE##_##WTYPE##_##ITYPE##_##RTYPE##_##OTYPE##_##CTYPE)
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-inline __device__ float2 operator+(const float2 & a, const float2 & b){
-    return {a.x + b.x, a.y + b.y};
-}
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-inline __device__ void operator+=(float2 & a, const float2 & b){
-    a.x += b.x;
-    a.y += b.y;
-}
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-template<typename T>
-struct Sum {
-    inline __device__ Sum(){}
-    inline __device__ T operator()(const T &a, const T &b){
-        return a + b;
-    }
-};
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-template<typename T>
-inline __device__ T warp_shuffle_xor(const T & x, uint32_t idx){
-    return __shfl_xor_sync(uint32_t(-1), x, idx);
-}
-
-template<>
-inline __device__ float2 warp_shuffle_xor<float2>(const float2 & x, uint32_t idx){
-    return { warp_shuffle_xor(x.x, idx), warp_shuffle_xor(x.y, idx) };
-}
-
-template<typename T>
-inline __device__ T warp_shuffle_down(const T & x, uint32_t idx){
-    return __shfl_down_sync(uint32_t(-1), x, idx);
-}
-
-template<>
-inline __device__ float2 warp_shuffle_down<float2>(const float2 & x, uint32_t idx){
-    return { warp_shuffle_down(x.x, idx), warp_shuffle_down(x.y, idx) };
-}
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-namespace layer_norm {
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-struct uint16 {
-    uint4 u;
-    uint4 v;
-    uint4 s;
-    uint4 t;
-};
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-struct uint8 {
-    uint4 u;
-    uint4 v;
-};
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-template<int BYTES>
-struct BytesToType {};
-
-template<>
-struct BytesToType<64> {
-    using Type = uint16;
-    static_assert(sizeof(Type) == 64);
-};
-
-template<>
-struct BytesToType<32> {
-    using Type = uint8;
-    static_assert(sizeof(Type) == 32);
-};
-
-template<>
-struct BytesToType<16> {
-    using Type = uint4;
-    static_assert(sizeof(Type) == 16);
-};
-
-template<>
-struct BytesToType<8> {
-    using Type = uint64_t;
-    static_assert(sizeof(Type) == 8);
-};
-
-template<>
-struct BytesToType<4> {
-    using Type = uint32_t;
-    static_assert(sizeof(Type) == 4);
-};
-
-template<>
-struct BytesToType<2> {
-    using Type = uint16_t;
-    static_assert(sizeof(Type) == 2);
-};
-
-template<>
-struct BytesToType<1> {
-    using Type = uint8_t;
-    static_assert(sizeof(Type) == 1);
-};
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-template<typename T>
-struct TypeToVec2 {};
-
-template<>
-struct TypeToVec2<float> {
-    using Type = float2;
-};
-
-template<>
-struct TypeToVec2<half> {
-    using Type = half2;
-};
-
-template<>
-struct TypeToVec2<nv_bfloat16> {
-    using Type = nv_bfloat162;
-};
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-template<int INDEX>
-struct Get {
-    template<typename T, typename R>
-    static inline __device__ R of(const T &vec);
-};
-
-template<>
-template<typename T, typename R>
-inline __device__ R Get<0>::of(const T &vec) {
-    return vec.x;
-}
-
-template<>
-template<typename T, typename R>
-inline __device__ R Get<1>::of(const T &vec) {
-    return vec.y;
-}
-
-template<>
-template<typename T, typename R>
-inline __device__ R Get<2>::of(const T &vec) {
-    return vec.z;
-}
-
-template<>
-template<typename T, typename R>
-inline __device__ R Get<3>::of(const T &vec) {
-    return vec.w;
-}
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-template<typename Src, typename Dst>
-struct Converter{
-    static inline __device__ Dst convert(const Src &from) {
-        return Dst(from);
-    }
-};
-
-template<>
-struct Converter<float2, half2>{
-    static inline __device__ half2 convert(const float2 &x) {
-        return __float22half2_rn(x);
-    }
-};
-
-template<>
-struct Converter<float2, nv_bfloat162>{
-    static inline __device__ nv_bfloat162 convert(const float2 &x) {
-#if __CUDA_ARCH__ >= 800
-        return __float22bfloat162_rn(x);
-#else
-        union {
-            nv_bfloat162 raw;
-            nv_bfloat16 x;
-            nv_bfloat16 y;
-        } tmp;
-        tmp.x = __float2bfloat16_rn(x.x);
-        tmp.y = __float2bfloat16_rn(x.y);
-        return tmp.raw;
-#endif
-    }
-};
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-template<typename T>
-struct Zeros{
-    static inline __device__ T get() {
-        return T(0.f);
-    }
-};
-
-template<> 
-struct Zeros<float2>{
-    static inline __device__ float2 get() {
-        return make_float2(0.f, 0.f);
-    }
-};
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-template<typename Elt_type, uint32_t NUM_ELT>
-struct Vec {
-
-    enum { BYTES = NUM_ELT * sizeof(Elt_type) };
-
-    using Vec_type = typename BytesToType<BYTES>::Type;
-
-    using Alias_type = union {
-        Vec_type vec;
-        Elt_type elt[NUM_ELT];
-    };
-
-    Alias_type data;
-
-    template<typename S>
-    inline __device__ void to(Vec<S, NUM_ELT> &other) {
-        #pragma unroll
-        for( int it = 0; it < NUM_ELT; it++ ) {
-            other.data.elt[it] = S(this->data.elt[it]);
-        }
-    }
-
-    template<typename Op>
-    inline __device__ void assign(const Op &op) {
-        #pragma unroll
-        for( int it = 0; it < NUM_ELT; it++ ) {
-            this->data.elt[it] = op(it);
-        }
-    }
-
-    inline __device__ void zero_() {
-        #pragma unroll
-        for( int it = 0; it < NUM_ELT; it++ ) {
-            this->data.elt[it] = Elt_type(0.f);
-        }
-    }
-
-    inline __device__ void load_from(const void *base_ptr, const size_t idx) {
-        this->data.vec = static_cast<const Vec_type *>(base_ptr)[idx];
-    }
-
-    inline __device__ void store_to(void *base_ptr, const size_t idx) {
-        static_cast<Vec_type *>(base_ptr)[idx] = this->data.vec;
-    }
-};
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-template<uint32_t CTAS_PER_ROW>
-struct InterCTASync {
-
-    template<typename Params>
-    inline __device__ InterCTASync(Params & params, uint32_t bidm, uint32_t bidn)
-        : phase_counter_(0)
-        , b0_(params.barrier + bidm) // The barrier for this group of CTAs.
-        , b1_(params.barrier + bidm + params.ctas_per_col) // The barrier for this group of CTAs.
-    {
-        // BARRIERS ARE ASSUMED TO BE INITIALIZED TO 0!
-    }
-
-    inline __device__ void spin_wait_(int *barrier, int step, int expected) {
-        asm volatile("red.release.gpu.global.add.s32 [%0], %1;" ::"l"(barrier), "r"(step));
-        for( int found = -1; found != expected; ) {
-            asm volatile("ld.global.acquire.gpu.b32 %0, [%1];" : "=r"(found) : "l"(barrier));
-        }
-    }
-
-    inline __device__ void sync(){
-        // ALL THREADS MUST ENTER!
-
-        // We switch barrier every iteration.
-        int *barrier = phase_counter_ & 0x1 ? b1_ : b0_;
-        // We decrement every other iteration.
-        bool dec = phase_counter_ & 0x2;
-        int step = dec ? -1 : 1;
-        int expected = dec ? 0 : CTAS_PER_ROW;
-        // There are only 4 phases: up/down for b0/b1.
-        phase_counter_ = (phase_counter_ + 1) & 0x3;
-
-        if( threadIdx.x == 0 ) {
-            spin_wait_(barrier, step, expected);
-        }
-        // CTA waits for thread 0
-        __syncthreads();
-    }
-
-    int phase_counter_;
-    int * b0_;
-    int * b1_;
-};
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-template<typename T, uint32_t CTAS_PER_ROW, uint32_t WARPS_M, uint32_t WARPS_N>
-struct Reducer : public Reducer<T, 1, WARPS_M, WARPS_N> {
-
-    using InterCTASync = InterCTASync<CTAS_PER_ROW>;
-    using Base = Reducer<T, 1, WARPS_M, WARPS_N>;
-    using Type = typename Base::Type;
-
-    enum { SMEM_BYTES = Base::SMEM_BYTES };
-
-    enum { WS_BARRIER_BYTES = 2 * sizeof(int) };
-    enum { WS_DATA_BYTES = WARPS_M * CTAS_PER_ROW * sizeof(T) };
-
-    // size of the barriers + temporary result per CTA (multiply with CTAS_PER_ROW to get total)
-    enum { WORKSPACE_BYTES_PER_GROUP = Base::WORKSPACE_BYTES_PER_GROUP + WS_BARRIER_BYTES + WS_DATA_BYTES };
-
-    template<typename Params>
-    inline __device__ Reducer(Params & params, uint32_t bidm, uint32_t bidn, uint32_t warp_m, uint32_t warp_n, uint32_t lane, void * smem)
-        : Base(params, bidm, bidn, warp_m, warp_n, lane, smem) 
-        , inter_cta_(params, bidm, bidn)
-        , bidn_(bidn) // CTA id within the group.
-        , w0_(static_cast<T*>(params.workspace) + (bidm * WARPS_M + warp_m) * CTAS_PER_ROW)
-        , w1_(w0_ + params.ctas_per_col * WARPS_M * CTAS_PER_ROW)
-    {
-    }
-
-    template<typename Op>
-    inline __device__ T allreduce(T data, Op &op) {
-        data = Base::reduce(data, op);
-        // We switch workspace every iteration.
-        T *workspace = inter_cta_.phase_counter_ & 0x1 ? w1_ : w0_;
-
-        // Warp leaders 0 hold the CTA-local results.
-        if( this->warp_n_ == 0 && this->lane_ == 0 ) {
-            workspace[bidn_] = data;
-        }
-        inter_cta_.sync();
-        static_assert(CTAS_PER_ROW <= 32);
-        T total = Zeros<T>::get();
-        if(this->lane_ < CTAS_PER_ROW){
-            total = workspace[this->lane_];
-        }
-        total = Reducer<T, 1, 1, 1>::allreduce_(total, op);
-
-        return total;
-    }
-
-    InterCTASync inter_cta_;
-
-    T *w0_;
-    T *w1_;
-    int bidn_;
-};
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-template<typename T, uint32_t WARPS_M>
-struct Reducer<T, 1, WARPS_M, 1> {
-
-    using Type = T;
-    enum { SMEM_BYTES = 0 };
-    enum { WORKSPACE_BYTES_PER_GROUP = 0 };
-
-    enum { THREADS_PER_WARP = 32 };
-
-    template<typename Params>
-    inline __device__ Reducer(Params & params, uint32_t bidm, uint32_t bidn, uint32_t warp_m, uint32_t warp_n, uint32_t lane, void * smem) 
-        : warp_n_(warp_n)
-        , lane_(lane)
-    {
-    }
-
-    template<typename Op>
-    static inline __device__ T allreduce_(T data, Op &op) {
-        #pragma unroll
-        for( int it = 1; it < THREADS_PER_WARP; it *= 2 ) {
-            data = op(data, warp_shuffle_xor(data, it));
-        }
-        return data;
-    }
-
-    template<typename Op>
-    inline __device__ T allreduce(T data, Op &op) {
-        return allreduce_(data, op);
-    }
-
-    template<typename Op>
-    inline __device__ T reduce(T data, Op &op){
-        // only lane 0 holds the result!
-        #pragma unroll
-        for( int it = THREADS_PER_WARP / 2; it > 0; it /= 2 ) {
-            data = op(data, warp_shuffle_down(data, it));
-        }  
-        return data;
-    }
-    int warp_n_;
-    int lane_;
-};
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-template<typename T, uint32_t WARPS_M, uint32_t WARPS_N>
-struct Reducer<T, 1, WARPS_M, WARPS_N> : public Reducer<T, 1, WARPS_M, 1> {
-
-    using Base = Reducer<T, 1, WARPS_M, 1>;
-
-    using Type = T;
-
-    enum { SMEM_BYTES = Base::SMEM_BYTES + WARPS_M * WARPS_N * sizeof(T) * 2 };
-    enum { WORKSPACE_BYTES_PER_GROUP = 0 };
-
-    enum { THREADS_PER_WARP = 32 };
-
-    template<typename Params>
-    inline __device__ Reducer(Params & params, uint32_t bidm, uint32_t bidn, uint32_t warp_m, uint32_t warp_n, uint32_t lane, void * smem) 
-        : Base(params, bidm, bidn, warp_m, warp_n, lane, smem) 
-        , use0_(true)
-    {
-        smem0_ = &static_cast<T *>(smem)[warp_m * WARPS_N];
-        smem1_ = smem0_ + WARPS_M * WARPS_N;
-    }
-
-    template<typename Op>
-    inline __device__ T allreduce(T data, Op & op) {
-        T * smem = use0_ ? smem0_ : smem1_;
-        use0_ = !use0_;
-        data = Base::reduce(data, op);
-        if( this->lane_ == 0 ) {
-            smem[this->warp_n_] = data;
-        }
-        __syncthreads();
-        T out = Zeros<T>::get();
-        #pragma unroll
-        for( int it = 0; it < WARPS_N; it++ ) {
-            out = op(out, smem[it]);
-        }
-        return out;
-    }
-
-    template<typename Op>
-    inline __device__ T reduce(T data, Op &op) {
-        T * smem = use0_ ? smem0_ : smem1_;
-        use0_ = !use0_;
-        // only intra-CTA group leader holds the result!
-        data = Base::reduce(data, op);
-        if( this->lane_ == 0 ) {
-            smem[this->warp_n_] = data;
-        }
-        __syncthreads();
-        T out = Zeros<T>::get();
-        if( this->warp_n_ == 0 && this->lane_ == 0 ) {
-            #pragma unroll
-            for( int it = 0; it < WARPS_N; it++ ) {
-                out = op(out, smem[it]);
-            }
-        }
-        return out;
-    }
-
-    T * smem0_;
-    T * smem1_;
-    bool use0_;
-
-};
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
- 
-template<typename T, typename int_t>
-inline __device__ void warp_chan_upd_dynamic(T &m_a, T &m2_a, int_t &n_a, int num_active){
-    //Assume at least leftmost is valid and init: step = next_pow2(num_active) / 2 (might get NaN otherwise)
-    const int highest_bit_set = (8 * sizeof(num_active)) - __clz(num_active - 1);
-    
-    #pragma unroll
-    for( int step = (1 << (highest_bit_set - 1)); step > 0; step /= 2 ) {
-        // Exchange
-        int_t n_b = warp_shuffle_down(n_a, step);
-        T m_b = warp_shuffle_down(m_a, step);
-        T m2_b = warp_shuffle_down(m2_a, step);
-
-        // Update
-        const int_t n_ab = n_a + n_b; // We can handle one of them being 0, not both.
-        const T rn_ab = 1.f / n_ab; // Might have different n per thread, otherwise this would simplify :(
-        const T delta = m_a - m_b;
-        const float m2_ab = m2_a + m2_b + delta * delta * n_a * n_b * rn_ab;
-        const float m_ab = (n_a * m_a + n_b * m_b) * rn_ab;
-
-        n_a = n_ab;
-        m_a = m_ab;
-        m2_a = m2_ab;
-    }
-    // Intra-warp broadcast (only lane 0 has valid stats).
-    m_a = __shfl_sync(uint32_t(-1), m_a, 0);
-    m2_a = __shfl_sync(uint32_t(-1), m2_a, 0);
-}
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-template<typename T, uint32_t CTAS_PER_ROW, uint32_t WARPS_M, uint32_t WARPS_N>
-struct Stats {
-    // This could be done generically with the Reducer. But then we would have to exchange 3 instead of 2 fields.
-
-    using InterCTASync = InterCTASync<CTAS_PER_ROW>;
-    using BlockStats = Stats<T, 1, WARPS_M, WARPS_N>;
-    using stats_t = typename BlockStats::stats_t;
-
-    enum { SMEM_BYTES = BlockStats::SMEM_BYTES };
-
-    template<typename Params>
-    inline __device__ Stats(Params & params, uint32_t bidm, uint32_t bidn, uint32_t warp_m, uint32_t warp_n, uint32_t lane, void * smem) 
-        : inter_cta_(params, bidm, bidn)
-        , block_stats_(params, bidm, bidn, warp_m, warp_n, lane, smem)
-        , bidn_(bidn) // CTA id within the group.
-        , w0_(static_cast<stats_t*>(params.workspace) + (bidm * WARPS_M + warp_m) * CTAS_PER_ROW)
-        , w1_(w0_ + params.ctas_per_col * WARPS_M * CTAS_PER_ROW)
-        , warp_n_(warp_n)
-        , lane_(lane)
-    {
-    }
-
-    template<uint32_t N>
-    inline __device__ stats_t compute(const T (&elts)[N], const T rn) {
-        constexpr T ELTS_PER_ROW_PER_CTA = N * WARPS_N * THREADS_PER_WARP;
-        // TODO rn is not really needed here..
-        constexpr T block_rn = 1.f / T(ELTS_PER_ROW_PER_CTA);
-        stats_t block_stats = block_stats_.compute(elts, block_rn);
-
-        stats_t *workspace = inter_cta_.phase_counter_ & 0x1 ? w1_ : w0_;
-
-        if( warp_n_ == 0 && lane_ == 0 ) {
-            workspace[bidn_] = block_stats;
-        }
-
-        // Wait for all CTAS_PER_ROW CTAS in the group to have written their result.
-        inter_cta_.sync();
-
-        T n = Zeros<T>::get();
-        T m = Zeros<T>::get();
-        T m2 = Zeros<T>::get();
-
-        // Assume CTA group size in N less than 32, such that we can finalize with a single warp.
-        static_assert(CTAS_PER_ROW <= 32);
-
-        // Every warp does the final reduction locally. 
-        if( lane_ < CTAS_PER_ROW ) {
-            stats_t result = workspace[lane_];
-            n = ELTS_PER_ROW_PER_CTA;
-            m = layer_norm::Get<0>::of<stats_t, T>(result);
-            m2 = layer_norm::Get<1>::of<stats_t, T>(result);
-        }
-
-        warp_chan_upd_dynamic(m, m2, n, CTAS_PER_ROW);
-
-        return { m, m2 };
-    }
-
-    InterCTASync inter_cta_;
-    BlockStats block_stats_;
-
-    stats_t *w0_;
-    stats_t *w1_;
-    int bidn_;
-    int warp_n_;
-    int lane_;
-};
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-template<typename T, uint32_t WARPS_M, uint32_t WARPS_N>
-struct Stats<T, 1, WARPS_M, WARPS_N> {
-
-    using WarpStats = Stats<T, 1, WARPS_M, 1>;
-    using stats_t = typename WarpStats::stats_t;
-
-    enum { SMEM_BYTES = WARPS_M * WARPS_N * sizeof(stats_t) * 2 };
-
-    template<typename Params>
-    inline __device__ Stats(Params & params, uint32_t bidm, uint32_t bidn, uint32_t warp_m, uint32_t warp_n, uint32_t lane, void * smem) 
-        : warp_stats_(params, bidm, bidn, warp_m, warp_n, lane, smem)
-        , use0_(true)
-    {
-        smem0_ = static_cast<stats_t*>(smem) + warp_m * WARPS_N;
-        smem1_ = smem0_ + WARPS_M * WARPS_N;
-    }
-
-    template<bool Is_even_cols, uint32_t N, typename function_t>
-    inline __device__ stats_t compute(const T (&elts)[N], const T row_norm_factor,
-                                      function_t valid_elts_in_warp_fn, const int num_valid_elts = N) {
-        stats_t * smem = use0_ ? smem0_ : smem1_;
-        use0_ = !use0_;
-        // Compute warp local for all WARPS_N
-        const auto warp_n = warp_stats_.reducer_.warp_n_;
-        const T warp_norm_factor = 1.f / T(Is_even_cols ? N * THREADS_PER_WARP : valid_elts_in_warp_fn(warp_n));
-        stats_t warp_stats = warp_stats_.template compute<Is_even_cols>(
-            elts, warp_norm_factor, valid_elts_in_warp_fn, num_valid_elts
-        );
-
-        //Each warp warp leader stores its stats
-        const auto lane = warp_stats_.reducer_.lane_;
-        if( lane == 0 ) {
-            smem[warp_n] = warp_stats;
-        }
-        __syncthreads();
-
-        int n = 0;;
-        T m = Zeros<T>::get();
-        T m2 = Zeros<T>::get();
-
-        // Assume that there are less than 32 warps, such that we can finalize with a single warp
-        static_assert(WARPS_N <= 32);
-        if(lane < WARPS_N){
-            stats_t result = smem[lane];
-            n = Is_even_cols ? N * THREADS_PER_WARP : valid_elts_in_warp_fn(lane);
-            m = layer_norm::Get<0>::of<stats_t, T>(result);
-            m2 = layer_norm::Get<1>::of<stats_t, T>(result);
-        }
-
-        warp_chan_upd_dynamic(m, m2, n, WARPS_N);
-
-        return { m, m2 };
-    }
-    WarpStats warp_stats_;
-    stats_t * smem0_;
-    stats_t * smem1_;
-    bool use0_;
-};
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-template<typename T, uint32_t WARPS_M>
-struct Stats<T, 1, WARPS_M, 1> {
-
-    using stats_t = typename TypeToVec2<T>::Type;
-    // The simple Warp reducer.
-    using Reducer = Reducer<T, 1, WARPS_M, 1>;
-
-    enum { SMEM_BYTES = 0 };
-
-    template<typename Params>
-    inline __device__ Stats(Params & params, uint32_t bidm, uint32_t bidn, uint32_t warp_m, uint32_t warp_n, uint32_t lane, void * smem) 
-        : reducer_(params, bidm, bidn, warp_m, warp_n, lane, smem)
-    {
-    }
-
-    template<bool Is_even_cols, uint32_t N, typename function_t>
-    inline __device__ stats_t compute(const T (&elts)[N], const T row_norm_factor,
-                                      // const int valid_elts_in_warp_ignored_, const int num_valid_elts = N) {
-                                      function_t valid_elts_in_warp_fn, const int num_valid_elts = N) {
-
-        auto sum = Sum<T>();
-
-        T m = Zeros<T>::get();
-        #pragma unroll
-        for( int it = 0; it < N; it++ ) {
-            if (Is_even_cols || (it < num_valid_elts)) {
-                m += elts[it];
-            }
-        }
-        m = reducer_.allreduce(m, sum) * row_norm_factor;
-
-        T m2 = Zeros<T>::get();
-        #pragma unroll
-        for( int it = 0; it < N; it++ ) {
-            if (Is_even_cols || (it < num_valid_elts)) {
-                T diff = (elts[it] - m);
-                m2 += diff * diff;
-            }
-        }
-        m2 = reducer_.allreduce(m2, sum);
-
-        return {m, m2};
-    }
-
-    Reducer reducer_;
-};
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-}  // namespace layer_norm
diff --git a/based/csrc/layer_norm/setup.py b/based/csrc/layer_norm/setup.py
deleted file mode 100644
index dfad468..0000000
--- a/based/csrc/layer_norm/setup.py
+++ /dev/null
@@ -1,204 +0,0 @@
-# Adapted from https://github.com/NVIDIA/apex/blob/master/setup.py
-import sys
-import warnings
-import os
-from packaging.version import parse, Version
-
-import torch
-from torch.utils.cpp_extension import BuildExtension, CppExtension, CUDAExtension, CUDA_HOME
-from setuptools import setup, find_packages
-import subprocess
-
-# ninja build does not work unless include_dirs are abs path
-this_dir = os.path.dirname(os.path.abspath(__file__))
-
-
-def get_cuda_bare_metal_version(cuda_dir):
-    raw_output = subprocess.check_output([cuda_dir + "/bin/nvcc", "-V"], universal_newlines=True)
-    output = raw_output.split()
-    release_idx = output.index("release") + 1
-    bare_metal_version = parse(output[release_idx].split(",")[0])
-
-    return raw_output, bare_metal_version
-
-
-def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
-    raw_output, bare_metal_version = get_cuda_bare_metal_version(cuda_dir)
-    torch_binary_version = parse(torch.version.cuda)
-
-    print("\nCompiling cuda extensions with")
-    print(raw_output + "from " + cuda_dir + "/bin\n")
-
-    if (bare_metal_version != torch_binary_version):
-        raise RuntimeError(
-            "Cuda extensions are being compiled with a version of Cuda that does "
-            "not match the version used to compile Pytorch binaries.  "
-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
-            + "In some cases, a minor-version mismatch will not cause later errors:  "
-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
-            "You can try commenting out this check (at your own risk)."
-        )
-
-
-def raise_if_cuda_home_none(global_option: str) -> None:
-    if CUDA_HOME is not None:
-        return
-    raise RuntimeError(
-        f"{global_option} was requested, but nvcc was not found.  Are you sure your environment has nvcc available?  "
-        "If you're installing within a container from https://hub.docker.com/r/pytorch/pytorch, "
-        "only images whose names contain 'devel' will provide nvcc."
-    )
-
-
-def append_nvcc_threads(nvcc_extra_args):
-    _, bare_metal_version = get_cuda_bare_metal_version(CUDA_HOME)
-    if bare_metal_version >= Version("11.2"):
-        return nvcc_extra_args + ["--threads", "4"]
-    return nvcc_extra_args
-
-
-if not torch.cuda.is_available():
-    # https://github.com/NVIDIA/apex/issues/486
-    # Extension builds after https://github.com/pytorch/pytorch/pull/23408 attempt to query torch.cuda.get_device_capability(),
-    # which will fail if you are compiling in an environment without visible GPUs (e.g. during an nvidia-docker build command).
-    print(
-        "\nWarning: Torch did not find available GPUs on this system.\n",
-        "If your intention is to cross-compile, this is not an error.\n"
-        "By default, Apex will cross-compile for Pascal (compute capabilities 6.0, 6.1, 6.2),\n"
-        "Volta (compute capability 7.0), Turing (compute capability 7.5),\n"
-        "and, if the CUDA version is >= 11.0, Ampere (compute capability 8.0).\n"
-        "If you wish to cross-compile for a single specific architecture,\n"
-        'export TORCH_CUDA_ARCH_LIST="compute capability" before running setup.py.\n',
-    )
-    if os.environ.get("TORCH_CUDA_ARCH_LIST", None) is None and CUDA_HOME is not None:
-        _, bare_metal_version = get_cuda_bare_metal_version(CUDA_HOME)
-        if bare_metal_version >= Version("11.8"):
-            os.environ["TORCH_CUDA_ARCH_LIST"] = "6.0;6.1;6.2;7.0;7.5;8.0;8.6;9.0"
-        elif bare_metal_version >= Version("11.1"):
-            os.environ["TORCH_CUDA_ARCH_LIST"] = "6.0;6.1;6.2;7.0;7.5;8.0;8.6"
-        elif bare_metal_version == Version("11.0"):
-            os.environ["TORCH_CUDA_ARCH_LIST"] = "6.0;6.1;6.2;7.0;7.5;8.0"
-        else:
-            os.environ["TORCH_CUDA_ARCH_LIST"] = "6.0;6.1;6.2;7.0;7.5"
-
-
-print("\n\ntorch.__version__  = {}\n\n".format(torch.__version__))
-TORCH_MAJOR = int(torch.__version__.split(".")[0])
-TORCH_MINOR = int(torch.__version__.split(".")[1])
-
-cmdclass = {}
-ext_modules = []
-
-# Check, if ATen/CUDAGeneratorImpl.h is found, otherwise use ATen/cuda/CUDAGeneratorImpl.h
-# See https://github.com/pytorch/pytorch/pull/70650
-generator_flag = []
-torch_dir = torch.__path__[0]
-if os.path.exists(os.path.join(torch_dir, "include", "ATen", "CUDAGeneratorImpl.h")):
-    generator_flag = ["-DOLD_GENERATOR_PATH"]
-
-raise_if_cuda_home_none("--fast_layer_norm")
-# Check, if CUDA11 is installed for compute capability 8.0
-cc_flag = []
-_, bare_metal_version = get_cuda_bare_metal_version(CUDA_HOME)
-if bare_metal_version < Version("11.0"):
-    raise RuntimeError("dropout_layer_norm is only supported on CUDA 11 and above")
-cc_flag.append("-gencode")
-cc_flag.append("arch=compute_70,code=sm_70")
-cc_flag.append("-gencode")
-cc_flag.append("arch=compute_80,code=sm_80")
-if bare_metal_version >= Version("11.8"):
-    cc_flag.append("-gencode")
-    cc_flag.append("arch=compute_90,code=sm_90")
-
-ext_modules.append(
-    CUDAExtension(
-        name="dropout_layer_norm",
-        sources=[
-            "ln_api.cpp",
-            "ln_fwd_256.cu",
-            "ln_bwd_256.cu",
-            "ln_fwd_512.cu",
-            "ln_bwd_512.cu",
-            "ln_fwd_768.cu",
-            "ln_bwd_768.cu",
-            "ln_fwd_1024.cu",
-            "ln_bwd_1024.cu",
-            "ln_fwd_1280.cu",
-            "ln_bwd_1280.cu",
-            "ln_fwd_1536.cu",
-            "ln_bwd_1536.cu",
-            "ln_fwd_2048.cu",
-            "ln_bwd_2048.cu",
-            "ln_fwd_2560.cu",
-            "ln_bwd_2560.cu",
-            "ln_fwd_3072.cu",
-            "ln_bwd_3072.cu",
-            "ln_fwd_4096.cu",
-            "ln_bwd_4096.cu",
-            "ln_fwd_5120.cu",
-            "ln_bwd_5120.cu",
-            "ln_fwd_6144.cu",
-            "ln_bwd_6144.cu",
-            "ln_fwd_7168.cu",
-            "ln_bwd_7168.cu",
-            "ln_fwd_8192.cu",
-            "ln_bwd_8192.cu",
-            "ln_parallel_fwd_256.cu",
-            "ln_parallel_bwd_256.cu",
-            "ln_parallel_fwd_512.cu",
-            "ln_parallel_bwd_512.cu",
-            "ln_parallel_fwd_768.cu",
-            "ln_parallel_bwd_768.cu",
-            "ln_parallel_fwd_1024.cu",
-            "ln_parallel_bwd_1024.cu",
-            "ln_parallel_fwd_1280.cu",
-            "ln_parallel_bwd_1280.cu",
-            "ln_parallel_fwd_1536.cu",
-            "ln_parallel_bwd_1536.cu",
-            "ln_parallel_fwd_2048.cu",
-            "ln_parallel_bwd_2048.cu",
-            "ln_parallel_fwd_2560.cu",
-            "ln_parallel_bwd_2560.cu",
-            "ln_parallel_fwd_3072.cu",
-            "ln_parallel_bwd_3072.cu",
-            "ln_parallel_fwd_4096.cu",
-            "ln_parallel_bwd_4096.cu",
-            "ln_parallel_fwd_5120.cu",
-            "ln_parallel_bwd_5120.cu",
-            "ln_parallel_fwd_6144.cu",
-            "ln_parallel_bwd_6144.cu",
-            "ln_parallel_fwd_7168.cu",
-            "ln_parallel_bwd_7168.cu",
-            "ln_parallel_fwd_8192.cu",
-            "ln_parallel_bwd_8192.cu",
-        ],
-        extra_compile_args={
-            "cxx": ["-O3"] + generator_flag,
-            "nvcc": append_nvcc_threads(
-                [
-                    "-O3",
-                    "-U__CUDA_NO_HALF_OPERATORS__",
-                    "-U__CUDA_NO_HALF_CONVERSIONS__",
-                    "-U__CUDA_NO_BFLOAT16_OPERATORS__",
-                    "-U__CUDA_NO_BFLOAT16_CONVERSIONS__",
-                    "-U__CUDA_NO_BFLOAT162_OPERATORS__",
-                    "-U__CUDA_NO_BFLOAT162_CONVERSIONS__",
-                    "--expt-relaxed-constexpr",
-                    "--expt-extended-lambda",
-                    "--use_fast_math",
-                ]
-                + generator_flag
-                + cc_flag
-            ),
-        },
-        include_dirs=[this_dir],
-    )
-)
-
-setup(
-    name="dropout_layer_norm",
-    version="0.1",
-    description="Fused dropout + add + layer norm",
-    ext_modules=ext_modules,
-    cmdclass={"build_ext": BuildExtension} if ext_modules else {},
-)
diff --git a/based/csrc/layer_norm/static_switch.h b/based/csrc/layer_norm/static_switch.h
deleted file mode 100644
index 7920ac0..0000000
--- a/based/csrc/layer_norm/static_switch.h
+++ /dev/null
@@ -1,25 +0,0 @@
-// Inspired by https://github.com/NVIDIA/DALI/blob/main/include/dali/core/static_switch.h
-// and https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/Dispatch.h
-
-#pragma once
-
-/// @param COND       - a boolean expression to switch by
-/// @param CONST_NAME - a name given for the constexpr bool variable.
-/// @param ...       - code to execute for true and false
-///
-/// Usage:
-/// ```
-/// BOOL_SWITCH(flag, BoolConst, [&] {
-///     some_function<BoolConst>(...);
-/// });
-/// ```
-#define BOOL_SWITCH(COND, CONST_NAME, ...)                                           \
-    [&] {                                                                            \
-        if (COND) {                                                                  \
-            constexpr bool CONST_NAME = true;                                        \
-            return __VA_ARGS__();                                                    \
-        } else {                                                                     \
-            constexpr bool CONST_NAME = false;                                       \
-            return __VA_ARGS__();                                                    \
-        }                                                                            \
-    }()
diff --git a/based/generation.py b/based/generation.py
deleted file mode 100644
index 5cc0872..0000000
--- a/based/generation.py
+++ /dev/null
@@ -1,840 +0,0 @@
-# Copyright (c) 2023, Tri Dao.
-# Adapted from https://github.com/NVIDIA/Megatron-LM/blob/0bb597b42c53355a567aba2a1357cc34b9d99ddd/megatron/text_generation/forward_step.py#L31
-import gc
-import time
-from collections import namedtuple
-from dataclasses import dataclass, field
-from functools import partial
-from typing import Callable, Optional, Sequence, Union
-
-import torch
-import torch.nn.functional as F
-from einops import rearrange, repeat
-from torch import Tensor
-from torch.profiler import ProfilerActivity, profile, record_function
-from transformers.generation import GreedySearchDecoderOnlyOutput, SampleDecoderOnlyOutput
-
-
-@dataclass
-class InferenceParams:
-    """Inference parameters that are passed to the main model in order
-    to efficienly calculate and store the context during inference."""
-
-    max_seqlen: int
-    max_batch_size: int
-    seqlen_offset: int = 0
-    batch_size_offset: int = 0
-    key_value_memory_dict: dict = field(default_factory=dict)
-    lengths_per_sample: Optional[Tensor] = None
-
-    def reset(self, max_seqlen, max_batch_size):
-        self.max_seqlen = max_seqlen
-        self.max_batch_size = max_batch_size
-        self.seqlen_offset = 0
-        if self.lengths_per_sample is not None:
-            self.lengths_per_sample.zero_()
-
-
-# https://github.com/NVIDIA/Megatron-LM/blob/0bb597b42c53355a567aba2a1357cc34b9d99ddd/megatron/text_generation/sampling.py
-# https://github.com/huggingface/transformers/blob/a44985b41cfa2de48a5e1de7f1f93b7483da25d1/src/transformers/generation/logits_process.py#L231
-def modify_logits_for_top_k_filtering(logits, top_k):
-    """Set the logits for none top-k values to -inf. Done in-place."""
-    indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]
-    logits.masked_fill_(indices_to_remove, float("-Inf"))
-
-
-# https://github.com/NVIDIA/Megatron-LM/blob/0bb597b42c53355a567aba2a1357cc34b9d99ddd/megatron/text_generation/sampling.py
-# https://github.com/huggingface/transformers/blob/a44985b41cfa2de48a5e1de7f1f93b7483da25d1/src/transformers/generation/logits_process.py#L170
-def modify_logits_for_top_p_filtering(logits, top_p):
-    """Set the logits for none top-p values to -inf. Done in-place."""
-    if top_p <= 0.0 or top_p >= 1.0:
-        return
-    # First sort and calculate cumulative sum of probabilities.
-    sorted_logits, sorted_indices = torch.sort(logits, descending=False)
-    cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)
-    # Remove tokens with cumulative top_p above the threshold (token with 0 are kept)
-    sorted_indices_to_remove = cumulative_probs <= (1 - top_p)
-    # scatter sorted tensors to original indexing
-    indices_to_remove = sorted_indices_to_remove.scatter(
-        1, sorted_indices, sorted_indices_to_remove
-    )
-    logits.masked_fill_(indices_to_remove, float("-inf"))
-
-
-def sample(logits, top_k=1, top_p=0.0, temperature=1.0):
-    """Sample from top-k logits.
-    Arguments:
-        logits: Tensor of shape (batch_size, vocab_size)
-    """
-    if top_k == 1:  # Short-circuit for greedy decoding
-        return logits.argmax(dim=-1)
-    else:
-        if top_p > 0.0:
-            assert top_p <= 1.0, "top-p should be in (0, 1]."
-        if top_k > 0:
-            top_k = min(top_k, logits.size(-1))  # Safety check
-            logits_top, indices = torch.topk(logits, top_k, dim=-1)
-            if temperature != 1.0:
-                logits_top /= temperature
-            modify_logits_for_top_p_filtering(logits_top, top_p)
-            return indices[
-                torch.arange(indices.shape[0], device=indices.device),
-                torch.multinomial(torch.softmax(logits_top, dim=-1), num_samples=1).squeeze(dim=-1),
-            ]
-        else:
-            # Clone so that when we modify for top_p we don't change the original logits
-            logits_top = logits / temperature if temperature != 1.0 else logits.clone()
-            modify_logits_for_top_p_filtering(logits_top, top_p)
-            return torch.multinomial(torch.softmax(logits_top, dim=-1), num_samples=1).squeeze(
-                dim=-1
-            )
-
-
-@torch.inference_mode()
-def decode(
-    input_ids,
-    model,
-    max_length,
-    top_k=1,
-    top_p=0.0,
-    temperature=1.0,
-    eos_token_id=None,
-    teacher_outputs=None,
-    vocab_size=None,
-    tensor_parallel=1,
-    cg=False,
-    enable_timing=False,
-    stopping_criteria: any = None,
-    **kwargs,
-):
-    """Decoding, either greedy or with top-k or top-p sampling.
-    If top-k = 0, don't limit the number of candidates (pure sampling).
-    Top-k and top-p can be used together. If top_k > 0 and top_p > 0, then top-k is applied first,
-    then top-p.
-    We assume that all sequences in the same batch have the same length.
-
-    Arguments:
-        input_ids: (batch, seq_len)
-        max_length: int
-        teacher_outputs (optional): (batch, seq_len). If provided, instead of sampling from the
-            logits, the next token is taken from the teacher_outputs. Useful for testing.
-    Returns: GreedySearchDecoderOnlyOutput or SampleDecoderOnlyOutput, with the following fields:
-        sequences: (batch, max_length)
-        scores: tuples of (batch, vocab_size)
-    """
-    batch_size, seqlen_og = input_ids.shape
-    teacher_output_len = teacher_outputs.shape[1] if teacher_outputs is not None else 0
-    if cg:
-        if not hasattr(model, "_decoding_cache"):
-            model._decoding_cache = None
-        model._decoding_cache = update_graph_cache(
-            model,
-            model._decoding_cache,
-            batch_size,
-            seqlen_og,
-            max_length,
-            tensor_parallel=tensor_parallel,
-        )
-        inference_params = model._decoding_cache.inference_params
-        inference_params.reset(max_length, batch_size)
-    else:
-        inference_params = InferenceParams(max_seqlen=max_length, max_batch_size=batch_size)
-
-    def get_logits(input_ids, inference_params):
-        decoding = inference_params.seqlen_offset > 0
-        if decoding:
-            position_ids = torch.full(
-                (batch_size, 1),
-                inference_params.seqlen_offset,
-                dtype=torch.long,
-                device=input_ids.device,
-            )
-        else:
-            position_ids = None
-        if not cg or not decoding:
-            logits = model(
-                input_ids,
-                position_ids=position_ids,
-                inference_params=inference_params,
-                num_last_tokens=1,
-            ).logits.squeeze(dim=1)
-        else:
-            logits = model._decoding_cache.run(
-                input_ids, position_ids, inference_params.seqlen_offset
-            ).squeeze(dim=1)
-        return logits[..., :vocab_size] if vocab_size is not None else logits
-
-    def sample_tokens(logits, inference_params):
-        if teacher_outputs is None or teacher_output_len <= inference_params.seqlen_offset:
-            token = sample(logits, top_k=top_k, top_p=top_p, temperature=temperature)
-        else:
-            token = teacher_outputs[:, inference_params.seqlen_offset]
-        # return rearrange(token, "b -> b 1")
-        return token.unsqueeze(1)
-
-    def should_stop(current_token, inference_params):
-        if inference_params.seqlen_offset == 0:
-            return False
-        if eos_token_id is not None and (current_token == eos_token_id).all():
-            return True
-        if inference_params.seqlen_offset >= max_length - 1:
-            return True
-        return False
-
-    start = torch.cuda.Event(enable_timing=enable_timing)
-    end = torch.cuda.Event(enable_timing=enable_timing)
-
-    if enable_timing:
-        if tensor_parallel > 1:
-            torch.distributed.barrier()
-        start.record()
-    scores, sequences = [], [input_ids]
-
-    while not should_stop(sequences[-1], inference_params):
-        scores.append(get_logits(sequences[-1], inference_params))
-        inference_params.seqlen_offset += sequences[-1].shape[1]
-        sequences.append(sample_tokens(scores[-1], inference_params))
-    if enable_timing:
-        end.record()
-        if tensor_parallel > 1:
-            torch.distributed.barrier()
-        torch.cuda.synchronize()
-        print(f"Prompt processing + decoding time: {(start.elapsed_time(end)):.0f}ms")
-    output_cls = GreedySearchDecoderOnlyOutput if top_k == 1 else SampleDecoderOnlyOutput
-    return output_cls(sequences=torch.cat(sequences, dim=1), scores=tuple(scores))
-
-
-def sample_speculative(logits, logits_draft, tokens_draft, top_k=1, top_p=0.0, temperature=1.0):
-    """Algorithm 1 from [1]
-    [1] Fast Inference from Transformers via Speculative Decoding
-    Yaniv Leviathan, Matan Kalman, Yossi Matias
-    https://arxiv.org/abs/2211.17192
-
-    Arguments:
-        logits: Tensor of shape (batch_size, seqlen + 1, vocab_size)
-        logits_draft: Tensor of shape (batch_size, seqlen, vocab_size)
-        tokens_draft: Tensor of shape (batch_size, seqlen)
-    Return:
-        tokens: Tensor of shape (batch_size, seqlen + 1)
-        num_generated_tokens: Tensor of shape (batch_size), with value in [1, seqlen + 1].
-            For each sequence in the batch, the number of valid tokens that were sampled by
-            speculative sampling.
-    """
-    batch, seqlen_p_1, vocab_size = logits.shape
-    seqlen = seqlen_p_1 - 1
-    assert logits_draft.shape == (batch, seqlen, vocab_size)
-    assert tokens_draft.shape == (batch, seqlen)
-    assert tokens_draft.dtype in [torch.int64, torch.int32]
-    # TODO: if top_k = 1 we can simplify things and only work with indices
-    if top_p > 0.0:
-        assert top_p <= 1.0, "top-p should be in (0, 1]."
-    # Clone so that when we modify for top_p we don't change the original logits
-    logits = logits / temperature if temperature != 1.0 else logits.clone()
-    logits_draft = logits_draft / temperature if temperature != 1.0 else logits_draft.clone()
-    if top_k > 0:
-        top_k = min(top_k, logits.size(-1))  # Safety check
-        modify_logits_for_top_k_filtering(logits, top_k)
-        modify_logits_for_top_k_filtering(logits_draft, top_k)
-    modify_logits_for_top_p_filtering(logits, top_p)
-    modify_logits_for_top_p_filtering(logits_draft, top_p)
-    probs = torch.softmax(logits, dim=-1)
-    probs_draft = torch.softmax(logits_draft, dim=-1)
-    gather = lambda probs, tokens: rearrange(
-        probs.gather(dim=-1, index=rearrange(tokens, "... -> ... 1")), "... 1 -> ..."
-    )
-    # (batch, seqlen)
-    accepted = torch.rand(batch, seqlen, device=probs.device) * gather(
-        probs_draft, tokens_draft
-    ) <= gather(probs[:, :-1], tokens_draft)
-    accepted_all = accepted.all(dim=-1)
-    # (batch,)
-    first_rejected_idx = torch.where(accepted_all, seqlen, accepted.int().argmin(dim=-1))
-    probs_diff = torch.clamp(probs[:, :-1] - probs_draft, min=0.0)
-    # torch.multinomial can deal with unnormalized probabilities
-    # probs_diff /= probs_diff.sum(dim=-1, keepdim=True)
-    resample_probs = torch.cat([probs_diff, probs[:, -1:]], dim=1)
-    resample_probs = rearrange(
-        resample_probs.gather(dim=1, index=repeat(first_rejected_idx, "b -> b 1 d", d=vocab_size)),
-        "b 1 d -> b d",
-    )
-    resample = torch.multinomial(resample_probs, num_samples=1).squeeze(dim=-1)  # (batch,)
-    tokens = F.pad(tokens_draft, (0, 1))
-    tokens[:, first_rejected_idx] = resample
-    return tokens, first_rejected_idx + 1
-
-
-@torch.inference_mode()
-def decode_speculative(
-    input_ids,
-    model,
-    model_draft,
-    max_length,
-    speculative_lookahead=3,
-    top_k=1,
-    top_p=0.0,
-    temperature=1.0,
-    eos_token_id=None,
-    vocab_size=None,
-    tensor_parallel=1,
-    cg=False,
-    enable_timing=False,
-    debug=False,
-):
-    """
-    TD: WIP, for my own understanding, lightly tested. Only support batch_size == 1 for now.
-
-    Speculative decoding, either greedy or with top-k or top-p sampling.
-    If top-k = 0, don't limit the number of candidates (pure sampling).
-    Top-k and top-p can be used together. If top_k > 0 and top_p > 0, then top-k is applied first,
-    then top-p.
-    We assume that all sequences in the same batch have the same length.
-
-    Arguments:
-        input_ids: (batch, seq_len)
-        max_length: int
-    Returns: GreedySearchDecoderOnlyOutput or SampleDecoderOnlyOutput, with the following fields:
-        sequences: (batch, max_length)
-        scores: tuples of (batch, vocab_size)
-    """
-    batch_size, seqlen_og = input_ids.shape
-    assert batch_size == 1, "Speculative decoding implementation only supports batch_size=1"
-    assert eos_token_id is None, "Speculative decoding implementation doesn't support eos_token_id"
-    if cg:
-        if not hasattr(model_draft, "_decoding_cache"):
-            model_draft._decoding_cache = None
-        model_draft._decoding_cache = update_graph_cache(
-            model_draft,
-            model_draft._decoding_cache,
-            batch_size,
-            seqlen_og,
-            max_length,
-            # draft model needs to process either 1 or 2 tokens at a time
-            decoding_seqlens=(1, 2),
-            tensor_parallel=tensor_parallel,
-        )
-        inference_params_draft = model_draft._decoding_cache.inference_params
-        inference_params_draft.reset(max_length, batch_size)
-        if not hasattr(model, "_decoding_cache"):
-            model._decoding_cache = None
-        model._decoding_cache = update_graph_cache(
-            model,
-            model._decoding_cache,
-            batch_size,
-            seqlen_og,
-            max_length,
-            decoding_seqlens=range(1, speculative_lookahead + 2),
-            tensor_parallel=tensor_parallel,
-        )
-        inference_params = model._decoding_cache.inference_params
-        inference_params.reset(max_length, batch_size)
-    else:
-        inference_params_draft = InferenceParams(max_seqlen=max_length, max_batch_size=batch_size)
-        inference_params = InferenceParams(max_seqlen=max_length, max_batch_size=batch_size)
-
-    def get_logits(input_ids, inference_params, model, num_last_tokens=1, cg=False):
-        decoding = inference_params.seqlen_offset > 0
-        if decoding:
-            seqlen = input_ids.shape[1]
-            # if inference_params.lengths_per_sample is None:
-            # TODO: in the case of batched decoding where each sequence has a different length,
-            # we need to compute the position_ids for each sequence using lengths_per_sample
-            if True:
-                cache_seqlens = torch.full(
-                    (input_ids.shape[0],),
-                    inference_params.seqlen_offset,
-                    dtype=torch.int32,
-                    device=input_ids.device,
-                )
-            else:
-                cache_seqlens = inference_params.lengths_per_sample
-            position_ids = cache_seqlens[:, None] + torch.arange(
-                seqlen, dtype=torch.long, device=input_ids.device
-            )
-        else:
-            position_ids = None
-        if not cg or not decoding:
-            logits = model(
-                input_ids,
-                position_ids=position_ids,
-                inference_params=inference_params,
-                num_last_tokens=num_last_tokens,
-            ).logits
-        else:
-            # NOTE: careful, CUDA graph is set to have num_last_tokens=input_ids.shape[1].
-            # This might not be compatible the num_last_tokens used here.
-            assert num_last_tokens <= input_ids.shape[1]
-            logits = model._decoding_cache.run(
-                input_ids, position_ids, inference_params.seqlen_offset
-            )[:, -num_last_tokens:]
-        return logits[..., :vocab_size] if vocab_size is not None else logits
-
-    def sample_tokens(input_ids, get_logits_fn, inference_params, sample_fn, num_tokens=1):
-        """Sample `num_tokens` tokens from the model, given the previous logits.
-        Also return the logits of the sampled tokens.
-        Arguments:
-            input_ids: (batch, seqlen)
-        Return:
-            tokens: (batch, num_tokens)
-            scores: (batch, num_tokens), which contains @previous_logits and the logits of the next
-                (num_tokens - 1) tokens. The logits of the last token isn't computed.
-        """
-        assert num_tokens >= 1
-        sequences, scores = [input_ids], []
-        for i in range(num_tokens):
-            scores.append(get_logits_fn(sequences[-1], inference_params)[:, -1])
-            inference_params.seqlen_offset += sequences[-1].shape[1]
-            sequences.append(sample_fn(scores[-1]).unsqueeze(1))
-        return torch.cat(sequences[1:], dim=1), torch.stack(scores, dim=1)
-
-    sampling_kwargs = dict(top_k=top_k, top_p=top_p, temperature=temperature)
-    sample_fn = partial(sample, **sampling_kwargs)
-    get_logits_main = partial(get_logits, model=model, cg=cg)
-    get_logits_draft = partial(get_logits, model=model_draft, cg=cg)
-    sample_tokens_main = partial(
-        sample_tokens,
-        get_logits_fn=get_logits_main,
-        sample_fn=sample_fn,
-        inference_params=inference_params,
-    )
-    sample_tokens_draft = partial(
-        sample_tokens,
-        get_logits_fn=get_logits_draft,
-        sample_fn=sample_fn,
-        inference_params=inference_params_draft,
-    )
-
-    if debug:
-        from transformers import AutoTokenizer
-
-        tokenizer = AutoTokenizer.from_pretrained("gpt2")
-    if enable_timing:
-        if tensor_parallel > 1:
-            torch.distributed.barrier()
-        torch.cuda.synchronize()
-        start = time.time()
-
-    sequences, scores = [input_ids], []
-    num_main_model_calls = 0
-    num_draft_tokens = 0
-    num_accepted_tokens_history = []
-    if seqlen_og >= max_length - 1:
-        # Don't do speculative sampling, just sample 1 token from the model
-        tokens, scores_new = sample_tokens_main(input_ids, num_tokens=1)
-        sequences.append(tokens)
-        scores.append(scores_new)
-    else:
-        # Sample from draft model, which produces @n_spec_tokens, and @model
-        # will then use to produce between 1 and 1 + @n_spec_tokens tokens.
-        # We want seqlen_og + 1 + @n_spec_tokens to be <= @max_length.
-        n_spec_tokens = min(speculative_lookahead, max_length - seqlen_og - 1)
-        tokens_draft, scores_draft = sample_tokens_draft(input_ids, num_tokens=n_spec_tokens)
-        num_draft_tokens += n_spec_tokens
-        if debug:
-            scores_draft_ref = model_draft(
-                torch.cat([input_ids, tokens_draft], dim=1), num_last_tokens=n_spec_tokens + 1
-            ).logits
-            print((scores_draft - scores_draft_ref[:, :-1]).abs().max())
-
-        # Evaluate the draft tokens with the model
-        logits = get_logits_main(
-            torch.cat([input_ids, tokens_draft], dim=1),
-            inference_params,
-            num_last_tokens=n_spec_tokens + 1,
-        )
-        num_main_model_calls += 1
-        if debug:
-            logits_ref = model(
-                torch.cat([input_ids, tokens_draft], dim=1), num_last_tokens=n_spec_tokens + 1
-            ).logits
-            print((logits - logits_ref).abs().max())
-        tokens, num_generated_tokens = sample_speculative(
-            logits, scores_draft, tokens_draft, **sampling_kwargs
-        )
-        num_accepted_tokens_history.append(num_generated_tokens - 1)
-        if debug:
-            print(tokens)
-            print(num_generated_tokens)
-        # TODO: we're using the fact that batch_size == 1
-        # TODO: check eos_token_id
-        sequences.append(tokens[:1, : num_generated_tokens[0]])
-        scores.append(logits[:1, : num_generated_tokens[0]])
-        # Note that @model has not evaluated the last sampled token yet, so we'll need to pass
-        # that in the next time we call @model.
-        num_generated = num_generated_tokens[0].item()
-        inference_params.seqlen_offset = seqlen_og + num_generated - 1
-        inference_params_draft.seqlen_offset = (
-            inference_params.seqlen_offset - 1
-            if num_generated > 1
-            else inference_params.seqlen_offset
-        )
-        if debug:
-            cur_ids = torch.cat([input_ids, sequences[-1]], dim=1)
-            scores_ref = model(cur_ids, num_last_tokens=num_generated_tokens[0].item() + 1).logits
-            print((scores[-1] - scores_ref[:, :-1]).abs().max())
-
-    while True:
-        # seqlen_offset is total length generated - 1
-        if inference_params.seqlen_offset >= max_length - 1:
-            break
-        if inference_params.seqlen_offset >= max_length - 2:
-            # Don't do speculative sampling, just sample 1 token from the model
-            tokens, scores_new = sample_tokens_main(sequences[-1][:, -1:], num_tokens=1)
-            sequences.append(tokens)
-            scores.append(scores_new)
-            break
-        # Sample from draft model
-        n_spec_tokens = min(
-            speculative_lookahead, max_length - inference_params_draft.seqlen_offset - 2
-        )
-        # If the main model accepts all the draft tokens, plus it samples one new token,
-        # then at the next iteration the draft model need to evaluate the logits of the last draft
-        # token and the logits of the newly sampled token. So here we pass in the last 2 tokens
-        # of sequences[-1].
-        # This exception is when the main model rejects all the draft tokens, in which case we
-        # will only have 1 token to pass in.
-        tokens_draft, scores_draft = sample_tokens_draft(
-            sequences[-1][:, -2:], num_tokens=n_spec_tokens
-        )
-        num_draft_tokens += n_spec_tokens
-        if debug:
-            scores_draft_ref = model_draft(
-                torch.cat([cur_ids, tokens_draft], dim=1), num_last_tokens=n_spec_tokens + 1
-            ).logits
-            print((scores_draft - scores_draft_ref[:, :-1]).abs().max())
-
-        # Evaluate the draft tokens with the model
-        logits = get_logits_main(
-            torch.cat([sequences[-1][:, -1:], tokens_draft], dim=1),
-            inference_params,
-            num_last_tokens=n_spec_tokens + 1,
-        )  # (batch, n_spec_tokens + 1, vocab_size)
-        num_main_model_calls += 1
-        if debug:
-            logits_ref = model(
-                torch.cat([cur_ids, tokens_draft], dim=1), num_last_tokens=n_spec_tokens + 1
-            ).logits
-            print((logits - logits_ref).abs().max())
-        tokens, num_generated_tokens = sample_speculative(
-            logits, scores_draft, tokens_draft, **sampling_kwargs
-        )
-        num_accepted_tokens_history.append(num_generated_tokens - 1)
-        if debug:
-            print(tokens)
-            print(num_generated_tokens)
-        sequences.append(tokens[:1, : num_generated_tokens[0]])
-        scores.append(logits[:1, : num_generated_tokens[0]])
-        # We've evaluated 1 token from sequences[-1][:, -1:] above, plus
-        # num_generated_tokens[0].item() - 1 tokens from the draft model.
-        num_generated = num_generated_tokens[0].item()
-        inference_params.seqlen_offset += num_generated
-        inference_params_draft.seqlen_offset = (
-            inference_params.seqlen_offset - 1
-            if num_generated > 1
-            else inference_params.seqlen_offset
-        )
-        if debug:
-            cur_ids = torch.cat([cur_ids, sequences[-1]], dim=1)
-            scores_ref = model(cur_ids, num_last_tokens=num_generated_tokens[0].item() + 1).logits
-            print((scores[-1] - scores_ref[:, :-1]).abs().max())
-
-    if enable_timing:
-        if tensor_parallel > 1:
-            torch.distributed.barrier()
-        torch.cuda.synchronize()
-        print(f"Prompt processing + decoding time: {(time.time() - start) * 1000:.0f}ms")
-        print(f"Number of calls to main model: {num_main_model_calls}")
-        print(
-            f"Acceptance rate: {torch.cat(num_accepted_tokens_history).sum().item() / num_draft_tokens * 100:.2f}%"
-        )
-    sequences = torch.cat(sequences, dim=1)
-    scores = torch.cat(scores, dim=1)
-    if debug:
-        scores_ref = model(sequences).logits
-        print((scores - scores_ref[:, seqlen_og - 1 : -1]).abs().max())
-    output_cls = GreedySearchDecoderOnlyOutput if top_k == 1 else SampleDecoderOnlyOutput
-    return output_cls(sequences=sequences, scores=scores)
-
-
-class GenerationMixin:
-    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):
-        raise NotImplementedError
-
-    def generate(
-        self,
-        input_ids,
-        max_length,
-        top_k=1,
-        top_p=0.0,
-        temperature=1.0,
-        return_dict_in_generate=False,
-        output_scores=False,
-        **kwargs,
-    ):
-        output = decode(
-            input_ids, self, max_length, top_k=top_k, top_p=top_p, temperature=temperature, **kwargs
-        )
-        if not output_scores:
-            output.scores = None
-        return output if return_dict_in_generate else output.sequences
-
-
-def allocate_inference_cache(
-    max_batch_size,
-    max_seqlen,
-    nheads,
-    headdim,
-    layers: Union[int, Sequence],
-    device,
-    dtype=torch.float16,
-):
-    assert dtype in [torch.float16, torch.bfloat16, torch.float32]
-    packsize = 4 if dtype == torch.float32 else 8
-    assert headdim % packsize == 0
-    k_cache_shape = (max_batch_size, nheads, headdim // packsize, max_seqlen, packsize)
-    v_cache_shape = (max_batch_size, nheads, max_seqlen, headdim)
-    kv_cache_shape = (max_batch_size, max_seqlen, 2, nheads, headdim)
-    if isinstance(layers, int):
-        layers = range(layers)
-    return {i: torch.empty(kv_cache_shape, device=device, dtype=dtype) for i in layers}
-
-
-@dataclass
-class DecodingCGCache:
-    max_batch_size: int = 0
-    max_seqlen: int = 0
-    device = None
-    dtype = None
-    callables: dict = field(default_factory=dict)
-    mempool = None
-    inference_params: Optional[InferenceParams] = None
-    run: Optional[Callable] = None
-
-
-@torch.inference_mode()
-def update_graph_cache(
-    model,
-    cache,
-    batch_size,
-    seqlen_og,
-    max_seqlen,
-    decoding_seqlens=(1,),
-    tensor_parallel=1,
-    dtype=None,
-    n_warmups=2,
-):
-    if cache is None:
-        cache = DecodingCGCache()
-    param_example = next(iter(model.parameters()))
-    device = param_example.device
-    if dtype is None:
-        dtype = param_example.dtype
-    if (
-        (device, dtype) != (cache.device, cache.dtype)
-        or batch_size > cache.max_batch_size
-        or max_seqlen > cache.max_seqlen
-    ):  # Invalidate the cache
-        cache.callables = {}
-        cache.mempool = None
-        cache.inference_params = None
-        gc.collect()
-        cache.device, cache.dtype = device, dtype
-        cache.max_batch_size, cache.max_seqlen = batch_size, max_seqlen
-        if hasattr(model, "allocate_inference_cache"):
-            inf_cache = model.allocate_inference_cache(batch_size, max_seqlen, dtype)
-        else:
-            headdim = getattr(
-                model.config,
-                "head_dim",
-                model.config.hidden_size // model.config.num_attention_heads,
-            )
-            inf_cache = allocate_inference_cache(
-                batch_size,
-                max_seqlen,
-                model.config.num_attention_heads // tensor_parallel,
-                headdim,
-                model.config.num_hidden_layers,
-                device,
-                dtype,
-            )
-        lengths_per_sample = torch.full((batch_size,), seqlen_og, dtype=torch.int32, device=device)
-        cache.inference_params = InferenceParams(
-            max_seqlen=max_seqlen,
-            max_batch_size=batch_size,
-            seqlen_offset=seqlen_og,
-            key_value_memory_dict=inf_cache,
-            lengths_per_sample=lengths_per_sample,
-        )
-        cache.mempool = torch.cuda.graphs.graph_pool_handle()
-    for decoding_seqlen in decoding_seqlens:
-        if (batch_size, decoding_seqlen) not in cache.callables:
-            cache.callables[batch_size, decoding_seqlen] = capture_graph(
-                model,
-                cache.inference_params,
-                batch_size,
-                max_seqlen,
-                decoding_seqlen=decoding_seqlen,
-                mempool=cache.mempool,
-                n_warmups=n_warmups,
-            )
-
-    def dispatch(input_ids, position_ids, seqlen):
-        batch_size, decoding_seqlen = input_ids.shape[:2]
-        return cache.callables[batch_size, decoding_seqlen](input_ids, position_ids, seqlen)
-
-    cache.run = dispatch
-    cache.inference_params.seqlen_offset = 0  # Reset so it's not confusing
-    return cache
-
-
-def capture_graph(
-    model, inference_params, batch_size, max_seqlen, decoding_seqlen=1, mempool=None, n_warmups=2
-):
-    device = next(iter(model.parameters())).device
-    input_ids = torch.full((batch_size, decoding_seqlen), 0, dtype=torch.long, device=device)
-    position_ids = torch.full((batch_size, decoding_seqlen), 0, dtype=torch.long, device=device)
-    seqlen_offset_og = inference_params.seqlen_offset
-    inference_params.seqlen_offset = max_seqlen - decoding_seqlen
-    inference_params.lengths_per_sample[:] = inference_params.seqlen_offset
-
-    # Warmup before capture
-    s = torch.cuda.Stream()
-    s.wait_stream(torch.cuda.current_stream())
-    with torch.cuda.stream(s):
-        # SA: passing in stream for custom cuda calls
-        for _ in range(n_warmups):
-            logits = model(
-                input_ids,
-                position_ids=position_ids,
-                inference_params=inference_params,
-                num_last_tokens=decoding_seqlen,
-                stream=s,
-            ).logits
-        s.synchronize()
-        # This might be needed for correctness if we run with NCCL_GRAPH_MIXING_SUPPORT=0,
-        # which requires that graph launch and non-captured launch to not overlap (I think,
-        # that's how I interpret the documentation). I'm not sure if this is required.
-        if torch.distributed.is_initialized():
-            torch.distributed.barrier()
-    torch.cuda.current_stream().wait_stream(s)
-    # Captures the graph
-    # To allow capture, automatically sets a side stream as the current stream in the context
-    graph = torch.cuda.CUDAGraph()
-    with torch.cuda.graph(graph, pool=mempool):
-        logits = model(
-            input_ids,
-            position_ids=position_ids,
-            inference_params=inference_params,
-            num_last_tokens=decoding_seqlen,
-            stream=s,
-        ).logits
-
-    def run(new_input_ids, new_position_ids, seqlen):
-        inference_params.lengths_per_sample[:] = seqlen
-        input_ids.copy_(new_input_ids)
-        position_ids.copy_(new_position_ids)
-        graph.replay()
-        return logits.clone()
-
-    inference_params.seqlen_offset = seqlen_offset_og
-    return run
-
-
-@torch.inference_mode()
-def decode_naive(
-    input_ids,
-    model,
-    max_length,
-    top_k=1,
-    top_p=0.0,
-    temperature=1.0,
-    eos_token_id=None,
-    teacher_outputs=None,
-    vocab_size=None,
-    tensor_parallel=1,
-    cg=False,
-    enable_timing=False,
-):
-    """Decoding, either greedy or with top-k or top-p sampling.
-    If top-k = 0, don't limit the number of candidates (pure sampling).
-    Top-k and top-p can be used together. If top_k > 0 and top_p > 0, then top-k is applied first,
-    then top-p.
-    We assume that all sequences in the same batch have the same length.
-
-    Arguments:
-        input_ids: (batch, seq_len)
-        max_length: int
-        teacher_outputs (optional): (batch, seq_len). If provided, instead of sampling from the
-            logits, the next token is taken from the teacher_outputs. Useful for testing.
-    Returns: GreedySearchDecoderOnlyOutput or SampleDecoderOnlyOutput, with the following fields:
-        sequences: (batch, max_length)
-        scores: tuples of (batch, vocab_size)
-    """
-    batch_size, seqlen_og = input_ids.shape
-    teacher_output_len = teacher_outputs.shape[1] if teacher_outputs is not None else 0
-
-    inference_params = InferenceParams(max_seqlen=max_length, max_batch_size=batch_size)
-
-    def get_logits(input_ids, inference_params):
-        logits = model(
-            input_ids,
-            position_ids=None,
-            inference_params=None,  # need to pass in None so that we don't get use the cache
-            num_last_tokens=1,
-        ).logits.squeeze(dim=1)
-
-        return logits[..., :vocab_size] if vocab_size is not None else logits
-
-    def sample_tokens(logits, inference_params):
-        if teacher_outputs is None or teacher_output_len <= inference_params.seqlen_offset:
-            token = sample(logits, top_k=top_k, top_p=top_p, temperature=temperature)
-        else:
-            token = teacher_outputs[:, inference_params.seqlen_offset]
-        # return rearrange(token, "b -> b 1")
-        return token.unsqueeze(1)
-
-    def should_stop(current_token, inference_params):
-        if inference_params.seqlen_offset == 0:
-            return False
-        if eos_token_id is not None and (current_token == eos_token_id).all():
-            return True
-        if inference_params.seqlen_offset >= max_length - 1:
-            return True
-        return False
-
-    scores, sequences = [], [input_ids]
-    while not should_stop(sequences[-1], inference_params):
-        logits = get_logits(input_ids, inference_params)
-        scores.append(logits)
-        inference_params.seqlen_offset += sequences[-1].shape[1]
-        tokens = sample_tokens(scores[-1], inference_params)
-        input_ids = torch.cat([input_ids, tokens], dim=1)
-        sequences.append(tokens)
-    output_cls = GreedySearchDecoderOnlyOutput if top_k == 1 else SampleDecoderOnlyOutput
-    return output_cls(sequences=torch.cat(sequences, dim=1), scores=tuple(scores))
-
-
-class NaiveGenerationMixin:
-    """
-    Naive generation mixin for models that do not support generation recurrent inference.
-    Useful for testing and for sanity checking recurrent implementations.
-    """
-
-    def generate_naive(
-        self,
-        input_ids: torch.Tensor,
-        max_length,
-        top_k=1,
-        top_p=0.0,
-        temperature=1.0,
-        return_dict_in_generate=False,
-        output_scores=False,
-        **kwargs,
-    ):
-        print("Using naive generation")
-        output = decode_naive(
-            input_ids, self, max_length, top_k=top_k, top_p=top_p, temperature=temperature, **kwargs
-        )
-        if not output_scores:
-            output.scores = None
-        return output if return_dict_in_generate else output.sequences
-
diff --git a/based/models/__init__.py b/based/models/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/based/models/block.py b/based/models/block.py
deleted file mode 100755
index 0b63d64..0000000
--- a/based/models/block.py
+++ /dev/null
@@ -1,439 +0,0 @@
-# Copyright (c) 2022, Tri Dao.
-
-from functools import partial
-from typing import Optional
-
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-from torch import Tensor
-from torchvision.ops import StochasticDepth
-
-from flash_attn.modules.mha import MHA
-from flash_attn.modules.mlp import Mlp
-
-try:
-    from flash_attn.ops.layer_norm import dropout_add_layer_norm
-except ImportError:
-    dropout_add_layer_norm = None
-
-try:
-    from flash_attn.ops.layer_norm import dropout_add_layer_norm_parallel_residual
-except ImportError:
-    dropout_add_layer_norm_parallel_residual = None
-
-try:
-    from flash_attn.ops.rms_norm import RMSNorm, dropout_add_rms_norm
-except ImportError:
-    RMSNorm, dropout_add_rms_norm = None, None
-
-try:
-    from flash_attn.ops.rms_norm import dropout_add_rms_norm_parallel_residual
-except ImportError:
-    dropout_add_rms_norm_parallel_residual = None
-
-
-class Block(nn.Module):
-    def __init__(
-        self,
-        dim,
-        mixer_cls=None,
-        mlp_cls=None,
-        norm_cls=nn.LayerNorm,
-        dropout_cls=nn.Dropout,
-        prenorm=True,
-        resid_dropout1=0.0,
-        resid_dropout2=0.0,
-        drop_path1=0.0,
-        drop_path2=0.0,
-        fused_dropout_add_ln=False,
-        return_residual=False,
-        residual_in_fp32=False,
-        sequence_parallel=False,
-        mark_shared_params=False,
-        layer_idx=None,
-        **kwargs,
-    ):
-        """
-        For prenorm=True, this Block has a slightly different structure compared to a regular
-        prenorm Transformer block.
-        The standard block is: LN -> MHA -> Dropout -> Add -> LN -> MLP -> Dropout -> Add.
-        [Ref: https://arxiv.org/abs/2002.04745]
-        Here we have: Dropout -> Add -> LN -> MHA -> Dropout -> Add -> LN -> MLP, returning both
-        the hidden_states (output of the MLP) and the residual.
-        This is for performance reasons, as we can fuse the dropout, add and LayerNorm.
-        The residual needs to be provided (except for the very first block).
-
-        For prenorm=False, this Block has the same structure as a regular postnorm Transformer
-        block: MHA -> Dropout -> Add -> LN -> MLP -> Dropout -> Add -> LN.
-
-        return_residual: whether each of the sub-layers (mixer and mlp) will return the residual.
-        This is for performance reason: for post-norm architecture, returning the input allows us
-        to fuse the backward of nn.Linear with the residual connection.
-        """
-        super().__init__()
-
-        self.prenorm = prenorm
-        self.fused_dropout_add_ln = fused_dropout_add_ln
-        self.return_residual = return_residual
-        self.residual_in_fp32 = residual_in_fp32
-        if self.residual_in_fp32:
-            assert self.prenorm, "residual_in_fp32 is only compatible with prenorm=True"
-        if mixer_cls is None:
-            mixer_cls = partial(MHA, num_heads=dim // 64)
-        if mlp_cls is None:
-            mlp_cls = partial(Mlp, hidden_features=4 * dim)
-        self.mixer = mixer_cls(dim)
-        self.dropout1 = dropout_cls(resid_dropout1)
-        self.drop_path1 = StochasticDepth(drop_path1, mode="row")
-        self.norm1 = norm_cls(dim)
-        self.mlp = mlp_cls(dim)
-        if not isinstance(self.mlp, nn.Identity):
-            self.dropout2 = dropout_cls(resid_dropout2)
-            self.drop_path2 = StochasticDepth(drop_path2, mode="row")
-            self.norm2 = norm_cls(dim)
-
-        if self.fused_dropout_add_ln:
-            assert dropout_add_layer_norm is not None, "dropout_layer_norm is not installed"
-            assert dropout_add_rms_norm is not None, "dropout_layer_norm is not installed"
-            assert isinstance(self.norm1, (nn.LayerNorm, RMSNorm)) and isinstance(
-                self.dropout1, nn.Dropout
-            )
-
-        self.layer_idx = layer_idx
-
-        # TD [2023-01-07]: TODO: During training, if sequence_parallel is False and dropout != 0.0,
-        # then the input to each worker in the tensor parallel group will be different.
-        # This would produce wrong outputs? Somehow we'd need to sync the RNG state across workers.
-        # For now this is not an issue because we always use sequence_parallel=True during training
-        # and only use sequence_parallel=False during inference.
-
-        # Mark the norm parameters as "sequence_parallel" so that we run all-reduce on their grads.
-        if sequence_parallel:
-            for p in self.norm1.parameters():
-                p._sequence_parallel = True
-            if hasattr(self, "norm2"):
-                for p in self.norm2.parameters():
-                    p._sequence_parallel = True
-        # Mark the norm parameters as "shared_params" so that we sync their values at init.
-        if mark_shared_params:
-            for p in self.norm1.parameters():
-                p._shared_params = True
-            if hasattr(self, "norm2"):
-                for p in self.norm2.parameters():
-                    p._shared_params = True
-
-    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):
-        return self.mixer.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs)
-
-    def forward(
-        self,
-        hidden_states: Tensor,
-        residual: Optional[Tensor] = None,
-        mixer_subset=None,
-        mixer_kwargs=None,
-        position_ids=None,
-        decay=None,
-        stream=None,
-    ):  
-        r"""Pass the input through the encoder layer.
-
-        Args:
-            hidden_states: the sequence to the encoder layer (required).
-            residual: if postnorm, residual=None, If prenorm, hidden_states = Attn/MLP(LN(residual))
-            mixer_subset: for cross-attention only. If not None, will take a subset of x
-                before applying the query projection. Useful for e.g., ViT where we only care
-                about the CLS token in the last layer.
-        """
-        fused_add_norm_fn = (
-            dropout_add_rms_norm
-            if RMSNorm and isinstance(self.norm1, RMSNorm)
-            else dropout_add_layer_norm
-        )
-        
-        if self.prenorm:
-            if not self.fused_dropout_add_ln:
-                dropped = self.drop_path1(self.dropout1(hidden_states))
-                residual = (dropped + residual) if residual is not None else dropped
-                hidden_states = self.norm1(residual.to(dtype=self.norm1.weight.dtype))
-                if self.residual_in_fp32:
-                    residual = residual.to(torch.float32)
-            else:
-                if self.drop_path1.p == 0 or not self.training:
-                    rowscale1 = None
-                else:
-                    rowscale1 = self.drop_path1(
-                        torch.ones(
-                            hidden_states.shape[:-1],
-                            device=hidden_states.device,
-                            dtype=hidden_states.dtype,
-                        )
-                    )
-                hidden_states, residual = fused_add_norm_fn(
-                    hidden_states,
-                    residual,
-                    self.norm1.weight,
-                    self.norm1.bias,
-                    self.dropout1.p if self.training else 0.0,
-                    self.norm1.eps,
-                    rowscale=rowscale1,
-                    prenorm=True,
-                    residual_in_fp32=self.residual_in_fp32,
-                )
-            if mixer_kwargs is None:
-                mixer_kwargs = {}
-            
-            if mixer_subset is not None:
-                mixer_kwargs["mixer_subset"] = mixer_subset
-            if position_ids is not None or decay is not None:
-                hidden_states = self.mixer(hidden_states, position_ids=position_ids, decay=decay, **mixer_kwargs)
-            else:
-                hidden_states = self.mixer(hidden_states, **mixer_kwargs)
-            if mixer_subset is not None:
-                residual = residual[:, mixer_subset]
-            if not isinstance(self.mlp, nn.Identity):
-                if not self.fused_dropout_add_ln:
-                    dropped = self.drop_path2(self.dropout2(hidden_states))
-                    residual = (dropped + residual) if residual is not None else dropped
-                    hidden_states = self.norm2(residual.to(dtype=self.norm2.weight.dtype))
-                    if self.residual_in_fp32:
-                        residual = residual.to(torch.float32)
-                else:
-                    if self.drop_path2.p == 0 or not self.training:
-                        rowscale2 = None
-                    else:
-                        rowscale2 = self.drop_path2(
-                            torch.ones(
-                                hidden_states.shape[:-1],
-                                device=hidden_states.device,
-                                dtype=hidden_states.dtype,
-                            )
-                        )
-                    hidden_states, residual = fused_add_norm_fn(
-                            hidden_states,
-                            residual,
-                            self.norm2.weight,
-                            self.norm2.bias,
-                            self.dropout2.p if self.training else 0.0,
-                            self.norm2.eps,
-                            rowscale=rowscale2,
-                            prenorm=True,
-                            residual_in_fp32=self.residual_in_fp32,
-                        )
-                hidden_states = self.mlp(hidden_states)
-            return hidden_states, residual
-        else:
-            assert residual is None
-            if position_ids is not None or decay is not None:
-                mixer_out = self.mixer(
-                    hidden_states, position_ids=position_ids, decay=decay, **(mixer_kwargs if mixer_kwargs is not None else {})
-                )
-            else:
-                mixer_out = self.mixer(hidden_states, **(mixer_kwargs if mixer_kwargs is not None else {}))
-            if self.return_residual:  # mixer out is actually a pair here
-                mixer_out, hidden_states = mixer_out
-            if not self.fused_dropout_add_ln:
-                hidden_states = self.norm1(
-                    (self.drop_path1(self.dropout1(mixer_out)) + hidden_states).to(
-                        dtype=self.norm1.weight.dtype
-                    )
-                )
-            else:
-                if self.drop_path1.p == 0 or not self.training:
-                    rowscale1 = None
-                else:
-                    rowscale1 = self.drop_path1(
-                        torch.ones(
-                            mixer_out.shape[:-1], device=mixer_out.device, dtype=mixer_out.dtype
-                        )
-                    )
-                hidden_states = fused_add_norm_fn(
-                    mixer_out,
-                    hidden_states,
-                    self.norm1.weight,
-                    self.norm1.bias,
-                    self.dropout1.p if self.training else 0.0,
-                    self.norm1.eps,
-                    rowscale=rowscale1,
-                    prenorm=False,
-                )
-            if not isinstance(self.mlp, nn.Identity):
-                mlp_out = self.mlp(hidden_states)
-                if self.return_residual:  # mlp out is actually a pair here
-                    mlp_out, hidden_states = mlp_out
-                if not self.fused_dropout_add_ln:
-                    hidden_states = self.norm2(
-                        (self.drop_path2(self.dropout2(mlp_out)) + hidden_states).to(
-                            dtype=self.norm2.weight.dtype
-                        )
-                    )
-                else:
-                    if self.drop_path2.p == 0 or not self.training:
-                        rowscale2 = None
-                    else:
-                        rowscale2 = self.drop_path2(
-                            torch.ones(
-                                mlp_out.shape[:-1], device=mlp_out.device, dtype=mlp_out.dtype
-                            )
-                        )
-                    hidden_states = fused_add_norm_fn(
-                        mlp_out,
-                        hidden_states,
-                        self.norm2.weight,
-                        self.norm2.bias,
-                        self.dropout2.p if self.training else 0.0,
-                        self.norm2.eps,
-                        rowscale=rowscale2,
-                        prenorm=False,
-                    )
-            return hidden_states
-
-
-class ParallelBlock(nn.Module):
-    """The attention (mixer) and MLP blocks are done in parallel, similar to GPT-J, GPT-NeoX,
-    and PaLM.
-    """
-
-    def __init__(
-        self,
-        dim,
-        mixer_cls=None,
-        mlp_cls=None,
-        norm_cls=nn.LayerNorm,
-        dropout_cls=nn.Dropout,
-        resid_dropout1=0.0,
-        resid_dropout2=0.0,
-        tied_norm=False,
-        fused_dropout_add_ln=False,
-        residual_in_fp32=False,
-        sequence_parallel=False,
-        mark_shared_params=False,
-    ):
-        """
-        This Block has a slightly different structure compared to a regular
-        prenorm Transformer block.
-        The standard block is: LN -> MHA / MLP -> Dropout -> Add.
-        [Ref: https://arxiv.org/abs/2002.04745]
-        Here we have: Dropout -> Add -> LN -> MHA / MLP, returning both
-        the hidden_states (output1 of the MHA / MLP) and the residual.
-        This is for performance reasons, as we can fuse the dropout, add and LayerNorm.
-        The residual needs to be provided (except for the very first block).
-        """
-        super().__init__()
-        self.tied_norm = tied_norm
-        self.fused_dropout_add_ln = fused_dropout_add_ln
-        self.residual_in_fp32 = residual_in_fp32
-        if mixer_cls is None:
-            mixer_cls = partial(MHA, num_heads=dim // 64)
-        if mlp_cls is None:
-            mlp_cls = partial(Mlp, hidden_features=4 * dim)
-        self.mixer = mixer_cls(dim)
-        self.dropout1 = dropout_cls(resid_dropout1)
-        self.norm1 = norm_cls(dim)
-        self.mlp = mlp_cls(dim)
-        self.dropout2 = dropout_cls(resid_dropout2)
-        if not self.tied_norm:
-            self.norm2 = norm_cls(dim)
-
-        if self.fused_dropout_add_ln:
-            assert (
-                dropout_add_layer_norm_parallel_residual is not None
-            ), "dropout_layer_norm is not installed"
-            assert (
-                dropout_add_rms_norm_parallel_residual is not None
-            ), "dropout_layer_norm is not installed"
-            assert isinstance(self.norm1, (nn.LayerNorm, RMSNorm)) and isinstance(
-                self.dropout1, nn.Dropout
-            )
-
-        # TD [2023-01-07]: TODO: During training, if sequence_parallel is False and dropout != 0.0,
-        # then the input to each worker in the tensor parallel group will be different.
-        # This would produce wrong outputs? Somehow we'd need to sync the RNG state across workers.
-        # For now this is not an issue because we always use sequence_parallel=True during training
-        # and only use sequence_parallel=False during inference.
-
-        # Mark the norm parameters as "sequence_parallel" so that we run all-reduce on their grads.
-        if sequence_parallel:
-            for p in self.norm1.parameters():
-                p._sequence_parallel = True
-            if hasattr(self, "norm2"):
-                for p in self.norm2.parameters():
-                    p._sequence_parallel = True
-        # Mark the norm parameters as "shared_params" so that we sync their values at init.
-        if mark_shared_params:
-            for p in self.norm1.parameters():
-                p._shared_params = True
-            if hasattr(self, "norm2"):
-                for p in self.norm2.parameters():
-                    p._shared_params = True
-
-    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):
-        return self.mixer.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs)
-
-    def forward(
-        self,
-        hidden_states1: Tensor,
-        hidden_states2: Optional[Tensor] = None,
-        residual: Optional[Tensor] = None,
-        mixer_kwargs=None,
-        position_ids=None, 
-        decay=None,
-    ):
-        r"""Pass the input through the encoder layer.
-
-        Args:
-            hidden_states1: the output of the previous attention (mixer) or embedding layer.
-            hidden_states2: the output of the previous MLP layer (if None, will use hidden_states1).
-            residual.
-        """
-        # TODO: Ideally we should only do the allgather / allreduce once for
-        # the Linear to MLP & Attention
-        fused_add_norm_fn = (
-            dropout_add_rms_norm_parallel_residual
-            if isinstance(self.norm1, RMSNorm)
-            else dropout_add_layer_norm_parallel_residual
-        )
-        if not self.fused_dropout_add_ln:
-            dropped1 = self.dropout1(hidden_states1)
-            # For the very 1st block, we only want 1 dropout, not two different dropouts
-            if hidden_states2 is not None:
-                dropped2 = self.dropout2(hidden_states2)
-                residual = (
-                    (residual + dropped1 + dropped2)
-                    if residual is not None
-                    else dropped1 + dropped2
-                )
-            else:
-                residual = (residual + dropped1) if residual is not None else dropped1
-            hidden_states1 = self.norm1(residual.to(dtype=self.norm1.weight.dtype))
-            hidden_states2 = (
-                self.norm2(residual.to(dtype=self.norm2.weight.dtype))
-                if not self.tied_norm
-                else hidden_states1
-            )
-            if self.residual_in_fp32:
-                residual = residual.to(torch.float32)
-        else:
-            weight2, bias2 = (
-                (self.norm2.weight, self.norm2.bias) if not self.tied_norm else (None, None)
-            )
-            hidden_states1, hidden_states2, residual = fused_add_norm_fn(
-                hidden_states1,
-                hidden_states2,
-                residual,
-                self.norm1.weight,
-                self.norm1.bias,
-                weight2,
-                bias2,
-                self.dropout1.p if self.training else 0.0,
-                self.norm1.eps,
-                prenorm=True,
-                residual_in_fp32=self.residual_in_fp32,
-            )
-            if self.tied_norm:
-                hidden_states2 = hidden_states1
-        if mixer_kwargs is None:
-            mixer_kwargs = {}
-        hidden_states1 = self.mixer(hidden_states1, position_ids=position_ids, decay=decay, **mixer_kwargs)
-        hidden_states2 = self.mlp(hidden_states2)
-        return hidden_states1, hidden_states2, residual
diff --git a/based/models/gpt.py b/based/models/gpt.py
deleted file mode 100755
index 0d2238f..0000000
--- a/based/models/gpt.py
+++ /dev/null
@@ -1,1234 +0,0 @@
-# Copyright (c) 2023, Tri Dao.
-
-import logging
-import math
-import re
-from collections import OrderedDict, namedtuple
-from collections.abc import Sequence
-from functools import partial
-from typing import Dict, List
-
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-from einops import rearrange
-from transformers import GPT2Config
-import hydra
-
-from flash_attn.modules.block import Block, ParallelBlock
-from .block import Block, ParallelBlock
-from flash_attn.modules.embedding import GPT2Embeddings, ParallelGPT2Embeddings
-from flash_attn.modules.mha import MHA, ParallelMHA
-from flash_attn.modules.mlp import (
-    FusedMLP,
-    # GatedMlp,
-    Mlp,
-    ParallelFusedMLP,
-    # ParallelGatedMlp,
-    ParallelMLP,
-)
-from .mlp import GatedMlp, ParallelGatedMlp
-from flash_attn.ops.activations import sqrelu_fwd
-from flash_attn.utils.distributed import all_gather_raw, get_dim_for_local_rank, sync_shared_params
-# from flash_attn.utils.generation import GenerationMixin
-from based.generation import GenerationMixin, NaiveGenerationMixin
-from flash_attn.utils.pretrained import state_dict_from_pretrained
-
-try:
-    from flash_attn.ops.fused_dense import ColumnParallelLinear
-except ImportError:
-    ColumnParallelLinear = None
-
-try:
-    from flash_attn.ops.layer_norm import dropout_add_layer_norm
-except ImportError:
-    dropout_add_layer_norm = None
-
-try:
-    from flash_attn.ops.layer_norm import dropout_add_layer_norm_parallel_residual
-except ImportError:
-    dropout_add_layer_norm_parallel_residual = None
-
-try:
-    # from based.ops.triton.layer_norm import RMSNorm
-    # dropout_add_rms_norm = None
-    from flash_attn.ops.rms_norm import RMSNorm, dropout_add_rms_norm
-except ImportError:
-    RMSNorm, dropout_add_rms_norm = None, None
-# You can replace the above import with the following if you do not install the flash_attention kernel
-# from based.ops.triton.layer_norm import RMSNorm
-
-try:
-    from flash_attn.ops.rms_norm import dropout_add_rms_norm_parallel_residual
-except ImportError:
-    dropout_add_rms_norm_parallel_residual = None
-
-try:
-    from flash_attn.ops.triton.mlp import FusedDenseSqreluDense
-except ImportError:
-    FusedDenseSqreluDense = None
-
-logger = logging.getLogger(__name__)
-
-# torch.backends.cuda.matmul.allow_tf32 = False   # FLAG
-
-from based.utils.hf import load_config_hf, load_state_dict_hf
-
-
-class GPT2MixerConfig(GPT2Config):
-    def __init__(self, *args, **kwargs):
-        self.mixer = kwargs.pop("mixer", None)
-        super().__init__(*args, **kwargs)
-
-
-def create_mixer_cls(config, layer_idx=None, process_group=None, device=None, dtype=None):
-    tag = 'mixer'
-    value = config.mixer
-    alt_mixer_layers = getattr(config, "alt_mixer_layers", None)
-    alt_mixer_2_layers = getattr(config, "alt_mixer_2_layers", None)
-    alt_mixer = getattr(config, "alt_mixer", None)
-    alt_mixer_2 = getattr(config, "alt_mixer_2", None)
-    if alt_mixer_2_layers is not None and layer_idx in alt_mixer_2_layers:
-        value = None
-        if alt_mixer_2 is not None:
-            tag = 'alt_mixer_2'
-            value = config.alt_mixer_2
-    elif alt_mixer_layers is not None and layer_idx in alt_mixer_layers:
-        value = None
-        if alt_mixer is not None:
-            tag = 'alt_mixer'
-            value = config.alt_mixer
-    if (not hasattr(config, tag)) or value is None:
-        return create_mha_cls(config, layer_idx, process_group=process_group, device=device)
-    import os
-    os.environ["HYDRA_FULL_ERROR"] = "1"
-
-    # SA: Use the FA settings to initialize the mixer (e.g. for rotary_emb_dim)
-    if "slide" in value['_target_']:
-        value = sliding_window_additions(value, config, layer_idx=layer_idx, process_group=process_group, device=device)
-
-    return hydra.utils.instantiate(
-        value, 
-        _partial_=True, 
-        device=device, 
-        dtype=dtype, 
-        layer_idx=layer_idx,
-    )
-
-
-def sliding_window_additions(value, config, layer_idx, process_group, device):
-    head_dim = getattr(config, "head_dim", config.hidden_size // config.num_attention_heads)
-    softmax_scale = 1.0 if not config.scale_attn_weights else head_dim ** (-0.5)
-    if config.scale_attn_by_inverse_layer_idx:
-        assert layer_idx is not None
-        softmax_scale /= float(layer_idx + 1)
-
-    qkv_proj_bias = getattr(config, "qkv_proj_bias", True)
-    out_proj_bias = getattr(config, "out_proj_bias", True)
-    rotary_emb_dim = int(getattr(config, "rotary_emb_fraction", 0.0) * head_dim)
-    rotary_emb_base = getattr(config, "rotary_emb_base", 10000.0)
-    rotary_emb_scale_base = getattr(config, "rotary_emb_scale_base", None)
-    rotary_emb_interleaved = getattr(config, "rotary_emb_interleaved", False)
-    use_flash_attn = getattr(config, "use_flash_attn", False)
-    fused_bias_fc = getattr(config, "fused_bias_fc", False)
-
-    serial_kwargs = (
-        {"fused_bias_fc": fused_bias_fc} if process_group is None else {}
-    )
-    parallel_kwargs = (
-        {
-            "process_group": process_group,
-            "sequence_parallel": getattr(config, "sequence_parallel", True),
-        }
-        if process_group is not None
-        else {}
-    )
-    num_heads_kv = getattr(config, "n_head_kv", None)
-
-    value.update({
-        "num_heads": config.num_attention_heads,
-        "num_heads_kv": num_heads_kv,
-        "qkv_proj_bias": qkv_proj_bias,
-        "out_proj_bias": out_proj_bias,
-        "dropout": config.attn_pdrop,
-        "softmax_scale": softmax_scale,
-        "causal": True,
-        "layer_idx": layer_idx,
-        "rotary_emb_dim": rotary_emb_dim,
-        "rotary_emb_base": rotary_emb_base,
-        "rotary_emb_scale_base": rotary_emb_scale_base,
-        "rotary_emb_interleaved": rotary_emb_interleaved,
-        "use_flash_attn": use_flash_attn,
-        **serial_kwargs,
-        **parallel_kwargs,
-    })
-    return value
-
-
-def create_mha_cls(config, layer_idx=None, process_group=None, device=None, dtype=None):
-    factory_kwargs = {"device": device, "dtype": dtype}
-
-    head_dim = getattr(config, "head_dim", config.hidden_size // config.num_attention_heads)
-    softmax_scale = 1.0 if not config.scale_attn_weights else head_dim ** (-0.5)
-    if config.scale_attn_by_inverse_layer_idx:
-        assert layer_idx is not None
-        softmax_scale /= float(layer_idx + 1)
-    dwconv = getattr(config, "attn_dwconv", False)
-    if dwconv:
-        assert process_group is None, "TensorParallel MHA does not support dwconv yet"
-    qkv_proj_bias = getattr(config, "qkv_proj_bias", True)
-    out_proj_bias = getattr(config, "out_proj_bias", True)
-    rotary_emb_dim = int(getattr(config, "rotary_emb_fraction", 0.0) * head_dim)
-    rotary_emb_base = getattr(config, "rotary_emb_base", 10000.0)
-    rotary_emb_scale_base = getattr(config, "rotary_emb_scale_base", None)
-    rotary_emb_interleaved = getattr(config, "rotary_emb_interleaved", False)
-    use_flash_attn = getattr(config, "use_flash_attn", False)
-    fused_bias_fc = getattr(config, "fused_bias_fc", False)
-    if not fused_bias_fc:
-        assert process_group is None, "TensorParallel MHA requires fused_bias_fc"
-    mha_cls = MHA if process_group is None else ParallelMHA
-    serial_kwargs = (
-        {"fused_bias_fc": fused_bias_fc, "dwconv": dwconv} if process_group is None else {}
-    )
-    parallel_kwargs = (
-        {
-            "process_group": process_group,
-            "sequence_parallel": getattr(config, "sequence_parallel", True),
-        }
-        if process_group is not None
-        else {}
-    )
-    num_heads_kv = getattr(config, "n_head_kv", None)
-    mixer_cls = partial(
-        mha_cls,
-        num_heads=config.num_attention_heads,
-        num_heads_kv=num_heads_kv,
-        qkv_proj_bias=qkv_proj_bias,
-        out_proj_bias=out_proj_bias,
-        dropout=config.attn_pdrop,
-        softmax_scale=softmax_scale,
-        causal=True,
-        layer_idx=layer_idx,
-        rotary_emb_dim=rotary_emb_dim,
-        rotary_emb_base=rotary_emb_base,
-        rotary_emb_scale_base=rotary_emb_scale_base,
-        rotary_emb_interleaved=rotary_emb_interleaved,
-        use_flash_attn=use_flash_attn,
-        **serial_kwargs,
-        **parallel_kwargs,
-        **factory_kwargs,
-    )
-    return mixer_cls
-
-
-def create_mlp_cls(config, layer_idx=None, process_group=None, device=None, dtype=None):
-    factory_kwargs = {"device": device, "dtype": dtype}
-    mlp_fc1_bias = getattr(config, "mlp_fc1_bias", True)
-    mlp_fc2_bias = getattr(config, "mlp_fc2_bias", True)
-    fused_mlp = getattr(config, "fused_mlp", False)
-    if fused_mlp:
-        assert config.activation_function in [
-            "gelu_new",
-            "gelu_fast",
-            "gelu_approx",
-            "gelu_pytorch_tanh",
-            "relu",
-            "sqrelu",
-        ]
-    fused_dense_sqrelu_dense = getattr(config, "fused_dense_sqrelu_dense", False)
-    if fused_dense_sqrelu_dense:
-        assert config.activation_function == "sqrelu", (
-            "fused_dense_sqrelu_dense only " "supports approximate activation_function sqrelu"
-        )
-    assert not (fused_dense_sqrelu_dense and fused_mlp)
-    if not fused_mlp and not fused_dense_sqrelu_dense:
-        # print(f"mlp inner_dim -- not fused: {config.n_inner}")
-        assert config.activation_function in [
-            "gelu",
-            "gelu_new",
-            "gelu_fast",
-            "gelu_approx",
-            "gelu_pytorch_tanh",
-            "relu",
-            "sqrelu",
-            "glu",
-            "swiglu",
-            "geglu",
-        ]
-        if config.activation_function in ["glu", "swiglu", "geglu"]:
-            activation = (
-                F.sigmoid
-                if config.activation_function == "glu"
-                else (F.silu if config.activation_function == "swiglu" else F.gelu)
-            )
-            mlp_cls = GatedMlp if process_group is None else ParallelGatedMlp
-            # print(f"{mlp_cls=}")
-            parallel_kwargs = (
-                {
-                    "process_group": process_group,
-                    "sequence_parallel": getattr(config, "sequence_parallel", True),
-                }
-                if process_group is not None
-                else {}
-            )
-            mlp_type = getattr(config, "mlp_type", 'base')
-            mlp_cls = partial(
-                mlp_cls,
-                hidden_features=config.n_inner,
-                activation=activation,
-                bias1=mlp_fc1_bias,
-                bias2=mlp_fc2_bias,
-                mlp_type=mlp_type,
-                ff_mult=getattr(config, "ff_mult", 2),
-                **parallel_kwargs,
-                **factory_kwargs,
-            )
-        else:
-            if config.activation_function == "relu":
-                activation = partial(F.relu, inplace=True)
-            elif config.activation_function == "sqrelu":
-                activation = sqrelu_fwd
-            else:
-                approximate = (
-                    "tanh"
-                    if config.activation_function
-                    in ["gelu_new", "gelu_fast", "gelu_approx", "gelu_pytorch_tanh"]
-                    else "none"
-                )
-                activation = partial(F.gelu, approximate=approximate)
-            mlp_cls = Mlp if process_group is None else ParallelMLP
-            parallel_kwargs = (
-                {
-                    "process_group": process_group,
-                    "sequence_parallel": getattr(config, "sequence_parallel", True),
-                }
-                if process_group is not None
-                else {}
-            )
-            mlp_cls = partial(
-                mlp_cls,
-                hidden_features=config.n_inner,
-                activation=activation,
-                bias1=mlp_fc1_bias,
-                bias2=mlp_fc2_bias,
-                **parallel_kwargs,
-                **factory_kwargs,
-            )
-    else:
-        print(f"mlp inner_dim -- fused: {config.n_inner}")
-        mlp_checkpoint_lvl = getattr(config, "mlp_checkpoint_lvl", 0)
-        # mlp_checkpoint_lvl could be a list, which contains the checkpoint_lvl for each layer
-        if isinstance(mlp_checkpoint_lvl, Sequence):
-            assert layer_idx is not None
-            mlp_checkpoint_lvl = mlp_checkpoint_lvl[layer_idx]
-        if fused_mlp:
-            if FusedMLP is None:
-                raise ImportError("fused_dense is not installed")
-            activation = (
-                "gelu_approx"
-                if config.activation_function
-                in ["gelu_new", "gelu_fast", "gelu_approx", "gelu_pytorch_tanh"]
-                else config.activation_function
-            )
-            mlp_cls = FusedMLP if process_group is None else ParallelFusedMLP
-            parallel_kwargs = (
-                {
-                    "process_group": process_group,
-                    "sequence_parallel": getattr(config, "sequence_parallel", True),
-                }
-                if process_group is not None
-                else {}
-            )
-            mlp_cls = partial(
-                mlp_cls,
-                hidden_features=config.n_inner,
-                activation=activation,
-                checkpoint_lvl=mlp_checkpoint_lvl,
-                bias1=mlp_fc1_bias,
-                bias2=mlp_fc2_bias,
-                **parallel_kwargs,
-                **factory_kwargs,
-            )
-        elif fused_dense_sqrelu_dense:
-            if process_group is not None:
-                assert fused_mlp, "Tensor Parallel is not implemented for FusedDenseSqreluDense"
-            assert FusedDenseSqreluDense is not None
-            mlp_cls = partial(
-                FusedDenseSqreluDense,
-                hidden_features=config.n_inner,
-                checkpoint_lvl=mlp_checkpoint_lvl,
-                **factory_kwargs,
-            )
-        else:
-            raise RuntimeError("MLP type not supported")
-    return mlp_cls
-
-
-def create_block(config, layer_idx=None, process_group=None, device=None, dtype=None, **kwargs):
-    factory_kwargs = {"device": device, "dtype": dtype}
-    sequence_parallel = getattr(config, "sequence_parallel", True)
-    mixer_cls = create_mixer_cls(config, layer_idx, process_group=process_group, **factory_kwargs)
-    mlp_cls = create_mlp_cls(config, layer_idx, process_group=process_group, **factory_kwargs)
-    use_rms_norm = getattr(config, "rms_norm", False)
-    norm_cls = partial(
-        nn.LayerNorm if not use_rms_norm else RMSNorm,
-        eps=config.layer_norm_epsilon,
-        **factory_kwargs,
-    )
-    # TD [2022-07-30]: Force residual in fp32, seems to make fp16 training more stable
-    residual_in_fp32 = getattr(config, "residual_in_fp32", False)
-    resid_dropout1 = config.resid_pdrop if layer_idx is None or layer_idx > 0 else config.embd_pdrop
-    prenorm = getattr(config, "prenorm", True)
-    parallel_block = getattr(config, "parallel_block", False)
-    if not parallel_block:
-        block = Block(
-            config.hidden_size,
-            mixer_cls,
-            mlp_cls,
-            norm_cls=norm_cls,
-            prenorm=prenorm,
-            resid_dropout1=resid_dropout1,
-            resid_dropout2=config.resid_pdrop,
-            fused_dropout_add_ln=getattr(config, "fused_dropout_add_ln", False),
-            residual_in_fp32=residual_in_fp32,
-            sequence_parallel=sequence_parallel and process_group is not None,
-            mark_shared_params=process_group is not None,
-            layer_idx=layer_idx,
-        )
-    else:
-        assert prenorm
-        block = ParallelBlock(
-            config.hidden_size,
-            mixer_cls,
-            mlp_cls,
-            norm_cls=norm_cls,
-            resid_dropout1=resid_dropout1,
-            resid_dropout2=config.resid_pdrop,
-            tied_norm=getattr(config, "parallel_block_tied_norm", False),
-            fused_dropout_add_ln=getattr(config, "fused_dropout_add_ln", False),
-            residual_in_fp32=residual_in_fp32,
-            sequence_parallel=sequence_parallel and process_group is not None,
-            mark_shared_params=process_group is not None,
-        )
-    block.layer_idx = layer_idx
-    return block
-
-
-class GPTPreTrainedModel(nn.Module):
-    """An abstract class to handle weights initialization and
-    a simple interface for dowloading and loading pretrained models.
-    """
-
-    def __init__(self, config, *inputs, **kwargs):
-        super().__init__()
-        if not isinstance(config, GPT2Config):
-            raise ValueError(
-                "Parameter config in `{}(config)` should be an instance of class `GPT2Config`. "
-                "To create a model from a Google pretrained model use "
-                "`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`".format(
-                    self.__class__.__name__, self.__class__.__name__
-                )
-            )
-        self.config = config
-
-    @classmethod
-    def from_pretrained(
-        cls,
-        model_name,
-        config,
-        *args,
-        strict=True,
-        device=None,
-        dtype=None,
-        world_size=1,
-        rank=0,
-        **kwargs,
-    ):
-        """
-        Instantiate a GPTPreTrainedModel from a pre-trained model file or a pytorch state dict.
-        Download and cache the pre-trained model file if needed.
-        """
-        # Instantiate model.
-        model = cls(config, *args, device=device, dtype=dtype, **kwargs)
-        # Load state_dict in cpu because we already initialized the model in GPU, and we don't
-        # want extra stuff taking up more GPU memory
-        state_dict = state_dict_from_pretrained(model_name, device="cpu", dtype=dtype)
-        if model_name.startswith("gpt2"):
-            state_dict = remap_state_dict_hf_gpt2(state_dict, config)
-        else:
-            raise NotImplementedError(f"Model {model_name} not supported")
-        if world_size > 1:
-            state_dict = shard_state_dict_tp(state_dict, config, world_size, rank)
-        load_return = model.load_state_dict(state_dict, strict=strict)
-        logger.info(load_return)
-        return model
-        
-    @classmethod
-    def from_pretrained_hf(cls, pretrained_model_name, device=None, **kwargs):
-        config_data = load_config_hf(pretrained_model_name)
-        config = GPT2Config(**config_data)
-        model = cls(config, device=device, **kwargs)
-        state_dict = load_state_dict_hf(pretrained_model_name, device=device)
-
-        # remove the 'model.' prefix from the keys
-        state_dict = {re.sub("^model\.", "", k): v for k, v in state_dict.items()}
-        # remove Unexpected key(s) in state_dict: "train_metrics.num-tokens.count", "val_metrics.num-tokens.count", "test_metrics.num-tokens.count". from the state_dict
-        state_dict = {k: v for k, v in state_dict.items() if "metrics" not in k}
-
-        model.load_state_dict(state_dict)
-        return model.to(device=device)
-
-
-# https://github.com/huggingface/transformers/blob/c28d04e9e252a1a099944e325685f14d242ecdcd/src/transformers/models/gpt2/modeling_gpt2.py#L454
-def _init_weights(module, n_layer, initializer_range=0.02, rescale_prenorm_residual=True, use_weight_init=True):
-    if isinstance(module, nn.Linear):
-        if use_weight_init:
-            nn.init.normal_(module.weight, std=initializer_range)   # SA: this line isn't in Mamba init code
-        else:
-            print(f"Skipping!")
-        if module.bias is not None:
-            if not getattr(module.bias, "_no_reinit", False):
-                nn.init.zeros_(module.bias)
-    elif isinstance(module, nn.Embedding):
-        nn.init.normal_(module.weight, std=initializer_range)
-
-    if rescale_prenorm_residual:
-        # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:
-        #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale
-        #   > the weights of residual layers at initialization by a factor of 1/N where N is the # of residual layers.
-        #   >   -- GPT-2 :: https://openai.com/blog/better-language-models/
-        #
-        # Reference (Megatron-LM): https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py
-        for name, p in module.named_parameters():
-            if name in ["out_proj.weight", "fc2.weight"]:
-                # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block
-                nn.init.normal_(p, mean=0.0, std=initializer_range / math.sqrt(2 * n_layer))
-
-
-class DecayClass(nn.Module):
-    def __init__(self, config):
-        super().__init__()
-        self.l_max = config.mixer.get('l_max', None)
-        if self.l_max is None:
-            self.l_max = config.alt_mixer.get('l_max', None)
-        assert self.l_max > 0, print(f'double check l_max')
-        decay_const = getattr(config, "decay_const", -5)
-        self.decay_denom = getattr(config, "decay_denom", True)
-        decay = torch.log(1 - 2 ** (decay_const - torch.arange(config.n_head, dtype=torch.float)))
-        self.register_buffer("decay", decay)
-    
-    def forward(self):
-        index = torch.arange(self.l_max).to(self.decay)
-        mask = torch.tril(torch.ones(self.l_max, self.l_max).to(self.decay))
-        mask = torch.masked_fill(index[:, None] - index[None, :], ~mask.bool(), float("inf"))
-        mask = torch.exp(mask * self.decay[:, None, None])
-        mask = torch.nan_to_num(mask)
-        if self.decay_denom:
-            mask = mask / mask.sum(dim=-1, keepdim=True).sqrt()
-        return mask, torch.exp(self.decay)
-
-
-class GPTModel(GPTPreTrainedModel):
-    def __init__(self, config: GPT2Config, process_group=None, device=None, dtype=None):
-        super().__init__(config)
-        factory_kwargs = {"device": device, "dtype": dtype}
-        self.process_group = process_group
-        self.sequence_parallel = getattr(config, "sequence_parallel", True)
-        assert config.activation_function in [
-            "gelu",
-            "gelu_new",
-            "gelu_fast",
-            "gelu_approx",
-            "gelu_pytorch_tanh",
-            "relu",
-            "sqrelu",
-            "glu",
-            "swiglu",
-            "geglu",
-        ]
-        pad_vocab_size_multiple = getattr(config, "pad_vocab_size_multiple", 1)
-        vocab_size = (
-            math.ceil(config.vocab_size / pad_vocab_size_multiple) * pad_vocab_size_multiple
-        )
-        # TD [2022-07-30]: Force residual in fp32, seems to make fp16 training more stable
-        self.residual_in_fp32 = getattr(config, "residual_in_fp32", False)
-        # These 2 options are for OPT-350m
-        self.prenorm = getattr(config, "prenorm", True)
-        use_rms_norm = getattr(config, "rms_norm", False)
-        word_embed_proj_dim = getattr(config, "word_embed_proj_dim", None)
-        # For GPT-J, GPT-NeoX
-        self.parallel_block = getattr(config, "parallel_block", False)
-
-        if process_group is None:
-            self.embeddings = GPT2Embeddings(
-                config.hidden_size,
-                vocab_size,
-                config.max_position_embeddings,
-                word_embed_proj_dim=word_embed_proj_dim,
-                **factory_kwargs,
-            )
-        else:
-            self.embeddings = ParallelGPT2Embeddings(
-                config.hidden_size,
-                vocab_size,
-                config.max_position_embeddings,
-                process_group=process_group,
-                sequence_parallel=self.sequence_parallel,
-                **factory_kwargs,
-            )
-
-        # We change the order of dropout, residual and layer norm:
-        # Instead of LN -> Attn / MLP -> Dropout -> Add, we do:
-        # Dropout -> Add -> LN -> Attn / MLP, returning both the residual branch (output of Add) and
-        # the main branch (output of MLP). The model definition is unchanged, but the mapping of the
-        # nn.Dropout probabilities are changed.
-        # This is for performance reason: we can fuse dropout + add + layer_norm.
-        self.layers = nn.ModuleList(
-            [
-                create_block(config, layer_idx=i, process_group=process_group, **factory_kwargs)
-                for i in range(config.num_hidden_layers)
-            ]
-        )
-        self.fused_dropout_add_ln = getattr(config, "fused_dropout_add_ln", False)
-        if self.fused_dropout_add_ln:
-            if (not self.parallel_block and dropout_add_layer_norm is None) or (
-                self.parallel_block and dropout_add_layer_norm_parallel_residual is None
-            ):
-                raise ImportError("dropout_layer_norm is not installed")
-        if self.prenorm:
-            self.drop_f = nn.Dropout(config.resid_pdrop)
-            norm_cls = nn.LayerNorm if not use_rms_norm else RMSNorm
-            self.ln_f = norm_cls(
-                config.hidden_size, eps=config.layer_norm_epsilon, **factory_kwargs
-            )
-        if process_group is not None:
-            for p in self.ln_f.parameters():
-                # Mark the norm parameters as "shared_params" so that we sync their values at init.
-                p._shared_params = True
-                # Mark the norm params as "sequence_parallel" so we run all-reduce on their grads.
-                if self.sequence_parallel:
-                    p._sequence_parallel = True
-        
-        if getattr(config, "special_initializer", False):
-            initializer_range = (2 / (config.n_embd * 5)) ** 0.5
-        else:
-            initializer_range = config.initializer_range
-
-        if getattr(config, 'fixed_decay', False):
-            self.decay = DecayClass(config)
-        else:
-            self.decay = None
-
-        self.apply(
-            partial(
-                _init_weights,
-                n_layer=config.num_hidden_layers,
-                initializer_range=config.initializer_range,
-                use_weight_init=getattr(config, "use_weight_init", True),
-            )
-        )
-        self.tie_weights()
-
-    def tie_weights(self):
-        if self.process_group is not None:
-            sync_shared_params(self, self.process_group)
-
-    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):
-        return {
-            i: layer.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs)
-            for i, layer in enumerate(self.layers)
-        }
-
-    def forward(self, input_ids, position_ids=None, inference_params=None, attention_mask=None, stream=None):
-        # If using Tensor Parallel with sequence parallel, we combine the batch and the seqlen
-        # dimensions so that we can split on it easily, in case of small batch size.
-        # Only the attention layers need to know the seqlen.
-        embedding_kwargs = (
-            {"combine_batch_seqlen_dim": True}
-            if self.process_group is not None and self.sequence_parallel
-            else {}
-        )
-        hidden_states = self.embeddings(input_ids, position_ids=position_ids, **embedding_kwargs)
-        if self.parallel_block:
-            hidden_states2 = None
-        residual = None
-        mixer_kwargs = (
-            {"seqlen": input_ids.shape[1]}
-            if self.process_group is not None and self.sequence_parallel
-            else {}
-        )
-        if inference_params is not None:
-            mixer_kwargs["inference_params"] = inference_params
-            mixer_kwargs['stream'] = stream
-
-        # decay
-        if self.decay is not None:
-            decay = self.decay()
-        else:
-            decay = None
-
-        for layer in self.layers:
-            if self.prenorm:
-                layer_name = layer.mixer.__class__.__name__
-                if not self.parallel_block and layer_name not in ['MHA']:
-                    hidden_states, residual = layer(
-                        hidden_states, residual=residual, position_ids=position_ids, decay=decay, mixer_kwargs=mixer_kwargs
-                    )
-                elif not self.parallel_block and layer_name in ['MHA']:
-                    hidden_states, residual = layer(hidden_states, residual=residual, mixer_kwargs=mixer_kwargs)
-                else:
-                    hidden_states, hidden_states2, residual = layer(
-                        hidden_states, hidden_states2, residual=residual, position_ids=position_ids, decay=decay, mixer_kwargs=mixer_kwargs
-                    )
-            else:
-                hidden_states = layer(hidden_states, position_ids=position_ids, mixer_kwargs=mixer_kwargs)
-        if self.prenorm:
-            if not self.fused_dropout_add_ln:
-                dropped = self.drop_f(hidden_states)
-                if not self.parallel_block:
-                    residual = (dropped + residual) if residual is not None else dropped
-                else:
-                    dropped2 = self.drop_f(hidden_states2)
-                    residual = (
-                        (residual + dropped + dropped2)
-                        if residual is not None
-                        else dropped + dropped2
-                    )
-                hidden_states = self.ln_f(residual.to(dtype=self.ln_f.weight.dtype))
-            else:
-                # Set prenorm=False here since we don't need the residual
-                if not self.parallel_block:
-                    fused_add_norm_fn = (
-                        dropout_add_rms_norm
-                        if isinstance(self.ln_f, RMSNorm)
-                        else dropout_add_layer_norm
-                    )
-                    hidden_states = fused_add_norm_fn(
-                        hidden_states,
-                        residual,
-                        self.ln_f.weight,
-                        self.ln_f.bias,
-                        self.drop_f.p if self.training else 0.0,
-                        self.ln_f.eps,
-                        prenorm=False,
-                        residual_in_fp32=self.residual_in_fp32,
-                    )
-                else:
-                    fused_add_norm_fn = (
-                        dropout_add_rms_norm_parallel_residual
-                        if isinstance(self.ln_f, RMSNorm)
-                        else dropout_add_layer_norm_parallel_residual
-                    )
-                    hidden_states, _ = fused_add_norm_fn(
-                        hidden_states,
-                        hidden_states2,
-                        residual,
-                        self.ln_f.weight,
-                        self.ln_f.bias,
-                        None,
-                        None,
-                        self.drop_f.p if self.training else 0.0,
-                        self.ln_f.eps,
-                        prenorm=False,
-                        residual_in_fp32=self.residual_in_fp32,
-                    )
-        return hidden_states
-
-
-class GPTLMHeadModel(GPTPreTrainedModel, GenerationMixin, NaiveGenerationMixin):
-    def __init__(self, config: GPT2Config, process_group=None, device=None, dtype=None):
-        factory_kwargs = {"device": device, "dtype": dtype}
-        super().__init__(config)
-        self.process_group = process_group
-        self.transformer = GPTModel(config, process_group=process_group, **factory_kwargs)
-        self.tie_word_embeddings = getattr(config, "tie_word_embeddings", True)
-        lm_head_bias = getattr(config, "lm_head_bias", False)
-        pad_vocab_size_multiple = getattr(config, "pad_vocab_size_multiple", 1)
-        vocab_size = (
-            math.ceil(config.vocab_size / pad_vocab_size_multiple) * pad_vocab_size_multiple
-        )
-        # This option is for OPT-350m
-        word_embed_proj_dim = getattr(config, "word_embed_proj_dim", None)
-        embed_dim = config.n_embd if word_embed_proj_dim is None else word_embed_proj_dim
-        if word_embed_proj_dim is not None:
-            self.project_out = nn.Linear(config.n_embd, embed_dim, bias=False, **factory_kwargs)
-        else:
-            self.project_out = None
-        if process_group is None:
-            self.lm_head = nn.Linear(embed_dim, vocab_size, bias=lm_head_bias, **factory_kwargs)
-        else:
-            if ColumnParallelLinear is None:
-                raise ImportError("fused_dense_lib is not installed")
-            self.lm_head = ColumnParallelLinear(
-                embed_dim,
-                vocab_size,
-                process_group,
-                bias=lm_head_bias,
-                sequence_parallel=getattr(config, "sequence_parallel", True),
-                **factory_kwargs,
-            )
-        # Initialize weights and apply final processing
-        self.apply(
-            partial(
-                _init_weights,
-                n_layer=config.num_hidden_layers,
-                initializer_range=config.initializer_range,
-            )
-        )
-        self.tie_weights()
-        
-    def tie_weights(self):
-        if self.tie_word_embeddings:
-            # print(f"Tying Weights")
-            # import time
-            self.lm_head.weight = self.transformer.embeddings.word_embeddings.weight
-        if self.process_group is not None:
-            sync_shared_params(self, self.process_group)
-
-    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):
-        return self.transformer.allocate_inference_cache(
-            batch_size, max_seqlen, dtype=dtype, **kwargs
-        )
-
-    def forward(self, input_ids, position_ids=None, inference_params=None, num_last_tokens=0, stream=None, **kwargs):
-        """
-        input_ids: (batch, seqlen) int tensor
-        inference_params: for generation. Adapted from Megatron-LM (and Apex)
-        https://github.com/NVIDIA/apex/blob/3ff1a10f72ec07067c4e44759442329804ac5162/apex/transformer/testing/standalone_transformer_lm.py#L470
-        num_last_tokens: if > 0, only return the logits for the last n tokens
-        """
-        if type(input_ids) == list:
-            input_ids = input_ids[0]    
-        assert (
-            input_ids.ndim == 2
-        ), f"Expected `input_ids` to have shape [b, slen], but got shape {input_ids.shape}"
-        b, slen = input_ids.shape
-        hidden_states = self.transformer(
-            input_ids, position_ids=position_ids, inference_params=inference_params,
-            stream=stream
-        )
-        if inference_params is not None:
-            assert hidden_states.ndim == 3, "sequence_parallel is not supported in generation mode"
-        if num_last_tokens > 0:
-            hidden_states = hidden_states[:, -num_last_tokens:]
-        if self.project_out is not None:
-            hidden_states = self.project_out(hidden_states)
-        lm_logits = self.lm_head(hidden_states)
-        # During inference, we want the full logit for sampling
-        if ColumnParallelLinear is not None and isinstance(self.lm_head, ColumnParallelLinear) and inference_params is not None:
-            lm_logits, _ = all_gather_raw(lm_logits, self.lm_head.process_group)
-            lm_logits = rearrange(lm_logits, "(n b) ... d -> b ... (n d)", b=b)
-        CausalLMOutput = namedtuple("CausalLMOutput", ["logits"])
-        return CausalLMOutput(logits=lm_logits)
-
-    def load_state_dict(self, state_dict, strict=True):
-        # Remapping from our checkpoints that used a different ordering of layers in the block
-        # Previous: Attn / MLP -> Dropout -> Add -> LN
-        # Current: Dropout -> Add -> LN -> Attn / MLP
-        if "transformer.ln_0.weight" in state_dict:
-            n_layers = len(self.transformer.layers)
-            ln_weight = state_dict.pop(f"transformer.layers.{n_layers - 1}.norm2.weight")
-            ln_bias = state_dict.pop(f"transformer.layers.{n_layers - 1}.norm2.bias")
-            state_dict["transformer.ln_f.weight"] = ln_weight
-            state_dict["transformer.ln_f.bias"] = ln_bias
-            for l in reversed(range(n_layers)):
-                ln_weight = state_dict.pop(f"transformer.layers.{l}.norm1.weight")
-                ln_bias = state_dict.pop(f"transformer.layers.{l}.norm1.bias")
-                state_dict[f"transformer.layers.{l}.norm2.weight"] = ln_weight
-                state_dict[f"transformer.layers.{l}.norm2.bias"] = ln_bias
-                if l > 0:
-                    ln_weight = state_dict.pop(f"transformer.layers.{l - 1}.norm2.weight")
-                    ln_bias = state_dict.pop(f"transformer.layers.{l - 1}.norm2.bias")
-                    state_dict[f"transformer.layers.{l}.norm1.weight"] = ln_weight
-                    state_dict[f"transformer.layers.{l}.norm1.bias"] = ln_bias
-            ln_weight = state_dict.pop("transformer.ln_0.weight")
-            ln_bias = state_dict.pop("transformer.ln_0.bias")
-            state_dict[f"transformer.layers.0.norm1.weight"] = ln_weight
-            state_dict[f"transformer.layers.0.norm1.bias"] = ln_bias
-        return super().load_state_dict(state_dict, strict=strict)
-
-
-def shard_state_dict_tp(state_dict, config, world_size, rank):
-    """Convert the state_dict of a standard GPT model to the state_dict of a GPT model
-    with tensor parallel.
-
-    This function modifies state_dict in place.
-    """
-    pad_vocab_size_multiple = getattr(config, "pad_vocab_size_multiple", 1)
-    vocab_size = math.ceil(config.vocab_size / pad_vocab_size_multiple) * pad_vocab_size_multiple
-    assert vocab_size % world_size == 0
-    assert config.hidden_size % world_size == 0
-    inner_dim = config.n_inner if config.n_inner is not None else 4 * config.hidden_size
-    # print(f"inner_dim: {inner_dim}")
-    assert inner_dim % world_size == 0
-
-    n_head = config.n_head
-    n_head_kv = getattr(config, "n_head_kv", n_head)
-
-    embed_dim = config.hidden_size
-    head_dim = embed_dim // n_head
-
-    def shard_first_dim(state_dict, key):
-        if key in state_dict:
-            x = state_dict[key]
-            dim = x.shape[0] // world_size
-            state_dict[key] = x[rank * dim : (rank + 1) * dim]
-
-    def shard_last_dim(state_dict, key, multiple_of=1):
-        if key in state_dict:
-            x = state_dict[key]
-            dim_each_rank = [
-                get_dim_for_local_rank(x.size(-1), world_size, local_rank, multiple_of)
-                for local_rank in range(world_size)
-            ]
-            beg, end = tuple(sum(dim_each_rank[:pos]) for pos in (rank, rank + 1))
-            state_dict[key] = x[..., beg:end]
-
-    def shard_gatedmlp_fc1_dim(state_dict, key):
-        if key in state_dict:
-            x = state_dict[key]
-            dim = x.shape[0] // world_size // 2
-            state_dict[key] = rearrange(
-                rearrange(x, "(two o) ... -> two o ...", two=2)[:, rank * dim : (rank + 1) * dim],
-                "two o ... -> (two o) ...",
-            )
-
-    def shard_qkv_headdim(state_dict, key):
-        if key in state_dict:
-            n_head_each_rank = [
-                get_dim_for_local_rank(n_head, world_size, local_rank)
-                for local_rank in range(world_size)
-            ]
-            n_head_kv_each_rank = [
-                get_dim_for_local_rank(n_head_kv, world_size, local_rank)
-                for local_rank in range(world_size)
-            ]
-
-            beg_n_head = sum(n_head_each_rank[:rank])
-            end_n_head = sum(n_head_each_rank[: rank + 1])
-
-            beg_n_head_kv = sum(n_head_kv_each_rank[:rank])
-            end_n_head_kv = sum(n_head_kv_each_rank[: rank + 1])
-
-            if n_head_kv == n_head:
-                x = rearrange(state_dict[key], "(three d) ... -> three d ...", three=3)
-                state_dict[key] = rearrange(
-                    x[:, beg_n_head * head_dim : end_n_head * head_dim],
-                    "three d ... -> (three d) ...",
-                )
-            else:
-                x = rearrange(
-                    state_dict[key],
-                    "(nheadqkv headdim) ... -> nheadqkv headdim ...",
-                    nheadqkv=n_head + 2 * n_head_kv,
-                )
-                state_dict[key] = rearrange(
-                    torch.cat(
-                        [
-                            x[beg_n_head:end_n_head],
-                            x[n_head + beg_n_head_kv : n_head + end_n_head_kv],
-                            x[
-                                n_head
-                                + n_head_kv
-                                + beg_n_head_kv : n_head
-                                + n_head_kv
-                                + end_n_head_kv
-                            ],
-                        ],
-                        dim=0,
-                    ),
-                    "nheadqkv headdim ... -> (nheadqkv headdim) ...",
-                )
-
-    shard_first_dim(state_dict, "transformer.embeddings.word_embeddings.weight")
-    if "lm_head.weight" in state_dict:
-        shard_first_dim(state_dict, "lm_head.weight")
-    if "transformer.embeddings.position_embeddings.weight" in state_dict:
-        shard_last_dim(state_dict, "transformer.embeddings.position_embeddings.weight")
-    for i in range(config.num_hidden_layers):
-        shard_qkv_headdim(state_dict, f"transformer.layers.{i}.mixer.Wqkv.weight")
-        shard_qkv_headdim(state_dict, f"transformer.layers.{i}.mixer.Wqkv.bias")
-        shard_last_dim(
-            state_dict, f"transformer.layers.{i}.mixer.out_proj.weight", multiple_of=head_dim
-        )
-        if rank != 0:
-            state_dict.pop(f"transformer.layers.{i}.mixer.out_proj.bias", None)
-        if config.activation_function in ["glu", "swiglu", "geglu"]:
-            shard_gatedmlp_fc1_dim(state_dict, f"transformer.layers.{i}.mlp.fc1.weight")
-            shard_gatedmlp_fc1_dim(state_dict, f"transformer.layers.{i}.mlp.fc1.bias")
-        else:
-            shard_first_dim(state_dict, f"transformer.layers.{i}.mlp.fc1.weight")
-            shard_first_dim(state_dict, f"transformer.layers.{i}.mlp.fc1.bias")
-        shard_last_dim(state_dict, f"transformer.layers.{i}.mlp.fc2.weight")
-        if rank != 0:
-            state_dict.pop(f"transformer.layers.{i}.mlp.fc2.bias", None)
-    return state_dict
-
-
-def combine_state_dicts_tp(state_dicts: List[Dict[str, torch.Tensor]], config: GPT2Config):
-    """Convert the list of sharded state_dict of a GPT model with tensor parallel to
-    the state_dict of a standard GPT model.
-
-    This function is meant to be the "reverse" of shard_state_dict_tp.
-
-    Precondition:
-        - state_dicts should be ordered in the same way as the shards were created.
-    """
-    world_size = len(state_dicts)
-    keys = state_dicts[0].keys()
-    pad_vocab_size_multiple = getattr(config, "pad_vocab_size_multiple", 1)
-    vocab_size = math.ceil(config.vocab_size / pad_vocab_size_multiple) * pad_vocab_size_multiple
-    assert vocab_size % world_size == 0
-    assert config.hidden_size % world_size == 0
-    inner_dim = config.n_inner if config.n_inner is not None else 4 * config.hidden_size
-    assert inner_dim % world_size == 0
-    assert config.hidden_size % config.n_head == 0
-    headdim = config.hidden_size // config.n_head
-
-    # Sometimes the word embeddings are sharded on the 0th dim, sometimes on the 1st dim.
-    # vocab_size // world_size coordinates are nonzero.
-    def combine_word_embeddings(state_dicts, state_dict, key):
-        dim = 0 if state_dicts[0][key].shape[0] == vocab_size // world_size else 1
-        state_dict[key] = torch.cat([s[key] for s in state_dicts], dim=dim)
-
-    def combine_dim(state_dicts, state_dict, key, dim=-1):
-        if key in state_dict:
-            state_dict[key] = torch.cat([s[key] for s in state_dicts], dim=dim)
-
-    def combine_qkv_headdim(state_dicts, state_dict, key):
-        n_head = config.n_head
-        n_head_kv = getattr(config, "n_head_kv", n_head)
-        if key in state_dict:
-            if n_head_kv == n_head:
-                xs = [
-                    rearrange(s[key], "(three d) ... -> three d ...", three=3) for s in state_dicts
-                ]
-                state_dict[key] = rearrange(torch.cat(xs, dim=1), "three d ... -> (three d) ...")
-            else:
-                n_head_each_rank = [
-                    get_dim_for_local_rank(n_head, world_size, local_rank)
-                    for local_rank in range(world_size)
-                ]
-                n_head_kv_each_rank = [
-                    get_dim_for_local_rank(n_head_kv, world_size, local_rank)
-                    for local_rank in range(world_size)
-                ]
-                xs = [
-                    rearrange(
-                        s[key],
-                        "(nheadqkv headdim) ... -> nheadqkv headdim ...",
-                        nheadqkv=rank_n_head + 2 * rank_n_head_kv,
-                        headdim=headdim,
-                    )
-                    for s, rank_n_head, rank_n_head_kv in zip(
-                        state_dicts, n_head_each_rank, n_head_kv_each_rank
-                    )
-                ]
-                wq = torch.cat([x[: n_head_each_rank[rank]] for rank, x in enumerate(xs)], dim=0)
-                wk = torch.cat(
-                    [
-                        x[
-                            n_head_each_rank[rank] : n_head_each_rank[rank]
-                            + n_head_kv_each_rank[rank]
-                        ]
-                        for rank, x in enumerate(xs)
-                    ],
-                    dim=0,
-                )
-                wv = torch.cat(
-                    [
-                        x[n_head_each_rank[rank] + n_head_kv_each_rank[rank] :]
-                        for rank, x in enumerate(xs)
-                    ],
-                    dim=0,
-                )
-                wqkv = torch.cat(
-                    [wq, wk, wv],
-                    dim=0,
-                )
-                state_dict[key] = rearrange(
-                    wqkv,
-                    "nheadqkv headdim ... -> (nheadqkv headdim) ...",
-                )
-
-    def combine_gated_mlp(state_dicts, state_dict, key):
-        if key in state_dict:
-            xs = [rearrange(s[key], "(two d) ... -> two d ...", two=2) for s in state_dicts]
-            state_dict[key] = rearrange(torch.cat(xs, dim=1), "two d ... -> (two d) ...")
-
-    state_dict = state_dicts[0].copy()  # don't modify state_dict[0] inplace
-    combine_word_embeddings(
-        state_dicts, state_dict, "transformer.embeddings.word_embeddings.weight"
-    )
-    if "lm_head.weight" in state_dict:
-        combine_word_embeddings(state_dicts, state_dict, "lm_head.weight")
-    if "transformer.embeddings.position_embeddings.weight" in state_dict:
-        combine_dim(
-            state_dicts, state_dict, "transformer.embeddings.position_embeddings.weight", -1
-        )
-    mlp_combine_fn = (
-        combine_gated_mlp
-        if config.activation_function in ["glu", "swiglu", "geglu"]
-        else partial(combine_dim, dim=0)
-    )
-    for i in range(config.num_hidden_layers):
-        combine_qkv_headdim(state_dicts, state_dict, f"transformer.layers.{i}.mixer.Wqkv.weight")
-        combine_qkv_headdim(state_dicts, state_dict, f"transformer.layers.{i}.mixer.Wqkv.bias")
-        combine_dim(state_dicts, state_dict, f"transformer.layers.{i}.mixer.out_proj.weight", -1)
-        mlp_combine_fn(state_dicts, state_dict, f"transformer.layers.{i}.mlp.fc1.weight")
-        combine_dim(state_dicts, state_dict, f"transformer.layers.{i}.mlp.fc1.bias", 0)
-        combine_dim(state_dicts, state_dict, f"transformer.layers.{i}.mlp.fc2.weight", -1)
-    return state_dict
-
-
-def remap_state_dict_hf_gpt2(state_dict, config):
-    # Word embedding and position embedding
-    def key_mapping_pos_emb(key):
-        return re.sub(r"^wpe.", "transformer.embeddings.position_embeddings.", key)
-
-    state_dict = OrderedDict((key_mapping_pos_emb(k), v) for k, v in state_dict.items())
-    word_embeddings = state_dict.pop("wte.weight")
-    # It's possible that vocab_size is padded to be a multiple of 8, for example.
-    pad_vocab_size_multiple = getattr(config, "pad_vocab_size_multiple", 1)
-    vocab_size = math.ceil(config.vocab_size / pad_vocab_size_multiple) * pad_vocab_size_multiple
-    state_dict["transformer.embeddings.word_embeddings.weight"] = F.pad(
-        word_embeddings, (0, 0, 0, vocab_size - word_embeddings.shape[0])
-    )
-    state_dict["lm_head.weight"] = state_dict["transformer.embeddings.word_embeddings.weight"]
-
-    # LayerNorm
-    def key_mapping_ln(key):
-        key = re.sub(r"^ln_f.(weight|bias)", r"transformer.ln_f.\1", key)
-        key = re.sub(r"^h.(\d+).ln_(1|2).(weight|bias)", r"transformer.layers.\1.norm\2.\3", key)
-        return key
-
-    state_dict = OrderedDict((key_mapping_ln(k), v) for k, v in state_dict.items())
-
-    # MLP
-    for d in range(config.num_hidden_layers):
-        W1 = state_dict.pop(f"h.{d}.mlp.c_fc.weight")
-        state_dict[f"transformer.layers.{d}.mlp.fc1.weight"] = W1.t()
-        W2 = state_dict.pop(f"h.{d}.mlp.c_proj.weight")
-        state_dict[f"transformer.layers.{d}.mlp.fc2.weight"] = W2.t()
-
-    def key_mapping_mlp(key):
-        key = re.sub(r"^h.(\d+).mlp.c_fc.bias", r"transformer.layers.\1.mlp.fc1.bias", key)
-        key = re.sub(r"^h.(\d+).mlp.c_proj.bias", r"transformer.layers.\1.mlp.fc2.bias", key)
-        return key
-
-    state_dict = OrderedDict((key_mapping_mlp(k), v) for k, v in state_dict.items())
-
-    # Attention
-    for d in range(config.num_hidden_layers):
-        state_dict.pop(f"h.{d}.attn.bias")  # We don't store this bias
-        Wqkv = state_dict.pop(f"h.{d}.attn.c_attn.weight")
-        state_dict[f"transformer.layers.{d}.mixer.Wqkv.weight"] = Wqkv.t()
-        Wout = state_dict.pop(f"h.{d}.attn.c_proj.weight")
-        state_dict[f"transformer.layers.{d}.mixer.out_proj.weight"] = Wout.t()
-
-    def key_mapping_attn(key):
-        key = re.sub(r"^h.(\d+).attn.c_attn.bias", r"transformer.layers.\1.mixer.Wqkv.bias", key)
-        key = re.sub(
-            r"^h.(\d+).attn.c_proj.bias", r"transformer.layers.\1.mixer.out_proj.bias", key
-        )
-        return key
-
-    state_dict = OrderedDict((key_mapping_attn(k), v) for k, v in state_dict.items())
-
-    return state_dict
-
-
-def remap_state_dict_megatron(state_dict, config):
-    def key_mapping_transformer(key):
-        key = re.sub(r"^language_model.encoder.", "transformer.", key)
-        key = re.sub(r"^language_model.", "transformer.", key)
-        return key
-
-    state_dict = OrderedDict((key_mapping_transformer(k), v) for k, v in state_dict.items())
-
-    # Word embedding and position embedding
-    def key_mapping_pos_emb(key):
-        return re.sub(r"^wpe.", "transformer.embeddings.position_embeddings.", key)
-
-    state_dict = OrderedDict((key_mapping_pos_emb(k), v) for k, v in state_dict.items())
-    word_embeddings = state_dict.pop("transformer.embedding.word_embeddings.weight")
-    # It's possible that vocab_size is padded to be a multiple of 8, for example.
-    pad_vocab_size_multiple = getattr(config, "pad_vocab_size_multiple", 1)
-    vocab_size = (
-        math.ceil(word_embeddings.shape[0] / pad_vocab_size_multiple) * pad_vocab_size_multiple
-    )
-    state_dict["transformer.embeddings.word_embeddings.weight"] = F.pad(
-        word_embeddings, (0, 0, 0, vocab_size - word_embeddings.shape[0])
-    )
-    state_dict["lm_head.weight"] = state_dict["transformer.embeddings.word_embeddings.weight"]
-
-    # LayerNorm
-    def key_mapping_ln(key):
-        key = re.sub(r"^transformer.final_layernorm.(weight|bias)", r"transformer.ln_f.\1", key)
-        key = re.sub(
-            r"^transformer.layers.(\d+).input_layernorm.(weight|bias)",
-            r"transformer.layers.\1.norm1.\2",
-            key,
-        )
-        key = re.sub(
-            r"^transformer.layers.(\d+).post_attention_layernorm.(weight|bias)",
-            r"transformer.layers.\1.norm2.\2",
-            key,
-        )
-        return key
-
-    state_dict = OrderedDict((key_mapping_ln(k), v) for k, v in state_dict.items())
-
-    # MLP
-    def key_mapping_mlp(key):
-        key = re.sub(
-            r"^transformer.layers.(\d+).mlp.dense_h_to_4h.(weight|bias)",
-            r"transformer.layers.\1.mlp.fc1.\2",
-            key,
-        )
-        key = re.sub(
-            r"^transformer.layers.(\d+).mlp.dense_4h_to_h.(weight|bias)",
-            r"transformer.layers.\1.mlp.fc2.\2",
-            key,
-        )
-        return key
-
-    state_dict = OrderedDict((key_mapping_mlp(k), v) for k, v in state_dict.items())
-
-    # Attention
-    def key_mapping_attn(key):
-        key = re.sub(
-            r"^transformer.layers.(\d+).self_attention.rotary_emb.inv_freq",
-            r"transformer.layers.\1.mixer.rotary_emb.inv_freq",
-            key,
-        )
-        key = re.sub(
-            r"^transformer.layers.(\d+).self_attention.query_key_value.(weight|bias)",
-            r"transformer.layers.\1.mixer.Wqkv.\2",
-            key,
-        )
-        key = re.sub(
-            r"^transformer.layers.(\d+).self_attention.dense.(weight|bias)",
-            r"transformer.layers.\1.mixer.out_proj.\2",
-            key,
-        )
-        return key
-
-    state_dict = OrderedDict((key_mapping_attn(k), v) for k, v in state_dict.items())
-    # Megatron stores Wqkv as ((nheads 3 headdim), hidden_dim)
-    # while we store Wqkv as ((3 nheads headdim), hidden_dim)
-    headdim = config.hidden_size // config.num_attention_heads
-    for d in range(config.num_hidden_layers):
-        Wqkv = state_dict.pop(f"transformer.layers.{d}.mixer.Wqkv.weight")
-        state_dict[f"transformer.layers.{d}.mixer.Wqkv.weight"] = rearrange(
-            Wqkv,
-            "(nheads three headdim) ... -> (three nheads headdim) ...",
-            three=3,
-            headdim=headdim,
-        )
-        bqkv = state_dict.pop(f"transformer.layers.{d}.mixer.Wqkv.bias")
-        state_dict[f"transformer.layers.{d}.mixer.Wqkv.bias"] = rearrange(
-            bqkv, "(nheads three headdim) -> (three nheads headdim)", three=3, headdim=headdim
-        )
-
-    return state_dict
\ No newline at end of file
diff --git a/based/models/mamba.py b/based/models/mamba.py
deleted file mode 100755
index 055fff4..0000000
--- a/based/models/mamba.py
+++ /dev/null
@@ -1,307 +0,0 @@
-# Copyright (c) 2023, Albert Gu, Tri Dao.
-# This class is used to construct the Mamba architecture.
-
-import math
-from functools import partial
-import json
-import os
-import re
-
-from collections import namedtuple
-
-import torch
-import torch.nn as nn
-
-from based.models.mixers.mamba.modules.mamba_simple import Mamba, Block
-from based.models.mixers.mamba.utils.generation import GenerationMixin
-from based.models.mixers.mamba.utils.hf import load_config_hf, load_state_dict_hf
-
-try:
-    from based.models.mixers.mamba.ops.triton.layernorm import RMSNorm, layer_norm_fn, rms_norm_fn
-except ImportError:
-    RMSNorm, layer_norm_fn, rms_norm_fn = None, None, None
-
-from dataclasses import dataclass, field, asdict
-
-
-@dataclass
-class MambaConfig:
-    d_model: int = 1024
-    n_layer: int = 48
-    vocab_size: int = 50277
-    ssm_cfg: dict = field(default_factory=dict)
-    rms_norm: bool = True
-    residual_in_fp32: bool = True
-    fused_add_norm: bool = True
-    pad_vocab_size_multiple: int = 8
-    reorder_and_upcast_attn: bool = False
-    scale_attn_by_inverse_layer_idx: bool = False
-    n_positions: int = 2048
-    n_embd: int = 1024
-    n_head: int = 16
-    use_flash_attn: bool = False
-    fused_dropout_add_ln: bool = False
-    fused_mlp: bool = False
-    fused_bias_fc: bool = False
-    use_fast_path: bool = True
-
-    def to_dict(self):
-        return asdict(self)
-
-
-def create_block(
-    d_model,
-    ssm_cfg=None,
-    norm_epsilon=1e-5,
-    rms_norm=False,
-    residual_in_fp32=False,
-    fused_add_norm=False,
-    layer_idx=None,
-    device=None,
-    dtype=None,
-    use_fast_path=True,
-):
-    if ssm_cfg is None:
-        ssm_cfg = {}
-    factory_kwargs = {"device": device, "dtype": dtype}
-    mixer_cls = partial(Mamba, layer_idx=layer_idx, **ssm_cfg, **factory_kwargs)
-    norm_cls = partial(
-        nn.LayerNorm if not rms_norm else RMSNorm, eps=norm_epsilon, **factory_kwargs
-    )
-    block = Block(
-        d_model,
-        mixer_cls,
-        norm_cls=norm_cls,
-        fused_add_norm=fused_add_norm,
-        residual_in_fp32=residual_in_fp32,
-        use_fast_path=use_fast_path,
-    )
-    block.layer_idx = layer_idx
-    return block
-
-
-# https://github.com/huggingface/transformers/blob/c28d04e9e252a1a099944e325685f14d242ecdcd/src/transformers/models/gpt2/modeling_gpt2.py#L454
-def _init_weights(
-    module,
-    n_layer,
-    initializer_range=0.02,  # Now only used for embedding layer.
-    rescale_prenorm_residual=True,
-    n_residuals_per_layer=1,  # Change to 2 if we have MLP
-):
-    if isinstance(module, nn.Linear):
-        if module.bias is not None:
-            if not getattr(module.bias, "_no_reinit", False):
-                nn.init.zeros_(module.bias)
-    elif isinstance(module, nn.Embedding):
-        nn.init.normal_(module.weight, std=initializer_range)
-
-    if rescale_prenorm_residual:
-        # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:
-        #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale
-        #   > the weights of residual layers at initialization by a factor of 1/N where N is the # of residual layers.
-        #   >   -- GPT-2 :: https://openai.com/blog/better-language-models/
-        #
-        # Reference (Megatron-LM): https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py
-        for name, p in module.named_parameters():
-            if name in ["out_proj.weight", "fc2.weight"]:
-                # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block
-                # Following Pytorch init, except scale by 1/sqrt(2 * n_layer)
-                # We need to reinit p since this code could be called multiple times
-                # Having just p *= scale would repeatedly scale it down
-                nn.init.kaiming_uniform_(p, a=math.sqrt(5))
-                with torch.no_grad():
-                    p /= math.sqrt(n_residuals_per_layer * n_layer)
-
-
-class MixerModel(nn.Module):
-    def __init__(
-        self,
-        d_model: int,
-        n_layer: int,
-        vocab_size: int,
-        ssm_cfg=None,
-        norm_epsilon: float = 1e-5,
-        rms_norm: bool = False,
-        initializer_cfg=None,
-        fused_add_norm=False,
-        residual_in_fp32=False,
-        use_fast_path=True,
-        device=None,
-        dtype=None,
-    ) -> None:
-        factory_kwargs = {"device": device, "dtype": dtype}
-        super().__init__()
-        self.residual_in_fp32 = residual_in_fp32
-
-        self.embedding = nn.Embedding(vocab_size, d_model, **factory_kwargs)
-
-        # We change the order of residual and layer norm:
-        # Instead of LN -> Attn / MLP -> Add, we do:
-        # Add -> LN -> Attn / MLP / Mixer, returning both the residual branch (output of Add) and
-        # the main branch (output of MLP / Mixer). The model definition is unchanged.
-        # This is for performance reason: we can fuse add + layer_norm.
-        self.fused_add_norm = fused_add_norm
-        if self.fused_add_norm:
-            if layer_norm_fn is None or rms_norm_fn is None:
-                raise ImportError("Failed to import Triton LayerNorm / RMSNorm kernels")
-
-        self.layers = nn.ModuleList(
-            [
-                create_block(
-                    d_model,
-                    ssm_cfg=ssm_cfg,
-                    norm_epsilon=norm_epsilon,
-                    rms_norm=rms_norm,
-                    residual_in_fp32=residual_in_fp32,
-                    fused_add_norm=fused_add_norm,
-                    layer_idx=i,
-                    use_fast_path=use_fast_path,
-                    **factory_kwargs,
-                )
-                for i in range(n_layer)
-            ]
-        )
-
-        self.norm_f = (nn.LayerNorm if not rms_norm else RMSNorm)(
-            d_model, eps=norm_epsilon, **factory_kwargs
-        )
-
-        self.apply(
-            partial(
-                _init_weights,
-                n_layer=n_layer,
-                **(initializer_cfg if initializer_cfg is not None else {}),
-            )
-        )
-
-    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):
-        return {
-            i: layer.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs)
-            for i, layer in enumerate(self.layers)
-        }
-
-    def forward(self, input_ids, inference_params=None):
-        if type(input_ids) == list:
-            input_ids = input_ids[0]
-        hidden_states = self.embedding(input_ids)
-        residual = None
-        for layer in self.layers:
-            hidden_states, residual = layer(
-                hidden_states, residual, inference_params=inference_params
-            )
-        if not self.fused_add_norm:
-            residual = (hidden_states + residual) if residual is not None else hidden_states
-            hidden_states = self.norm_f(residual.to(dtype=self.norm_f.weight.dtype))
-        else:
-            # Set prenorm=False here since we don't need the residual
-            fused_add_norm_fn = rms_norm_fn if isinstance(self.norm_f, RMSNorm) else layer_norm_fn
-            hidden_states = fused_add_norm_fn(
-                hidden_states,
-                self.norm_f.weight,
-                self.norm_f.bias,
-                eps=self.norm_f.eps,
-                residual=residual,
-                prenorm=False,
-                residual_in_fp32=self.residual_in_fp32,
-            )
-        return hidden_states
-
-
-class MambaLMHeadModel(nn.Module, GenerationMixin):
-
-    def __init__(
-        self,
-        config: MambaConfig,
-        initializer_cfg=None,
-        device=None,
-        dtype=None,
-    ) -> None:
-        self.config = config
-        d_model = config.d_model
-        n_layer = config.n_layer
-        vocab_size = config.vocab_size
-        ssm_cfg = config.ssm_cfg
-        rms_norm = config.rms_norm
-        residual_in_fp32 = config.residual_in_fp32
-        fused_add_norm = config.fused_add_norm
-        pad_vocab_size_multiple = config.pad_vocab_size_multiple
-        factory_kwargs = {"device": device, "dtype": dtype}
-
-        super().__init__()
-        if vocab_size % pad_vocab_size_multiple != 0:
-            vocab_size += pad_vocab_size_multiple - (vocab_size % pad_vocab_size_multiple)
-        self.backbone = MixerModel(
-            d_model=d_model,
-            n_layer=n_layer,
-            vocab_size=vocab_size,
-            ssm_cfg=ssm_cfg,
-            rms_norm=rms_norm,
-            initializer_cfg=initializer_cfg,
-            fused_add_norm=fused_add_norm,
-            residual_in_fp32=residual_in_fp32,
-            use_fast_path=config.use_fast_path,
-            **factory_kwargs,
-        )
-        self.lm_head = nn.Linear(d_model, vocab_size, bias=False, **factory_kwargs)
-
-        # Initialize weights and apply final processing
-        self.apply(
-            partial(
-                _init_weights,
-                n_layer=n_layer,
-                **(initializer_cfg if initializer_cfg is not None else {}),
-            )
-        )
-        self.tie_weights()
-
-    def tie_weights(self):
-        self.lm_head.weight = self.backbone.embedding.weight
-
-    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):
-        return self.backbone.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs)
-
-    def forward(self, input_ids, position_ids=None, inference_params=None, num_last_tokens=0):
-        """
-        "position_ids" is just to be compatible with Transformer generation. We don't use it.
-        num_last_tokens: if > 0, only return the logits for the last n tokens
-        """
-        hidden_states = self.backbone(input_ids, inference_params=inference_params)
-        if num_last_tokens > 0:
-            hidden_states = hidden_states[:, -num_last_tokens:]
-        lm_logits = self.lm_head(hidden_states)
-        CausalLMOutput = namedtuple("CausalLMOutput", ["logits"])
-        return CausalLMOutput(logits=lm_logits)
-
-    @classmethod
-    def from_pretrained_hf(cls, pretrained_model_name, device=None, dtype=None, **kwargs):
-        config_data = load_config_hf(pretrained_model_name)
-        config = MambaConfig(**config_data)
-        model = cls(config, device=device, dtype=dtype, **kwargs)
-        state_dict = load_state_dict_hf(pretrained_model_name, device=device, dtype=dtype)
-
-        # remove the 'model.' prefix from the keys
-        state_dict = {re.sub("^model\.", "", k): v for k, v in state_dict.items()}
-        # remove Unexpected key(s) in state_dict: "train_metrics.num-tokens.count", "val_metrics.num-tokens.count", "test_metrics.num-tokens.count". from the state_dict
-        state_dict = {k: v for k, v in state_dict.items() if "metrics" not in k}
-
-        model.load_state_dict(state_dict)
-        return model
-
-    def save_pretrained(self, save_directory):
-        """
-        Minimal implementation of save_pretrained for MambaLMHeadModel.
-        Save the model and its configuration file to a directory.
-        """
-        # Ensure save_directory exists
-        if not os.path.exists(save_directory):
-            os.makedirs(save_directory)
-
-        # Save the model's state_dict
-        model_path = os.path.join(save_directory, 'pytorch_model.bin')
-        torch.save(self.state_dict(), model_path)
-
-        # Save the configuration of the model
-        config_path = os.path.join(save_directory, 'config.json')
-        with open(config_path, 'w') as f:
-            json.dump(self.config.__dict__, f)
-
diff --git a/based/models/mixers/__init__.py b/based/models/mixers/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/based/models/mixers/convolution.py b/based/models/mixers/convolution.py
deleted file mode 100644
index 300f669..0000000
--- a/based/models/mixers/convolution.py
+++ /dev/null
@@ -1,217 +0,0 @@
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-from einops import rearrange
-from typing import List
-
-from flash_attn.utils.generation import InferenceParams
-try:
-    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update
-except:
-    causal_conv1d_fn = None
-    causal_conv1d_update = None
-
-
-class ShortConvolution(nn.Module):
-    """
-    Simple wrapper around nn.Conv1d that accepts dimension last. 
-    """
-
-    def __init__(
-        self, 
-        d_model: int,
-        kernel_size: int,
-        layer_idx: int=None,
-        use_cuda: bool=False,
-        conv_bias: bool=False,
-        **kwargs,
-    ): 
-        super().__init__()
-        self.d_model = d_model 
-        self.kernel_size = kernel_size
-        self.layer_idx = layer_idx
-        self.use_cuda = use_cuda and causal_conv1d_fn is not None
-        if self.use_cuda:
-            conv_bias = True
-        self.conv = nn.Conv1d(
-            in_channels=d_model,
-            out_channels=d_model,
-            kernel_size=kernel_size,
-            groups=d_model,
-            padding=kernel_size - 1,
-            bias=conv_bias
-        )
-    
-
-    def forward(
-        self, x: torch.Tensor, inference_params: InferenceParams=None, **kwargs
-    ):
-        """
-        Args:
-            x: (b, l, d) tensor
-        Returns: 
-            y: (b, l, d) tensor
-        """
-        b, l, d = x.shape
-        state = None
-        if inference_params is not None:
-            if inference_params.seqlen_offset > 0:
-                # check if we are after the first step of inference, step if so
-                state = self._get_state(inference_params)
-                return self.step(x, state)
-            else:
-                # otherwise, we are at the first step of inference, so we update the state
-                self._init_state(inference_params)  # create state if it doesn't exist, zero it out otherwise
-                state = self._get_state(inference_params)
-                k = min(self.kernel_size, x.shape[1])
-                state[..., -k: ] = x[:, -k:].transpose(1, 2)
-
-
-        if self.use_cuda:
-            if state is not None:
-                # If we just take x[:, :, -self.d_conv :], it will error if seqlen < self.d_conv
-                # Instead F.pad will pad with zeros if seqlen < self.d_conv, and truncate otherwise.
-                state.copy_(F.pad(x, (self.kernel_size - x.shape[-1], 0)))  # Update state (B D W)
-            y = causal_conv1d_fn(
-                x=x.transpose(1,2),
-                weight=rearrange(self.conv.weight, "d 1 w -> d w"),
-                bias=self.conv.bias,
-                activation="silu",
-            ).transpose(1, 2)
-        else:
-            y = self.conv(x.transpose(1, 2))[..., :l].transpose(1, 2)
-        return y 
-
-    def step(self, x: torch.Tensor, state: torch.Tensor):
-        if self.use_cuda:
-            # Conv step
-            if causal_conv1d_update is None:
-                state.copy_(torch.roll(state, shifts=-1, dims=-1))  # Update state (B D W)
-                state[:, :, -1] = x
-                x = torch.sum(state * rearrange(self.conv.weight, "d 1 w -> d w"), dim=-1)  # (B D)
-                if self.conv.bias is not None:
-                    x = x + self.conv1d.bias
-                x = self.act(x).to(dtype=x.dtype)
-            else:
-                x = causal_conv1d_update(
-                    x.squeeze(),
-                    state,
-                    rearrange(self.conv.weight, "d 1 w -> d w"),
-                    self.conv.bias,
-                    "silu",
-                ).unsqueeze(1)
-            return x
-        else:
-            state.copy_(torch.roll(state, shifts=-1, dims=-1))  # Update state (B D W)
-            state[:, :, -1] = x.squeeze(1)
-            x = torch.einsum("bdk,dgk->bd", state, self.conv.weight).to(x.dtype)
-            return x.unsqueeze(1)
-    
-    def allocate_inference_cache(self, batch_size: int, max_seqlen: int, dtype=None, **kwargs):
-        """Creates a state tensor of shape (b, d, k)"""
-        return torch.zeros(
-            batch_size, 
-            self.d_model, 
-            self.kernel_size, 
-            device=self.conv.weight.device, 
-            dtype=self.conv.weight.dtype if dtype is None else dtype
-        )
-    
-    def _init_state(
-            self, 
-            inference_params: InferenceParams, 
-            layer_idx: int=None,
-            key_value_memory_dict: dict = None
-        ):
-        """Create the state if it doesn't exist, zero it out otherwise.
-        Do this recursively if the layer_idx is a tuple.
-        """
-        if key_value_memory_dict is None:
-            key_value_memory_dict = inference_params.key_value_memory_dict
-        if layer_idx is None:
-            layer_idx = self.layer_idx
-
-        if isinstance(layer_idx, (int, str)):
-            empty_state = self.allocate_inference_cache(
-                batch_size=inference_params.max_batch_size,
-                max_seqlen=inference_params.max_seqlen,
-            )
-            if layer_idx not in key_value_memory_dict:
-                # SE (02/25): this is needed for when cache graph is false
-                key_value_memory_dict[layer_idx] = empty_state
-            else: 
-                # SE (02/25): this is needed for when cache graph is true
-                key_value_memory_dict[layer_idx].copy_(empty_state)
-        else:
-            if layer_idx[0] not in key_value_memory_dict:
-                key_value_memory_dict[layer_idx[0]] = {}
-            self._init_state(
-                inference_params, 
-                layer_idx[1],
-                key_value_memory_dict[layer_idx[0]]
-            )
-
-    def _get_state(self, inference_params: InferenceParams, layer_idx: int=None):
-        """Returns the state tensors for the given layer.
-        Adds support for nested states. 
-        """
-        if layer_idx is None:
-            layer_idx = self.layer_idx
-
-        if isinstance(layer_idx, (int, str)):
-            return inference_params.key_value_memory_dict[layer_idx]
-        else:
-            return self._get_state(inference_params, layer_idx[0])[layer_idx[1]]
-        
-
-class BaseConv(nn.Module):
-    def __init__(
-        self,
-        d_model: int,
-        l_max: int,
-        kernel_size: int=3,
-        layer_idx: int=None,
-        use_bias=True,
-        expand_proj: int=2,
-        use_cuda: bool=False,
-        **kwargs
-    ):
-        super().__init__()
-        
-        self.d_model = d_model
-        self.l_max = l_max
-        self.layer_idx=layer_idx
-
-        self.d_inner = expand_proj*self.d_model // 2
-        self.in_proj = nn.Linear(self.d_model,  expand_proj*self.d_model, bias=use_bias)
-        self.out_proj = nn.Linear(self.d_inner,  self.d_model, bias=use_bias)
-
-        self.use_cuda = use_cuda and causal_conv1d_fn is not None
-
-        # prepare convolution
-        self.conv = ShortConvolution(self.d_inner, kernel_size=kernel_size, use_cuda=self.use_cuda, layer_idx=(layer_idx, "conv"))
-
-    def forward(self, u, position_ids=None, inference_params: InferenceParams=None, *args, **kwargs):
-        """
-        Args:
-            u: (b, l, d) tensor
-        Returns:
-            y: (b, l, d) tensor
-        """
-        u = self.in_proj(u)
-        u1, u2 = torch.split(u, self.d_inner, dim=-1)
-        u_conv = self.conv(u1, inference_params=inference_params)
-        if not self.use_cuda:
-            # SA: the silu is fused in the cuda version.
-            u_conv = nn.functional.silu(u_conv)
-        v = u_conv * u2
-        y = self.out_proj(v)
-        return y 
-
-        
-    def allocate_inference_cache(self, batch_size: int, max_seqlen: int, dtype=None, **kwargs):
-        """Creates a state tensor of shape (b, d, k)"""
-        return {
-            "conv": self.conv.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs),
-        }
-    
diff --git a/based/models/mixers/linear_attention.py b/based/models/mixers/linear_attention.py
deleted file mode 100644
index 5bbad15..0000000
--- a/based/models/mixers/linear_attention.py
+++ /dev/null
@@ -1,235 +0,0 @@
-"""
-Linear attention in Based. 
-"""
-import math
-
-import torch
-import torch.nn as nn
-from einops import rearrange
-
-from based.generation import InferenceParams
-
-import sys
-sys.path.append("../../../")
-
-try:
-    from train.csrc.causal_dot_prod import causal_dot_product  # linear attention cuda kernel
-    print(f"Successfully imported the causal dot product kernel! ")
-except:
-    print(f"Could not import the causal dot product kernel... ")
-    causal_dot_product = None
-
-try:
-    from fla.ops.based import fused_chunk_based, parallel_based
-    from fla.ops.based.naive import naive_parallel_based
-    print(f"Successfully imported the FLA triton kernels! ")
-except:
-    print(f"Could not import the FLA triton kernels... ")
-
-        
-class FeatureMap(nn.Module):
-    """
-    Parent feature map; default is identity function
-    """
-    def __init__(self, input_dim: int, **kwargs):
-        super().__init__()
-        self.input_dim = input_dim
-        
-    def forward(self, x: torch.Tensor):
-        """
-        Assume x.shape is (batch_size, n_heads, seq_len, head_dim)
-        """
-        return x
-
-class TaylorExp(FeatureMap):
-    """
-    Feature map to compute 2nd-order Taylor approx. of exp(q^T k / sqrt(d))
-    """
-    def __init__(
-            self, 
-            input_dim: int, 
-            **kwargs: any
-        ):
-        super().__init__(input_dim, **kwargs)
-        self.r2  = math.sqrt(2)
-        self.rd  = math.sqrt(input_dim)
-        self.rrd = math.sqrt(self.rd)
-        self.tril_indices = torch.tril_indices(self.input_dim, self.input_dim, -1)
-        
-    def forward(self, x: torch.Tensor):
-        # Get 2nd-order terms (rearrange(x * x), '... m n -> ... (m n)')
-        x2 = (x.unsqueeze(-1) * x.unsqueeze(-2)).flatten(start_dim=-2) / self.r2
-        # SE: raising to power 0 is a hacky way to get ones without calling torch.ones
-        # which is incompatible with cuda graph caching 
-        return torch.cat(
-            [x[..., :1] ** 0, x / self.rrd, x2 / self.rd], 
-            dim=-1
-        )
-
-class LinearAttention(nn.Module):
-    def __init__(
-        self,
-        d_model: int,
-        feature_map: FeatureMap, 
-        l_max: int = 2048,
-        feature_dim: int = 16,
-        head_dim: int = None,
-        num_heads: int = 16,
-        eps: float = 1e-12,
-        layer_idx: int = None,
-        parallel_implementation: str="quadratic",
-        **kwargs
-    ):
-        super().__init__()
-
-        self.layer_idx = layer_idx
-        self.d_model = d_model
-        self.l_max = l_max
-        self.eps = eps
-        self.parallel_implementation = parallel_implementation
-
-        # set dimension 
-        self.num_heads = num_heads
-        self.head_dim = self.d_model // self.num_heads if head_dim is None else head_dim      
-        self.feature_dim = feature_dim
-
-        # initialize projections and feature map
-        self.feature_map = feature_map
-        self.proj_q = nn.Linear(self.d_model, self.feature_dim * self.num_heads, bias=False)
-        self.proj_k = nn.Linear(self.d_model, self.feature_dim * self.num_heads, bias=False)
-        self.proj_v = nn.Linear(self.d_model, self.num_heads * self.head_dim, bias=False)
-        self.out_proj = nn.Linear(self.num_heads * self.head_dim, self.d_model, bias=False)
-
-        
-    def forward(self, 
-        hidden_states: torch.Tensor,
-        inference_params: InferenceParams = None,
-        *args: any, 
-        **kwargs: any
-    ):
-        """
-        x (torch.Tensor): tensor of shape (b, d, l)
-        y (torch.Tensor): tensor of shape (b, d, l)
-        """
-        b, l, _ = hidden_states.size()
-        q, k, v = self.proj_q(hidden_states), self.proj_k(hidden_states), self.proj_v(hidden_states)
-        q = q.view(b, l, self.num_heads, self.feature_dim).transpose(1, 2)
-        k = k.view(b, l, self.num_heads, self.feature_dim).transpose(1, 2)
-        v = v.view(b, l, self.num_heads, self.head_dim).transpose(1, 2)
-
-        if inference_params is None:
-            return self.parallel_forward(hidden_states, q, k, v)
-        else:
-            # check if we are doing prefill or generation
-            if inference_params.seqlen_offset > 0: # recurrent
-                kv_state, k_state = self._get_inference_cache(inference_params)
-                q, k = self.feature_map(q), self.feature_map(k)
-                return self.recurrent_forward(hidden_states, kv_state, k_state, q, k, v)
-            else:  # prefill
-                y = self.parallel_forward(hidden_states, q, k, v)
-                q, k = self.feature_map(q), self.feature_map(k)
-                kv_state = torch.einsum("bhnd,bhnf->bhfd", k, v)[:, :, None]
-                k_state = k.sum(dim=2)[:, :, None, None]
-                if self.layer_idx in inference_params.key_value_memory_dict:
-                    # # update the state in-place when graph caching is enabled
-                    inference_params.key_value_memory_dict[self.layer_idx][0].copy_(kv_state)
-                    inference_params.key_value_memory_dict[self.layer_idx][1].copy_(k_state)
-                else: 
-                    inference_params.key_value_memory_dict[self.layer_idx] = (kv_state, k_state)
-                return y
-
-    def parallel_forward(self, x: torch.Tensor, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor):
-        if self.parallel_implementation == "quadratic":
-            q, k = self.feature_map(q), self.feature_map(k)
-            A_qk = torch.einsum("bhnd,bhmd->bhnm", q, k) 
-            try:
-                A_qk = torch.tril(A_qk)       
-            except:
-                # tril is incompatible with certain data types
-                b, h, l, l = A_qk.shape
-                cumsum_matrix = torch.tril(torch.ones((l, l))).to(q.device, q.dtype)
-                A_qk = A_qk * cumsum_matrix
-            y = torch.einsum("bhnm,bhme->bhne", A_qk.to(x.dtype), v.to(x.dtype))
-            z = 1 / (torch.einsum("bhld,bhld->bhl", q, k.cumsum(2)) + self.eps)
-            y = y * z[..., None]
-            y = rearrange(y, 'b h l d -> b l (h d)')
-
-        elif self.parallel_implementation == "linear": 
-            q, k = self.feature_map(q), self.feature_map(k)
-            v = causal_dot_product(q.contiguous().to(dtype=torch.float32), k.contiguous().to(dtype=torch.float32),v.contiguous().to(dtype=torch.float32),)
-            z = 1 / (
-                torch.einsum(
-                    "bhld,bhld->bhl", 
-                    q.to(dtype=torch.float32), 
-                    k.to(dtype=torch.float32).cumsum(2)
-                ) + self.eps
-            )
-            y = v * z[..., None]
-            y = rearrange(y, 'b h l d -> b l (h d)')
-
-        elif self.parallel_implementation == "fla_parallel":
-            """ 
-            Computes both the feature map and causal dot products.
-            Booleans are for the denominator and the normalization 
-            """
-            y = parallel_based(q, k, v, True, True)
-            y = rearrange(y, 'b h l d -> b l (h d)')
-
-        elif self.parallel_implementation == "fla_chunk":
-            """ 
-            Computes both the feature map and causal dot products.
-            Booleans are for the denominator and the normalization 
-            """
-            y = fused_chunk_based(q, k, v, True, True)
-            y = rearrange(y, 'b h l d -> b l (h d)')
-
-        else: 
-            raise ValueError(f"Parallel implementation {self.parallel_implementation} not supported")
-
-        return self.out_proj(y.to(x.dtype))
-
-    
-    def recurrent_forward(self, hidden_states: torch.Tensor, kv_state: torch.Tensor, k_state: torch.Tensor, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, decay: torch.Tensor=None):
-        """
-        Compute linear attention with recurrent view
-        -> Assume q.shape is (b, h, 1, d); k and v.shape are (b, h, l, d)
-        """
-        b, h, l, d = q.shape
-        assert l == 1, f'q.shape is {q.shape} but should be ({b}, {h}, 1, {d})'
-        # Expand dims for broadcasting to compute linear attention
-        q, k, v = q.unsqueeze(-2), k.unsqueeze(-2), v.unsqueeze(-1)
-
-        kv_state += k[:, :, -1:] * v[:, :, -1:]
-        k_state  += k[:, :, -1:]
-
-        # Compute linear attention
-        num = (q * kv_state).sum(dim=-1)
-        if 'fla' in self.parallel_implementation: 
-            eps = 1e-6 # this code uses an alternate eps
-        else: 
-            eps = 1e-12
-        y = num / ((q * k_state).sum(dim=-1) + eps)
-
-        y = rearrange(y, 'b h l d -> b l (h d)').to(q.dtype)
-        return self.out_proj(y)
- 
-    
-    def expanded_size(self):
-        return self.feature_dim ** 2 + self.feature_dim + 1
-    
-    def allocate_inference_cache(self, batch_size: int, max_seqlen: int, dtype=None, **kwargs):
-        """Creates a state tensor of shape ..."""
-
-        kv_shape = (
-            batch_size, self.num_heads, 1, self.head_dim, self.expanded_size()
-        )
-        k_shape = (
-            batch_size, self.num_heads, 1, 1, self.expanded_size()
-        )
-        kv_state = torch.zeros(*kv_shape, dtype=dtype, device=self.out_proj.weight.device)
-        k_state = torch.zeros(*k_shape, dtype=dtype, device=self.out_proj.weight.device)
-        return (kv_state, k_state)
-     
-    def _get_inference_cache(self, inference_params: InferenceParams):
-        return inference_params.key_value_memory_dict[self.layer_idx]
-
diff --git a/based/models/mixers/mamba/__init__.py b/based/models/mixers/mamba/__init__.py
deleted file mode 100755
index f536fc9..0000000
--- a/based/models/mixers/mamba/__init__.py
+++ /dev/null
@@ -1,5 +0,0 @@
-__version__ = "1.1.1"
-
-from mamba_ssm.ops.selective_scan_interface import selective_scan_fn, mamba_inner_fn
-from mamba_ssm.modules.mamba_simple import Mamba
-from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel
diff --git a/based/models/mixers/mamba/modules/__init__.py b/based/models/mixers/mamba/modules/__init__.py
deleted file mode 100755
index e69de29..0000000
diff --git a/based/models/mixers/mamba/modules/mamba_simple.py b/based/models/mixers/mamba/modules/mamba_simple.py
deleted file mode 100755
index 9f68c16..0000000
--- a/based/models/mixers/mamba/modules/mamba_simple.py
+++ /dev/null
@@ -1,350 +0,0 @@
-# Copyright (c) 2023, Tri Dao, Albert Gu.
-
-import math
-from typing import Optional
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-from torch import Tensor
-from einops import rearrange, repeat
-
-from based.models.mixers.mamba.ops.selective_scan_interface import selective_scan_fn, mamba_inner_fn, selective_scan_ref
-
-try:
-    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update
-except ImportError:
-    causal_conv1d_fn, causal_conv1d_update = None
-
-try:
-    from based.models.mixers.mamba.ops.triton.selective_state_update import selective_state_update
-except ImportError:
-    selective_state_update = None
-
-try:
-    from based.models.mixers.mamba.ops.triton.layernorm import RMSNorm, layer_norm_fn, rms_norm_fn
-except ImportError:
-    RMSNorm, layer_norm_fn, rms_norm_fn = None, None, None
-
-
-class Mamba(nn.Module):
-    def __init__(
-        self,
-        d_model,
-        d_state=16,
-        d_conv=4,
-        expand=2,
-        dt_rank="auto",
-        dt_min=0.001,
-        dt_max=0.1,
-        dt_init="random",
-        dt_scale=1.0,
-        dt_init_floor=1e-4,
-        conv_bias=True,
-        bias=False,
-        use_fast_path=True,  # Fused kernel options
-        layer_idx=None,
-        device=None,
-        dtype=None,
-    ):
-    
-        factory_kwargs = {"device": device, "dtype": dtype}
-        super().__init__()
-        self.d_model = d_model
-        self.d_state = d_state
-        self.d_conv = d_conv
-        self.expand = expand
-        self.d_inner = int(self.expand * self.d_model)
-        self.dt_rank = math.ceil(self.d_model / 16) if dt_rank == "auto" else dt_rank
-        self.use_fast_path = use_fast_path
-        self.layer_idx = layer_idx
-
-        self.in_proj = nn.Linear(self.d_model, self.d_inner * 2, bias=bias, **factory_kwargs)
-
-        self.conv1d = nn.Conv1d(
-            in_channels=self.d_inner,
-            out_channels=self.d_inner,
-            bias=conv_bias,
-            kernel_size=d_conv,
-            groups=self.d_inner,
-            padding=d_conv - 1,
-            **factory_kwargs,
-        )
-
-        self.activation = "silu"
-        self.act = nn.SiLU()
-
-        self.x_proj = nn.Linear(
-            self.d_inner, self.dt_rank + self.d_state * 2, bias=False, **factory_kwargs
-        )
-        self.dt_proj = nn.Linear(self.dt_rank, self.d_inner, bias=True, **factory_kwargs)
-
-        # Initialize special dt projection to preserve variance at initialization
-        dt_init_std = self.dt_rank**-0.5 * dt_scale
-        if dt_init == "constant":
-            nn.init.constant_(self.dt_proj.weight, dt_init_std)
-        elif dt_init == "random":
-            nn.init.uniform_(self.dt_proj.weight, -dt_init_std, dt_init_std)
-        else:
-            raise NotImplementedError
-
-        # Initialize dt bias so that F.softplus(dt_bias) is between dt_min and dt_max
-        dt = torch.exp(
-            torch.rand(self.d_inner, **factory_kwargs) * (math.log(dt_max) - math.log(dt_min))
-            + math.log(dt_min)
-        ).clamp(min=dt_init_floor)
-        # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759
-        inv_dt = dt + torch.log(-torch.expm1(-dt))
-        with torch.no_grad():
-            self.dt_proj.bias.copy_(inv_dt)
-        # Our initialization would set all Linear.bias to zero, need to mark this one as _no_reinit
-        self.dt_proj.bias._no_reinit = True
-
-        # S4D real initialization
-        A = repeat(
-            torch.arange(1, self.d_state + 1, dtype=torch.float32, device=device),
-            "n -> d n",
-            d=self.d_inner,
-        ).contiguous()
-        A_log = torch.log(A)  # Keep A_log in fp32
-        self.A_log = nn.Parameter(A_log)
-        self.A_log._no_weight_decay = True
-
-        # D "skip" parameter
-        self.D = nn.Parameter(torch.ones(self.d_inner, device=device))  # Keep in fp32
-        self.D._no_weight_decay = True
-
-        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=bias, **factory_kwargs)
-
-    def forward(self, hidden_states, inference_params=None):
-        """
-        hidden_states: (B, L, D)
-        Returns: same shape as hidden_states
-        """
-        batch, seqlen, dim = hidden_states.shape
-        # print("batch, seqlen, dim", batch, seqlen, dim)
-        conv_state, ssm_state = None, None
-        if inference_params is not None:
-            conv_state, ssm_state = self._get_states_from_cache(inference_params, batch)
-            if inference_params.seqlen_offset > 0:
-                # The states are updated inplace
-                out, _, _ = self.step(hidden_states, conv_state, ssm_state)
-                return out
-
-        # We do matmul and transpose BLH -> HBL at the same time
-        xz = rearrange(
-            self.in_proj.weight @ rearrange(hidden_states, "b l d -> d (b l)"),
-            "d (b l) -> b d l",
-            l=seqlen,
-        )
-        if self.in_proj.bias is not None:
-            xz = xz + rearrange(self.in_proj.bias.to(dtype=xz.dtype), "d -> d 1")
-
-        A = -torch.exp(self.A_log.float())  # (d_inner, d_state)
-        # In the backward pass we write dx and dz next to each other to avoid torch.cat
-        if self.use_fast_path and inference_params is None:  # Doesn't support outputting the states
-            out = mamba_inner_fn(
-                xz,
-                self.conv1d.weight,
-                self.conv1d.bias,
-                self.x_proj.weight,
-                self.dt_proj.weight,
-                self.out_proj.weight,
-                self.out_proj.bias,
-                A,
-                None,  # input-dependent B
-                None,  # input-dependent C
-                self.D.float(),
-                delta_bias=self.dt_proj.bias.float(),
-                delta_softplus=True,
-            )
-        else:
-            x, z = xz.chunk(2, dim=1)
-            # Compute short convolution
-            if conv_state is not None:
-                # If we just take x[:, :, -self.d_conv :], it will error if seqlen < self.d_conv
-                # Instead F.pad will pad with zeros if seqlen < self.d_conv, and truncate otherwise.
-                conv_state.copy_(F.pad(x, (self.d_conv - x.shape[-1], 0)))  # Update state (B D W)
-            if causal_conv1d_fn is None:
-                x = self.act(self.conv1d(x)[..., :seqlen])
-            else:
-                assert self.activation in ["silu", "swish"]
-                x = causal_conv1d_fn(
-                    x=x,
-                    weight=rearrange(self.conv1d.weight, "d 1 w -> d w"),
-                    bias=self.conv1d.bias,
-                    activation=self.activation,
-                )
-
-            # We're careful here about the layout, to avoid extra transposes.
-            # We want dt to have d as the slowest moving dimension
-            # and L as the fastest moving dimension, since those are what the ssm_scan kernel expects.
-            x_dbl = self.x_proj(rearrange(x, "b d l -> (b l) d"))  # (bl d)
-            dt, B, C = torch.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=-1)
-            dt = self.dt_proj.weight @ dt.t()   # [B 2*D * L]
-            dt = rearrange(dt, "d (b l) -> b d l", l=seqlen)
-            B = rearrange(B, "(b l) dstate -> b dstate l", l=seqlen).contiguous()
-            C = rearrange(C, "(b l) dstate -> b dstate l", l=seqlen).contiguous()
-            assert self.activation in ["silu", "swish"]
-            y = selective_scan_fn(
-                x,
-                dt,
-                A,
-                B,
-                C,
-                self.D.float(),
-                z=z,
-                delta_bias=self.dt_proj.bias.float(),
-                delta_softplus=True,
-                return_last_state=ssm_state is not None,
-            )
-            if ssm_state is not None:
-                y, last_state = y
-                ssm_state.copy_(last_state)
-            y = rearrange(y, "b d l -> b l d")
-            out = self.out_proj(y)
-        return out
-
-    def step(self, hidden_states, conv_state, ssm_state):
-        dtype = hidden_states.dtype
-        assert hidden_states.shape[1] == 1, "Only support decoding with 1 token at a time for now"
-        xz = self.in_proj(hidden_states.squeeze(1))  # (B 2D)
-        x, z = xz.chunk(2, dim=-1)  # (B D)
-
-        # Conv step
-        if causal_conv1d_update is None:
-            conv_state.copy_(torch.roll(conv_state, shifts=-1, dims=-1))  # Update state (B D W)
-            conv_state[:, :, -1] = x
-            x = torch.sum(conv_state * rearrange(self.conv1d.weight, "d 1 w -> d w"), dim=-1)  # (B D)
-            if self.conv1d.bias is not None:
-                x = x + self.conv1d.bias
-            x = self.act(x).to(dtype=dtype)
-        else:
-            x = causal_conv1d_update(
-                x,
-                conv_state,
-                rearrange(self.conv1d.weight, "d 1 w -> d w"),
-                self.conv1d.bias,
-                self.activation,
-            )
-
-        x_db = self.x_proj(x)  # (B dt_rank+2*d_state)
-        dt, B, C = torch.split(x_db, [self.dt_rank, self.d_state, self.d_state], dim=-1)
-        # Don't add dt_bias here
-        dt = F.linear(dt, self.dt_proj.weight)  # (B d_inner)
-        A = -torch.exp(self.A_log.float())  # (d_inner, d_state)
-
-        # SSM step
-        if selective_state_update is None:
-            # Discretize A and B
-            dt = F.softplus(dt + self.dt_proj.bias.to(dtype=dt.dtype))
-            dA = torch.exp(torch.einsum("bd,dn->bdn", dt, A))
-            dB = torch.einsum("bd,bn->bdn", dt, B)
-            ssm_state.copy_(ssm_state * dA + rearrange(x, "b d -> b d 1") * dB)
-            y = torch.einsum("bdn,bn->bd", ssm_state.to(dtype), C)
-            y = y + self.D.to(dtype) * x
-            y = y * self.act(z)  # (B D)
-        else:
-            y = selective_state_update(
-                ssm_state, x, dt, A, B, C, self.D, z=z, dt_bias=self.dt_proj.bias, dt_softplus=True
-            )
-
-        out = self.out_proj(y)
-        return out.unsqueeze(1), conv_state, ssm_state
-
-    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):
-        device = self.out_proj.weight.device
-        conv_dtype = self.conv1d.weight.dtype if dtype is None else dtype
-        conv_state = torch.zeros(
-            batch_size, self.d_model * self.expand, self.d_conv, device=device, dtype=conv_dtype
-        )
-        ssm_dtype = self.dt_proj.weight.dtype if dtype is None else dtype
-        ssm_state = torch.zeros(
-            batch_size, self.d_model * self.expand, self.d_state, device=device, dtype=ssm_dtype
-        )
-        return conv_state, ssm_state
-
-    def _get_states_from_cache(self, inference_params, batch_size, initialize_states=False):
-        assert self.layer_idx is not None
-        if self.layer_idx not in inference_params.key_value_memory_dict:
-            batch_shape = (batch_size,)
-            conv_state = torch.zeros(
-                batch_size,
-                self.d_model * self.expand,
-                self.d_conv,
-                device=self.conv1d.weight.device,
-                dtype=self.conv1d.weight.dtype,
-            )
-            ssm_state = torch.zeros(
-                batch_size,
-                self.d_model * self.expand,
-                self.d_state,
-                device=self.dt_proj.weight.device,
-                dtype=self.dt_proj.weight.dtype,
-            )
-            inference_params.key_value_memory_dict[self.layer_idx] = (conv_state, ssm_state)
-        else:
-            conv_state, ssm_state = inference_params.key_value_memory_dict[self.layer_idx]
-            # TODO: What if batch size changes between generation, and we reuse the same states?
-            if initialize_states:
-                conv_state.zero_()
-                ssm_state.zero_()
-        return conv_state, ssm_state
-
-
-class Block(nn.Module):
-    def __init__(
-        self, dim, mixer_cls, norm_cls=nn.LayerNorm, fused_add_norm=False, residual_in_fp32=False,  use_fast_path=True
-    ):
-        """
-        Simple block wrapping a mixer class with LayerNorm/RMSNorm and residual connection"
-
-        This Block has a slightly different structure compared to a regular
-        prenorm Transformer block.
-        The standard block is: LN -> MHA/MLP -> Add.
-        [Ref: https://arxiv.org/abs/2002.04745]
-        Here we have: Add -> LN -> Mixer, returning both
-        the hidden_states (output of the mixer) and the residual.
-        This is purely for performance reasons, as we can fuse add and LayerNorm.
-        The residual needs to be provided (except for the very first block).
-        """
-        super().__init__()
-        self.residual_in_fp32 = residual_in_fp32
-        self.fused_add_norm = fused_add_norm
-        self.mixer = mixer_cls(dim, use_fast_path=use_fast_path)
-        self.norm = norm_cls(dim)
-        if self.fused_add_norm:
-            assert RMSNorm is not None, "RMSNorm import fails"
-            assert isinstance(
-                self.norm, (nn.LayerNorm, RMSNorm)
-            ), "Only LayerNorm and RMSNorm are supported for fused_add_norm"
-
-    def forward(
-        self, hidden_states: Tensor, residual: Optional[Tensor] = None, inference_params=None
-    ):
-        r"""Pass the input through the encoder layer.
-
-        Args:
-            hidden_states: the sequence to the encoder layer (required).
-            residual: hidden_states = Mixer(LN(residual))
-        """
-        if not self.fused_add_norm:
-            residual = (hidden_states + residual) if residual is not None else hidden_states
-            hidden_states = self.norm(residual.to(dtype=self.norm.weight.dtype))
-            if self.residual_in_fp32:
-                residual = residual.to(torch.float32)
-        else:
-            fused_add_norm_fn = rms_norm_fn if isinstance(self.norm, RMSNorm) else layer_norm_fn
-            hidden_states, residual = fused_add_norm_fn(
-                hidden_states,
-                self.norm.weight,
-                self.norm.bias,
-                residual=residual,
-                prenorm=True,
-                residual_in_fp32=self.residual_in_fp32,
-                eps=self.norm.eps,
-            )
-        hidden_states = self.mixer(hidden_states, inference_params=inference_params)
-        return hidden_states, residual
-
-    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):
-        return self.mixer.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs)
diff --git a/based/models/mixers/mamba/modules/models/__init__.py b/based/models/mixers/mamba/modules/models/__init__.py
deleted file mode 100755
index e69de29..0000000
diff --git a/based/models/mixers/mamba/modules/models/config_mamba.py b/based/models/mixers/mamba/modules/models/config_mamba.py
deleted file mode 100755
index ffd31ab..0000000
--- a/based/models/mixers/mamba/modules/models/config_mamba.py
+++ /dev/null
@@ -1,14 +0,0 @@
-from dataclasses import dataclass, field
-
-
-@dataclass
-class MambaConfig:
-
-    d_model: int = 2560
-    n_layer: int = 64
-    vocab_size: int = 50277
-    ssm_cfg: dict = field(default_factory=dict)
-    rms_norm: bool = True
-    residual_in_fp32: bool = True
-    fused_add_norm: bool = True
-    pad_vocab_size_multiple: int = 8
diff --git a/based/models/mixers/mamba/modules/models/mixer_seq_simple.py b/based/models/mixers/mamba/modules/models/mixer_seq_simple.py
deleted file mode 100755
index 25cea49..0000000
--- a/based/models/mixers/mamba/modules/models/mixer_seq_simple.py
+++ /dev/null
@@ -1,264 +0,0 @@
-# Copyright (c) 2023, Albert Gu, Tri Dao.
-
-import math
-from functools import partial
-import json
-import os
-
-from collections import namedtuple
-
-import torch
-import torch.nn as nn
-
-from src.models.mixers.mamba.models.config_mamba import MambaConfig
-from src.models.mixers.mamba.modules.mamba_simple import Mamba, Block
-from src.models.mixers.mamba.utils.generation import GenerationMixin
-from src.models.mixers.mamba.utils.hf import load_config_hf, load_state_dict_hf
-
-try:
-    from src.models.mixers.mamba.ops.triton.layernorm import RMSNorm, layer_norm_fn, rms_norm_fn
-except ImportError:
-    RMSNorm, layer_norm_fn, rms_norm_fn = None, None, None
-
-
-def create_block(
-    d_model,
-    ssm_cfg=None,
-    norm_epsilon=1e-5,
-    rms_norm=False,
-    residual_in_fp32=False,
-    fused_add_norm=False,
-    layer_idx=None,
-    device=None,
-    dtype=None,
-):
-    if ssm_cfg is None:
-        ssm_cfg = {}
-    factory_kwargs = {"device": device, "dtype": dtype}
-    mixer_cls = partial(Mamba, layer_idx=layer_idx, **ssm_cfg, **factory_kwargs)
-    norm_cls = partial(
-        nn.LayerNorm if not rms_norm else RMSNorm, eps=norm_epsilon, **factory_kwargs
-    )
-    block = Block(
-        d_model,
-        mixer_cls,
-        norm_cls=norm_cls,
-        fused_add_norm=fused_add_norm,
-        residual_in_fp32=residual_in_fp32,
-    )
-    block.layer_idx = layer_idx
-    return block
-
-
-# https://github.com/huggingface/transformers/blob/c28d04e9e252a1a099944e325685f14d242ecdcd/src/transformers/models/gpt2/modeling_gpt2.py#L454
-def _init_weights(
-    module,
-    n_layer,
-    initializer_range=0.02,  # Now only used for embedding layer.
-    rescale_prenorm_residual=True,
-    n_residuals_per_layer=1,  # Change to 2 if we have MLP
-):
-    if isinstance(module, nn.Linear):
-        if module.bias is not None:
-            if not getattr(module.bias, "_no_reinit", False):
-                nn.init.zeros_(module.bias)
-    elif isinstance(module, nn.Embedding):
-        nn.init.normal_(module.weight, std=initializer_range)
-
-    if rescale_prenorm_residual:
-        # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:
-        #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale
-        #   > the weights of residual layers at initialization by a factor of 1/N where N is the # of residual layers.
-        #   >   -- GPT-2 :: https://openai.com/blog/better-language-models/
-        #
-        # Reference (Megatron-LM): https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py
-        for name, p in module.named_parameters():
-            if name in ["out_proj.weight", "fc2.weight"]:
-                # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block
-                # Following Pytorch init, except scale by 1/sqrt(2 * n_layer)
-                # We need to reinit p since this code could be called multiple times
-                # Having just p *= scale would repeatedly scale it down
-                nn.init.kaiming_uniform_(p, a=math.sqrt(5))
-                with torch.no_grad():
-                    p /= math.sqrt(n_residuals_per_layer * n_layer)
-
-
-class MixerModel(nn.Module):
-    def __init__(
-        self,
-        d_model: int,
-        n_layer: int,
-        vocab_size: int,
-        ssm_cfg=None,
-        norm_epsilon: float = 1e-5,
-        rms_norm: bool = False,
-        initializer_cfg=None,
-        fused_add_norm=False,
-        residual_in_fp32=False,
-        device=None,
-        dtype=None,
-    ) -> None:
-        factory_kwargs = {"device": device, "dtype": dtype}
-        super().__init__()
-        self.residual_in_fp32 = residual_in_fp32
-
-        self.embedding = nn.Embedding(vocab_size, d_model, **factory_kwargs)
-
-        # We change the order of residual and layer norm:
-        # Instead of LN -> Attn / MLP -> Add, we do:
-        # Add -> LN -> Attn / MLP / Mixer, returning both the residual branch (output of Add) and
-        # the main branch (output of MLP / Mixer). The model definition is unchanged.
-        # This is for performance reason: we can fuse add + layer_norm.
-        self.fused_add_norm = fused_add_norm
-        if self.fused_add_norm:
-            if layer_norm_fn is None or rms_norm_fn is None:
-                raise ImportError("Failed to import Triton LayerNorm / RMSNorm kernels")
-
-        self.layers = nn.ModuleList(
-            [
-                create_block(
-                    d_model,
-                    ssm_cfg=ssm_cfg,
-                    norm_epsilon=norm_epsilon,
-                    rms_norm=rms_norm,
-                    residual_in_fp32=residual_in_fp32,
-                    fused_add_norm=fused_add_norm,
-                    layer_idx=i,
-                    **factory_kwargs,
-                )
-                for i in range(n_layer)
-            ]
-        )
-
-        self.norm_f = (nn.LayerNorm if not rms_norm else RMSNorm)(
-            d_model, eps=norm_epsilon, **factory_kwargs
-        )
-
-        self.apply(
-            partial(
-                _init_weights,
-                n_layer=n_layer,
-                **(initializer_cfg if initializer_cfg is not None else {}),
-            )
-        )
-
-    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):
-        return {
-            i: layer.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs)
-            for i, layer in enumerate(self.layers)
-        }
-
-    def forward(self, input_ids, inference_params=None):
-        hidden_states = self.embedding(input_ids)
-        residual = None
-        for layer in self.layers:
-            hidden_states, residual = layer(
-                hidden_states, residual, inference_params=inference_params
-            )
-        if not self.fused_add_norm:
-            residual = (hidden_states + residual) if residual is not None else hidden_states
-            hidden_states = self.norm_f(residual.to(dtype=self.norm_f.weight.dtype))
-        else:
-            # Set prenorm=False here since we don't need the residual
-            fused_add_norm_fn = rms_norm_fn if isinstance(self.norm_f, RMSNorm) else layer_norm_fn
-            hidden_states = fused_add_norm_fn(
-                hidden_states,
-                self.norm_f.weight,
-                self.norm_f.bias,
-                eps=self.norm_f.eps,
-                residual=residual,
-                prenorm=False,
-                residual_in_fp32=self.residual_in_fp32,
-            )
-        return hidden_states
-
-
-class MambaLMHeadModel(nn.Module, GenerationMixin):
-
-    def __init__(
-        self,
-        config: MambaConfig,
-        initializer_cfg=None,
-        device=None,
-        dtype=None,
-    ) -> None:
-        self.config = config
-        d_model = config.d_model
-        n_layer = config.n_layer
-        vocab_size = config.vocab_size
-        ssm_cfg = config.ssm_cfg
-        rms_norm = config.rms_norm
-        residual_in_fp32 = config.residual_in_fp32
-        fused_add_norm = config.fused_add_norm
-        pad_vocab_size_multiple = config.pad_vocab_size_multiple
-        factory_kwargs = {"device": device, "dtype": dtype}
-
-        super().__init__()
-        if vocab_size % pad_vocab_size_multiple != 0:
-            vocab_size += pad_vocab_size_multiple - (vocab_size % pad_vocab_size_multiple)
-        self.backbone = MixerModel(
-            d_model=d_model,
-            n_layer=n_layer,
-            vocab_size=vocab_size,
-            ssm_cfg=ssm_cfg,
-            rms_norm=rms_norm,
-            initializer_cfg=initializer_cfg,
-            fused_add_norm=fused_add_norm,
-            residual_in_fp32=residual_in_fp32,
-            **factory_kwargs,
-        )
-        self.lm_head = nn.Linear(d_model, vocab_size, bias=False, **factory_kwargs)
-
-        # Initialize weights and apply final processing
-        self.apply(
-            partial(
-                _init_weights,
-                n_layer=n_layer,
-                **(initializer_cfg if initializer_cfg is not None else {}),
-            )
-        )
-        self.tie_weights()
-
-    def tie_weights(self):
-        self.lm_head.weight = self.backbone.embedding.weight
-
-    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):
-        return self.backbone.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs)
-
-    def forward(self, input_ids, position_ids=None, inference_params=None, num_last_tokens=0):
-        """
-        "position_ids" is just to be compatible with Transformer generation. We don't use it.
-        num_last_tokens: if > 0, only return the logits for the last n tokens
-        """
-        hidden_states = self.backbone(input_ids, inference_params=inference_params)
-        if num_last_tokens > 0:
-            hidden_states = hidden_states[:, -num_last_tokens:]
-        lm_logits = self.lm_head(hidden_states)
-        CausalLMOutput = namedtuple("CausalLMOutput", ["logits"])
-        return CausalLMOutput(logits=lm_logits)
-
-    @classmethod
-    def from_pretrained(cls, pretrained_model_name, device=None, dtype=None, **kwargs):
-        config_data = load_config_hf(pretrained_model_name)
-        config = MambaConfig(**config_data)
-        model = cls(config, device=device, dtype=dtype, **kwargs)
-        model.load_state_dict(load_state_dict_hf(pretrained_model_name, device=device, dtype=dtype))
-        return model
-
-    def save_pretrained(self, save_directory):
-        """
-        Minimal implementation of save_pretrained for MambaLMHeadModel.
-        Save the model and its configuration file to a directory.
-        """
-        # Ensure save_directory exists
-        if not os.path.exists(save_directory):
-            os.makedirs(save_directory)
-
-        # Save the model's state_dict
-        model_path = os.path.join(save_directory, 'pytorch_model.bin')
-        torch.save(self.state_dict(), model_path)
-
-        # Save the configuration of the model
-        config_path = os.path.join(save_directory, 'config.json')
-        with open(config_path, 'w') as f:
-            json.dump(self.config.__dict__, f)
diff --git a/based/models/mixers/mamba/ops/__init__.py b/based/models/mixers/mamba/ops/__init__.py
deleted file mode 100755
index e69de29..0000000
diff --git a/based/models/mixers/mamba/ops/selective_scan_interface.py b/based/models/mixers/mamba/ops/selective_scan_interface.py
deleted file mode 100755
index cf784fd..0000000
--- a/based/models/mixers/mamba/ops/selective_scan_interface.py
+++ /dev/null
@@ -1,357 +0,0 @@
-# Copyright (c) 2023, Tri Dao, Albert Gu.
-
-import torch
-import torch.nn.functional as F
-from torch.cuda.amp import custom_bwd, custom_fwd
-
-from einops import rearrange, repeat
-
-try:
-    from causal_conv1d import causal_conv1d_fn
-    import causal_conv1d_cuda
-except ImportError:
-    causal_conv1d_fn = None
-    causal_conv1d_cuda = None
-
-import selective_scan_cuda
-
-
-class SelectiveScanFn(torch.autograd.Function):
-
-    @staticmethod
-    def forward(ctx, u, delta, A, B, C, D=None, z=None, delta_bias=None, delta_softplus=False,
-                return_last_state=False):
-        if u.stride(-1) != 1:
-            u = u.contiguous()
-        if delta.stride(-1) != 1:
-            delta = delta.contiguous()
-        if D is not None:
-            D = D.contiguous()
-        if B.stride(-1) != 1:
-            B = B.contiguous()
-        if C.stride(-1) != 1:
-            C = C.contiguous()
-        if z is not None and z.stride(-1) != 1:
-            z = z.contiguous()
-        if B.dim() == 3:
-            B = rearrange(B, "b dstate l -> b 1 dstate l")
-            ctx.squeeze_B = True
-        if C.dim() == 3:
-            C = rearrange(C, "b dstate l -> b 1 dstate l")
-            ctx.squeeze_C = True
-        out, x, *rest = selective_scan_cuda.fwd(u, delta, A, B, C, D, z, delta_bias, delta_softplus)
-        ctx.delta_softplus = delta_softplus
-        ctx.has_z = z is not None
-        last_state = x[:, :, -1, 1::2]  # (batch, dim, dstate)
-        if not ctx.has_z:
-            ctx.save_for_backward(u, delta, A, B, C, D, delta_bias, x)
-            return out if not return_last_state else (out, last_state)
-        else:
-            ctx.save_for_backward(u, delta, A, B, C, D, z, delta_bias, x, out)
-            out_z = rest[0]
-            return out_z if not return_last_state else (out_z, last_state)
-
-    @staticmethod
-    def backward(ctx, dout, *args):
-        if not ctx.has_z:
-            u, delta, A, B, C, D, delta_bias, x = ctx.saved_tensors
-            z = None
-            out = None
-        else:
-            u, delta, A, B, C, D, z, delta_bias, x, out = ctx.saved_tensors
-        if dout.stride(-1) != 1:
-            dout = dout.contiguous()
-        # The kernel supports passing in a pre-allocated dz (e.g., in case we want to fuse the
-        # backward of selective_scan_cuda with the backward of chunk).
-        # Here we just pass in None and dz will be allocated in the C++ code.
-        du, ddelta, dA, dB, dC, dD, ddelta_bias, *rest = selective_scan_cuda.bwd(
-            u, delta, A, B, C, D, z, delta_bias, dout, x, out, None, ctx.delta_softplus,
-            False  # option to recompute out_z, not used here
-        )
-        dz = rest[0] if ctx.has_z else None
-        dB = dB.squeeze(1) if getattr(ctx, "squeeze_B", False) else dB
-        dC = dC.squeeze(1) if getattr(ctx, "squeeze_C", False) else dC
-        return (du, ddelta, dA, dB, dC,
-                dD if D is not None else None,
-                dz,
-                ddelta_bias if delta_bias is not None else None,
-                None,
-                None)
-
-
-def selective_scan_fn(u, delta, A, B, C, D=None, z=None, delta_bias=None, delta_softplus=False,
-                     return_last_state=False):
-    """if return_last_state is True, returns (out, last_state)
-    last_state has shape (batch, dim, dstate). Note that the gradient of the last state is
-    not considered in the backward pass.
-    """
-    return SelectiveScanFn.apply(u, delta, A, B, C, D, z, delta_bias, delta_softplus, return_last_state)
-
-
-def selective_scan_ref(u, delta, A, B, C, D=None, z=None, delta_bias=None, delta_softplus=False,
-                      return_last_state=False):
-    """
-    u: r(B D L)
-    delta: r(B D L)
-    A: c(D N) or r(D N)
-    B: c(D N) or r(B N L) or r(B N 2L) or r(B G N L) or (B G N L)
-    C: c(D N) or r(B N L) or r(B N 2L) or r(B G N L) or (B G N L)
-    D: r(D)
-    z: r(B D L)
-    delta_bias: r(D), fp32
-
-    out: r(B D L)
-    last_state (optional): r(B D dstate) or c(B D dstate)
-    """
-    dtype_in = u.dtype
-    u = u.float()
-    delta = delta.float()
-    if delta_bias is not None:
-        delta = delta + delta_bias[..., None].float()
-    if delta_softplus:
-        delta = F.softplus(delta)
-    batch, dim, dstate = u.shape[0], A.shape[0], A.shape[1]
-    is_variable_B = B.dim() >= 3
-    is_variable_C = C.dim() >= 3
-    if A.is_complex():
-        if is_variable_B:
-            B = torch.view_as_complex(rearrange(B.float(), "... (L two) -> ... L two", two=2))
-        if is_variable_C:
-            C = torch.view_as_complex(rearrange(C.float(), "... (L two) -> ... L two", two=2))
-    else:
-        B = B.float()
-        C = C.float()
-    x = A.new_zeros((batch, dim, dstate))
-    ys = []
-    deltaA = torch.exp(torch.einsum('bdl,dn->bdln', delta, A))
-    if not is_variable_B:
-        deltaB_u = torch.einsum('bdl,dn,bdl->bdln', delta, B, u)
-    else:
-        if B.dim() == 3:
-            deltaB_u = torch.einsum('bdl,bnl,bdl->bdln', delta, B, u)
-        else:
-            B = repeat(B, "B G N L -> B (G H) N L", H=dim // B.shape[1])
-            deltaB_u = torch.einsum('bdl,bdnl,bdl->bdln', delta, B, u)
-    if is_variable_C and C.dim() == 4:
-        C = repeat(C, "B G N L -> B (G H) N L", H=dim // C.shape[1])
-    last_state = None
-    for i in range(u.shape[2]):
-        x = deltaA[:, :, i] * x + deltaB_u[:, :, i]
-        if not is_variable_C:
-            y = torch.einsum('bdn,dn->bd', x, C)
-        else:
-            if C.dim() == 3:
-                y = torch.einsum('bdn,bn->bd', x, C[:, :, i])
-            else:
-                y = torch.einsum('bdn,bdn->bd', x, C[:, :, :, i])
-        if i == u.shape[2] - 1:
-            last_state = x
-        if y.is_complex():
-            y = y.real * 2
-        ys.append(y)
-    y = torch.stack(ys, dim=2) # (batch dim L)
-    out = y if D is None else y + u * rearrange(D, "d -> d 1")
-    if z is not None:
-        out = out * F.silu(z)
-    out = out.to(dtype=dtype_in)
-    return out if not return_last_state else (out, last_state)
-
-
-class MambaInnerFn(torch.autograd.Function):
-
-    @staticmethod
-    @custom_fwd
-    def forward(ctx, xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,
-                out_proj_weight, out_proj_bias,
-                A, B=None, C=None, D=None, delta_bias=None, B_proj_bias=None,
-                C_proj_bias=None, delta_softplus=True, checkpoint_lvl=1):
-        """
-             xz: (batch, dim, seqlen)
-        """
-        assert causal_conv1d_cuda is not None, "causal_conv1d_cuda is not available. Please install causal-conv1d."
-        assert checkpoint_lvl in [0, 1]
-        L = xz.shape[-1]
-        delta_rank = delta_proj_weight.shape[1]
-        d_state = A.shape[-1] * (1 if not A.is_complex() else 2)
-        if torch.is_autocast_enabled():
-            x_proj_weight = x_proj_weight.to(dtype=torch.get_autocast_gpu_dtype())
-            delta_proj_weight = delta_proj_weight.to(dtype=torch.get_autocast_gpu_dtype())
-            out_proj_weight = out_proj_weight.to(dtype=torch.get_autocast_gpu_dtype())
-            out_proj_bias = (out_proj_bias.to(dtype=torch.get_autocast_gpu_dtype())
-                             if out_proj_bias is not None else None)
-        if xz.stride(-1) != 1:
-            xz = xz.contiguous()
-        conv1d_weight = rearrange(conv1d_weight, "d 1 w -> d w")
-        x, z = xz.chunk(2, dim=1)
-        conv1d_bias = conv1d_bias.contiguous() if conv1d_bias is not None else None
-        conv1d_out = causal_conv1d_cuda.causal_conv1d_fwd(
-            x, conv1d_weight, conv1d_bias, None, None, None, True
-        )
-        # We're being very careful here about the layout, to avoid extra transposes.
-        # We want delta to have d as the slowest moving dimension
-        # and L as the fastest moving dimension, since those are what the ssm_scan kernel expects.
-        x_dbl = F.linear(rearrange(conv1d_out, 'b d l -> (b l) d'), x_proj_weight)  # (bl d)
-        delta = rearrange(delta_proj_weight @ x_dbl[:, :delta_rank].t(), "d (b l) -> b d l", l = L)
-        ctx.is_variable_B = B is None
-        ctx.is_variable_C = C is None
-        ctx.B_proj_bias_is_None = B_proj_bias is None
-        ctx.C_proj_bias_is_None = C_proj_bias is None
-        if B is None:  # variable B
-            B = x_dbl[:, delta_rank:delta_rank + d_state]  # (bl dstate)
-            if B_proj_bias is not None:
-                B = B + B_proj_bias.to(dtype=B.dtype)
-            if not A.is_complex():
-                # B = rearrange(B, "(b l) dstate -> b dstate l", l=L).contiguous()
-                B = rearrange(B, "(b l) dstate -> b 1 dstate l", l=L).contiguous()
-            else:
-                B = rearrange(B, "(b l) (dstate two) -> b 1 dstate (l two)", l=L, two=2).contiguous()
-        else:
-            if B.stride(-1) != 1:
-                B = B.contiguous()
-        if C is None:  # variable C
-            C = x_dbl[:, -d_state:]  # (bl dstate)
-            if C_proj_bias is not None:
-                C = C + C_proj_bias.to(dtype=C.dtype)
-            if not A.is_complex():
-                # C = rearrange(C, "(b l) dstate -> b dstate l", l=L).contiguous()
-                C = rearrange(C, "(b l) dstate -> b 1 dstate l", l=L).contiguous()
-            else:
-                C = rearrange(C, "(b l) (dstate two) -> b 1 dstate (l two)", l=L, two=2).contiguous()
-        else:
-            if C.stride(-1) != 1:
-                C = C.contiguous()
-        if D is not None:
-            D = D.contiguous()
-        out, scan_intermediates, out_z = selective_scan_cuda.fwd(
-            conv1d_out, delta, A, B, C, D, z, delta_bias, delta_softplus
-        )
-        ctx.delta_softplus = delta_softplus
-        ctx.out_proj_bias_is_None = out_proj_bias is None
-        ctx.checkpoint_lvl = checkpoint_lvl
-        if checkpoint_lvl >= 1:  # Will recompute conv1d_out and delta in the backward pass
-            conv1d_out, delta = None, None
-        ctx.save_for_backward(xz, conv1d_weight, conv1d_bias, x_dbl, x_proj_weight,
-                              delta_proj_weight, out_proj_weight, conv1d_out, delta,
-                              A, B, C, D, delta_bias, scan_intermediates, out)
-        return F.linear(rearrange(out_z, "b d l -> b l d"), out_proj_weight, out_proj_bias)
-
-    @staticmethod
-    @custom_bwd
-    def backward(ctx, dout):
-        # dout: (batch, seqlen, dim)
-        assert causal_conv1d_cuda is not None, "causal_conv1d_cuda is not available. Please install causal-conv1d."
-        (xz, conv1d_weight, conv1d_bias, x_dbl, x_proj_weight, delta_proj_weight, out_proj_weight,
-         conv1d_out, delta, A, B, C, D, delta_bias, scan_intermediates, out) = ctx.saved_tensors
-        L = xz.shape[-1]
-        delta_rank = delta_proj_weight.shape[1]
-        d_state = A.shape[-1] * (1 if not A.is_complex() else 2)
-        x, z = xz.chunk(2, dim=1)
-        if dout.stride(-1) != 1:
-            dout = dout.contiguous()
-        if ctx.checkpoint_lvl == 1:
-            conv1d_out = causal_conv1d_cuda.causal_conv1d_fwd(
-                x, conv1d_weight, conv1d_bias, None, None, None, True
-            )
-            delta = rearrange(delta_proj_weight @ x_dbl[:, :delta_rank].t(),
-                              "d (b l) -> b d l", l = L)
-        # The kernel supports passing in a pre-allocated dz (e.g., in case we want to fuse the
-        # backward of selective_scan_cuda with the backward of chunk).
-        dxz = torch.empty_like(xz)  # (batch, dim, seqlen)
-        dx, dz = dxz.chunk(2, dim=1)
-        dout = rearrange(dout, "b l e -> e (b l)")
-        dout_y = rearrange(out_proj_weight.t() @ dout, "d (b l) -> b d l", l=L)
-        dconv1d_out, ddelta, dA, dB, dC, dD, ddelta_bias, dz, out_z = selective_scan_cuda.bwd(
-            conv1d_out, delta, A, B, C, D, z, delta_bias, dout_y, scan_intermediates, out, dz,
-            ctx.delta_softplus,
-            True  # option to recompute out_z
-        )
-        dout_proj_weight = torch.einsum("eB,dB->ed", dout, rearrange(out_z, "b d l -> d (b l)"))
-        dout_proj_bias = dout.sum(dim=(0, 1)) if not ctx.out_proj_bias_is_None else None
-        dD = dD if D is not None else None
-        dx_dbl = torch.empty_like(x_dbl)
-        dB_proj_bias = None
-        if ctx.is_variable_B:
-            if not A.is_complex():
-                dB = rearrange(dB, "b 1 dstate l -> (b l) dstate").contiguous()
-            else:
-                dB = rearrange(dB, "b 1 dstate (l two) -> (b l) (dstate two)", two=2).contiguous()
-            dB_proj_bias = dB.sum(0) if not ctx.B_proj_bias_is_None else None
-            dx_dbl[:, delta_rank:delta_rank + d_state] = dB  # (bl d)
-            dB = None
-        dC_proj_bias = None
-        if ctx.is_variable_C:
-            if not A.is_complex():
-                dC = rearrange(dC, "b 1 dstate l -> (b l) dstate").contiguous()
-            else:
-                dC = rearrange(dC, "b 1 dstate (l two) -> (b l) (dstate two)", two=2).contiguous()
-            dC_proj_bias = dC.sum(0) if not ctx.C_proj_bias_is_None else None
-            dx_dbl[:, -d_state:] = dC  # (bl d)
-            dC = None
-        ddelta = rearrange(ddelta, "b d l -> d (b l)")
-        ddelta_proj_weight = torch.einsum("dB,Br->dr", ddelta, x_dbl[:, :delta_rank])
-        dx_dbl[:, :delta_rank] = torch.einsum("dB,dr->Br", ddelta, delta_proj_weight)
-        dconv1d_out = rearrange(dconv1d_out, "b d l -> d (b l)")
-        dx_proj_weight = torch.einsum("Br,Bd->rd", dx_dbl, rearrange(conv1d_out, "b d l -> (b l) d"))
-        dconv1d_out = torch.addmm(dconv1d_out, x_proj_weight.t(), dx_dbl.t(), out=dconv1d_out)
-        dconv1d_out = rearrange(dconv1d_out, "d (b l) -> b d l", b=x.shape[0], l=x.shape[-1])
-        # The kernel supports passing in a pre-allocated dx (e.g., in case we want to fuse the
-        # backward of conv1d with the backward of chunk).
-        dx, dconv1d_weight, dconv1d_bias, *_ = causal_conv1d_cuda.causal_conv1d_bwd(
-            x, conv1d_weight, conv1d_bias, dconv1d_out, None, None, None, dx, False, True
-        )
-        dconv1d_bias = dconv1d_bias if conv1d_bias is not None else None
-        dconv1d_weight = rearrange(dconv1d_weight, "d w -> d 1 w")
-        return (dxz, dconv1d_weight, dconv1d_bias, dx_proj_weight, ddelta_proj_weight,
-                dout_proj_weight, dout_proj_bias,
-                dA, dB, dC, dD,
-                ddelta_bias if delta_bias is not None else None,
-                dB_proj_bias, dC_proj_bias, None)
-
-
-def mamba_inner_fn(
-    xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,
-    out_proj_weight, out_proj_bias,
-    A, B=None, C=None, D=None, delta_bias=None, B_proj_bias=None,
-    C_proj_bias=None, delta_softplus=True
-):
-    return MambaInnerFn.apply(xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,
-                              out_proj_weight, out_proj_bias,
-                              A, B, C, D, delta_bias, B_proj_bias, C_proj_bias, delta_softplus)
-
-
-def mamba_inner_ref(
-    xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,
-    out_proj_weight, out_proj_bias,
-    A, B=None, C=None, D=None, delta_bias=None, B_proj_bias=None,
-    C_proj_bias=None, delta_softplus=True
-):
-    assert causal_conv1d_fn is not None, "causal_conv1d_fn is not available. Please install causal-conv1d."
-    L = xz.shape[-1]
-    delta_rank = delta_proj_weight.shape[1]
-    d_state = A.shape[-1] * (1 if not A.is_complex() else 2)
-    x, z = xz.chunk(2, dim=1)
-    x = causal_conv1d_fn(x, rearrange(conv1d_weight, "d 1 w -> d w"), conv1d_bias, activation="silu")
-    # We're being very careful here about the layout, to avoid extra transposes.
-    # We want delta to have d as the slowest moving dimension
-    # and L as the fastest moving dimension, since those are what the ssm_scan kernel expects.
-    x_dbl = F.linear(rearrange(x, 'b d l -> (b l) d'), x_proj_weight)  # (bl d)
-    delta = delta_proj_weight @ x_dbl[:, :delta_rank].t()
-    delta = rearrange(delta, "d (b l) -> b d l", l=L)
-    if B is None:  # variable B
-        B = x_dbl[:, delta_rank:delta_rank + d_state]  # (bl d)
-        if B_proj_bias is not None:
-            B = B + B_proj_bias.to(dtype=B.dtype)
-        if not A.is_complex():
-            B = rearrange(B, "(b l) dstate -> b dstate l", l=L).contiguous()
-        else:
-            B = rearrange(B, "(b l) (dstate two) -> b dstate (l two)", l=L, two=2).contiguous()
-    if C is None:  # variable B
-        C = x_dbl[:, -d_state:]  # (bl d)
-        if C_proj_bias is not None:
-            C = C + C_proj_bias.to(dtype=C.dtype)
-        if not A.is_complex():
-            C = rearrange(C, "(b l) dstate -> b dstate l", l=L).contiguous()
-        else:
-            C = rearrange(C, "(b l) (dstate two) -> b dstate (l two)", l=L, two=2).contiguous()
-    y = selective_scan_fn(x, delta, A, B, C, D, z=z, delta_bias=delta_bias, delta_softplus=True)
-    return F.linear(rearrange(y, "b d l -> b l d"), out_proj_weight, out_proj_bias)
\ No newline at end of file
diff --git a/based/models/mixers/mamba/ops/triton/__init__.py b/based/models/mixers/mamba/ops/triton/__init__.py
deleted file mode 100755
index e69de29..0000000
diff --git a/based/models/mixers/mamba/ops/triton/layernorm.py b/based/models/mixers/mamba/ops/triton/layernorm.py
deleted file mode 100755
index 1babca4..0000000
--- a/based/models/mixers/mamba/ops/triton/layernorm.py
+++ /dev/null
@@ -1,635 +0,0 @@
-# Copyright (c) 2023, Tri Dao.
-# Implement residual + layer_norm / rms_norm.
-
-# Based on the Triton LayerNorm tutorial: https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html
-# For the backward pass, we keep weight_grad and bias_grad in registers and accumulate.
-# This is faster for dimensions up to 8k, but after that it's much slower due to register spilling.
-# The models we train have hidden dim up to 8k anyway (e.g. Llama 70B), so this is fine.
-
-import math
-
-import torch
-import torch.nn.functional as F
-from torch.cuda.amp import custom_fwd, custom_bwd
-
-import triton
-import triton.language as tl
-
-
-def layer_norm_ref(x, weight, bias, residual=None, eps=1e-6, prenorm=False, upcast=False):
-    dtype = x.dtype
-    if upcast:
-        weight = weight.float()
-        bias = bias.float() if bias is not None else None
-    if upcast:
-        x = x.float()
-        residual = residual.float() if residual is not None else residual
-    if residual is not None:
-        x = (x + residual).to(x.dtype)
-    out = F.layer_norm(x.to(weight.dtype), x.shape[-1:], weight=weight, bias=bias, eps=eps).to(
-        dtype
-    )
-    return out if not prenorm else (out, x)
-
-
-def rms_norm_ref(x, weight, bias, residual=None, eps=1e-6, prenorm=False, upcast=False):
-    dtype = x.dtype
-    if upcast:
-        weight = weight.float()
-        bias = bias.float() if bias is not None else None
-    if upcast:
-        x = x.float()
-        residual = residual.float() if residual is not None else residual
-    if residual is not None:
-        x = (x + residual).to(x.dtype)
-    rstd = 1 / torch.sqrt((x.square()).mean(dim=-1, keepdim=True) + eps)
-    out = (x * rstd * weight) + bias if bias is not None else (x * rstd * weight)
-    out = out.to(dtype)
-    return out if not prenorm else (out, x)
-
-
-@triton.autotune(
-    configs=[
-        triton.Config({}, num_warps=1),
-        triton.Config({}, num_warps=2),
-        triton.Config({}, num_warps=4),
-        triton.Config({}, num_warps=8),
-        triton.Config({}, num_warps=16),
-        triton.Config({}, num_warps=32),
-    ],
-    key=["N", "HAS_RESIDUAL", "STORE_RESIDUAL_OUT", "IS_RMS_NORM", "HAS_BIAS"],
-)
-# @triton.heuristics({"HAS_BIAS": lambda args: args["B"] is not None})
-# @triton.heuristics({"HAS_RESIDUAL": lambda args: args["RESIDUAL"] is not None})
-@triton.jit
-def _layer_norm_fwd_1pass_kernel(
-    X,  # pointer to the input
-    Y,  # pointer to the output
-    W,  # pointer to the weights
-    B,  # pointer to the biases
-    RESIDUAL,  # pointer to the residual
-    RESIDUAL_OUT,  # pointer to the residual
-    Mean,  # pointer to the mean
-    Rstd,  # pointer to the 1/std
-    stride_x_row,  # how much to increase the pointer when moving by 1 row
-    stride_y_row,
-    stride_res_row,
-    stride_res_out_row,
-    N,  # number of columns in X
-    eps,  # epsilon to avoid division by zero
-    IS_RMS_NORM: tl.constexpr,
-    BLOCK_N: tl.constexpr,
-    HAS_RESIDUAL: tl.constexpr,
-    STORE_RESIDUAL_OUT: tl.constexpr,
-    HAS_BIAS: tl.constexpr,
-):
-    # Map the program id to the row of X and Y it should compute.
-    row = tl.program_id(0)
-    X += row * stride_x_row
-    Y += row * stride_y_row
-    if HAS_RESIDUAL:
-        RESIDUAL += row * stride_res_row
-    if STORE_RESIDUAL_OUT:
-        RESIDUAL_OUT += row * stride_res_out_row
-    # Compute mean and variance
-    cols = tl.arange(0, BLOCK_N)
-    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)
-    if HAS_RESIDUAL:
-        residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0).to(tl.float32)
-        x += residual
-    if STORE_RESIDUAL_OUT:
-        tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)
-    if not IS_RMS_NORM:
-        mean = tl.sum(x, axis=0) / N
-        tl.store(Mean + row, mean)
-        xbar = tl.where(cols < N, x - mean, 0.0)
-        var = tl.sum(xbar * xbar, axis=0) / N
-    else:
-        xbar = tl.where(cols < N, x, 0.0)
-        var = tl.sum(xbar * xbar, axis=0) / N
-    rstd = 1 / tl.sqrt(var + eps)
-    tl.store(Rstd + row, rstd)
-    # Normalize and apply linear transformation
-    mask = cols < N
-    w = tl.load(W + cols, mask=mask).to(tl.float32)
-    if HAS_BIAS:
-        b = tl.load(B + cols, mask=mask).to(tl.float32)
-    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd
-    y = x_hat * w + b if HAS_BIAS else x_hat * w
-    # Write output
-    tl.store(Y + cols, y, mask=mask)
-
-
-def _layer_norm_fwd(
-    x, weight, bias, eps, residual=None, out_dtype=None, residual_dtype=None, is_rms_norm=False
-):
-    if residual is not None:
-        residual_dtype = residual.dtype
-    M, N = x.shape
-    assert x.stride(-1) == 1
-    if residual is not None:
-        assert residual.stride(-1) == 1
-        assert residual.shape == (M, N)
-    assert weight.shape == (N,)
-    assert weight.stride(-1) == 1
-    if bias is not None:
-        assert bias.stride(-1) == 1
-        assert bias.shape == (N,)
-    # allocate output
-    y = torch.empty_like(x, dtype=x.dtype if out_dtype is None else out_dtype)
-    assert y.stride(-1) == 1
-    if residual is not None or (residual_dtype is not None and residual_dtype != x.dtype):
-        residual_out = torch.empty(M, N, device=x.device, dtype=residual_dtype)
-        assert residual_out.stride(-1) == 1
-    else:
-        residual_out = None
-    mean = torch.empty((M,), dtype=torch.float32, device="cuda") if not is_rms_norm else None
-    rstd = torch.empty((M,), dtype=torch.float32, device="cuda")
-    # Less than 64KB per feature: enqueue fused kernel
-    MAX_FUSED_SIZE = 65536 // x.element_size()
-    BLOCK_N = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))
-    if N > BLOCK_N:
-        raise RuntimeError("This layer norm doesn't support feature dim >= 64KB.")
-    # heuristics for number of warps
-    with torch.cuda.device(x.device.index):
-        _layer_norm_fwd_1pass_kernel[(M,)](
-            x,
-            y,
-            weight,
-            bias,
-            residual,
-            residual_out,
-            mean,
-            rstd,
-            x.stride(0),
-            y.stride(0),
-            residual.stride(0) if residual is not None else 0,
-            residual_out.stride(0) if residual_out is not None else 0,
-            N,
-            eps,
-            is_rms_norm,
-            BLOCK_N,
-            residual is not None,
-            residual_out is not None,
-            bias is not None,
-        )
-    # residual_out is None if residual is None and residual_dtype == input_dtype
-    return y, mean, rstd, residual_out if residual_out is not None else x
-
-
-@triton.autotune(
-    configs=[
-        triton.Config({}, num_warps=1),
-        triton.Config({}, num_warps=2),
-        triton.Config({}, num_warps=4),
-        triton.Config({}, num_warps=8),
-        triton.Config({}, num_warps=16),
-        triton.Config({}, num_warps=32),
-    ],
-    key=["N", "HAS_DRESIDUAL", "STORE_DRESIDUAL", "IS_RMS_NORM", "HAS_BIAS"],
-)
-# @triton.heuristics({"HAS_BIAS": lambda args: args["B"] is not None})
-# @triton.heuristics({"HAS_DRESIDUAL": lambda args: args["DRESIDUAL"] is not None})
-# @triton.heuristics({"STORE_DRESIDUAL": lambda args: args["DRESIDUAL_IN"] is not None})
-@triton.heuristics({"RECOMPUTE_OUTPUT": lambda args: args["Y"] is not None})
-@triton.jit
-def _layer_norm_bwd_kernel(
-    X,  # pointer to the input
-    W,  # pointer to the weights
-    B,  # pointer to the biases
-    Y,  # pointer to the output to be recomputed
-    DY,  # pointer to the output gradient
-    DX,  # pointer to the input gradient
-    DW,  # pointer to the partial sum of weights gradient
-    DB,  # pointer to the partial sum of biases gradient
-    DRESIDUAL,
-    DRESIDUAL_IN,
-    Mean,  # pointer to the mean
-    Rstd,  # pointer to the 1/std
-    stride_x_row,  # how much to increase the pointer when moving by 1 row
-    stride_y_row,
-    stride_dy_row,
-    stride_dx_row,
-    stride_dres_row,
-    stride_dres_in_row,
-    M,  # number of rows in X
-    N,  # number of columns in X
-    eps,  # epsilon to avoid division by zero
-    rows_per_program,
-    IS_RMS_NORM: tl.constexpr,
-    BLOCK_N: tl.constexpr,
-    HAS_DRESIDUAL: tl.constexpr,
-    STORE_DRESIDUAL: tl.constexpr,
-    HAS_BIAS: tl.constexpr,
-    RECOMPUTE_OUTPUT: tl.constexpr,
-):
-    # Map the program id to the elements of X, DX, and DY it should compute.
-    row_block_id = tl.program_id(0)
-    row_start = row_block_id * rows_per_program
-    cols = tl.arange(0, BLOCK_N)
-    mask = cols < N
-    X += row_start * stride_x_row
-    if HAS_DRESIDUAL:
-        DRESIDUAL += row_start * stride_dres_row
-    if STORE_DRESIDUAL:
-        DRESIDUAL_IN += row_start * stride_dres_in_row
-    DY += row_start * stride_dy_row
-    DX += row_start * stride_dx_row
-    if RECOMPUTE_OUTPUT:
-        Y += row_start * stride_y_row
-    w = tl.load(W + cols, mask=mask).to(tl.float32)
-    if RECOMPUTE_OUTPUT and HAS_BIAS:
-        b = tl.load(B + cols, mask=mask, other=0.0).to(tl.float32)
-    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)
-    if HAS_BIAS:
-        db = tl.zeros((BLOCK_N,), dtype=tl.float32)
-    row_end = min((row_block_id + 1) * rows_per_program, M)
-    for row in range(row_start, row_end):
-        # Load data to SRAM
-        x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)
-        dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)
-        if not IS_RMS_NORM:
-            mean = tl.load(Mean + row)
-        rstd = tl.load(Rstd + row)
-        # Compute dx
-        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd
-        xhat = tl.where(mask, xhat, 0.0)
-        if RECOMPUTE_OUTPUT:
-            y = xhat * w + b if HAS_BIAS else xhat * w
-            tl.store(Y + cols, y, mask=mask)
-        wdy = w * dy
-        dw += dy * xhat
-        if HAS_BIAS:
-            db += dy
-        if not IS_RMS_NORM:
-            c1 = tl.sum(xhat * wdy, axis=0) / N
-            c2 = tl.sum(wdy, axis=0) / N
-            dx = (wdy - (xhat * c1 + c2)) * rstd
-        else:
-            c1 = tl.sum(xhat * wdy, axis=0) / N
-            dx = (wdy - xhat * c1) * rstd
-        if HAS_DRESIDUAL:
-            dres = tl.load(DRESIDUAL + cols, mask=mask, other=0).to(tl.float32)
-            dx += dres
-        # Write dx
-        if STORE_DRESIDUAL:
-            tl.store(DRESIDUAL_IN + cols, dx, mask=mask)
-        tl.store(DX + cols, dx, mask=mask)
-
-        X += stride_x_row
-        if HAS_DRESIDUAL:
-            DRESIDUAL += stride_dres_row
-        if STORE_DRESIDUAL:
-            DRESIDUAL_IN += stride_dres_in_row
-        if RECOMPUTE_OUTPUT:
-            Y += stride_y_row
-        DY += stride_dy_row
-        DX += stride_dx_row
-    tl.store(DW + row_block_id * N + cols, dw, mask=mask)
-    if HAS_BIAS:
-        tl.store(DB + row_block_id * N + cols, db, mask=mask)
-
-
-def _layer_norm_bwd(
-    dy,
-    x,
-    weight,
-    bias,
-    eps,
-    mean,
-    rstd,
-    dresidual=None,
-    has_residual=False,
-    is_rms_norm=False,
-    x_dtype=None,
-    recompute_output=False,
-):
-    M, N = x.shape
-    assert x.stride(-1) == 1
-    assert dy.stride(-1) == 1
-    assert dy.shape == (M, N)
-    if dresidual is not None:
-        assert dresidual.stride(-1) == 1
-        assert dresidual.shape == (M, N)
-    assert weight.shape == (N,)
-    assert weight.stride(-1) == 1
-    if bias is not None:
-        assert bias.stride(-1) == 1
-        assert bias.shape == (N,)
-    # allocate output
-    dx = (
-        torch.empty_like(x)
-        if x_dtype is None
-        else torch.empty(M, N, dtype=x_dtype, device=x.device)
-    )
-    dresidual_in = torch.empty_like(x) if has_residual and dx.dtype != x.dtype else None
-    y = torch.empty(M, N, dtype=dy.dtype, device=dy.device) if recompute_output else None
-
-    # Less than 64KB per feature: enqueue fused kernel
-    MAX_FUSED_SIZE = 65536 // x.element_size()
-    BLOCK_N = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))
-    if N > BLOCK_N:
-        raise RuntimeError("This layer norm doesn't support feature dim >= 64KB.")
-    sm_count = torch.cuda.get_device_properties(x.device).multi_processor_count
-    _dw = torch.empty((sm_count, N), dtype=torch.float32, device=weight.device)
-    _db = (
-        torch.empty((sm_count, N), dtype=torch.float32, device=bias.device)
-        if bias is not None
-        else None
-    )
-    rows_per_program = math.ceil(M / sm_count)
-    grid = (sm_count,)
-    with torch.cuda.device(x.device.index):
-        _layer_norm_bwd_kernel[grid](
-            x,
-            weight,
-            bias,
-            y,
-            dy,
-            dx,
-            _dw,
-            _db,
-            dresidual,
-            dresidual_in,
-            mean,
-            rstd,
-            x.stride(0),
-            0 if not recompute_output else y.stride(0),
-            dy.stride(0),
-            dx.stride(0),
-            dresidual.stride(0) if dresidual is not None else 0,
-            dresidual_in.stride(0) if dresidual_in is not None else 0,
-            M,
-            N,
-            eps,
-            rows_per_program,
-            is_rms_norm,
-            BLOCK_N,
-            dresidual is not None,
-            dresidual_in is not None,
-            bias is not None,
-        )
-    dw = _dw.sum(0).to(weight.dtype)
-    db = _db.sum(0).to(bias.dtype) if bias is not None else None
-    # Don't need to compute dresidual_in separately in this case
-    if has_residual and dx.dtype == x.dtype:
-        dresidual_in = dx
-    return (dx, dw, db, dresidual_in) if not recompute_output else (dx, dw, db, dresidual_in, y)
-
-
-class LayerNormFn(torch.autograd.Function):
-    @staticmethod
-    def forward(
-        ctx,
-        x,
-        weight,
-        bias,
-        residual=None,
-        eps=1e-6,
-        prenorm=False,
-        residual_in_fp32=False,
-        is_rms_norm=False,
-    ):
-        x_shape_og = x.shape
-        # reshape input data into 2D tensor
-        x = x.reshape(-1, x.shape[-1])
-        if x.stride(-1) != 1:
-            x = x.contiguous()
-        if residual is not None:
-            assert residual.shape == x_shape_og
-            residual = residual.reshape(-1, residual.shape[-1])
-            if residual.stride(-1) != 1:
-                residual = residual.contiguous()
-        weight = weight.contiguous()
-        if bias is not None:
-            bias = bias.contiguous()
-        residual_dtype = (
-            residual.dtype
-            if residual is not None
-            else (torch.float32 if residual_in_fp32 else None)
-        )
-        y, mean, rstd, residual_out = _layer_norm_fwd(
-            x, weight, bias, eps, residual, residual_dtype=residual_dtype, is_rms_norm=is_rms_norm
-        )
-        ctx.save_for_backward(residual_out, weight, bias, mean, rstd)
-        ctx.x_shape_og = x_shape_og
-        ctx.eps = eps
-        ctx.is_rms_norm = is_rms_norm
-        ctx.has_residual = residual is not None
-        ctx.prenorm = prenorm
-        ctx.x_dtype = x.dtype
-        y = y.reshape(x_shape_og)
-        return y if not prenorm else (y, residual_out.reshape(x_shape_og))
-
-    @staticmethod
-    def backward(ctx, dy, *args):
-        x, weight, bias, mean, rstd = ctx.saved_tensors
-        dy = dy.reshape(-1, dy.shape[-1])
-        if dy.stride(-1) != 1:
-            dy = dy.contiguous()
-        assert dy.shape == x.shape
-        if ctx.prenorm:
-            dresidual = args[0]
-            dresidual = dresidual.reshape(-1, dresidual.shape[-1])
-            if dresidual.stride(-1) != 1:
-                dresidual = dresidual.contiguous()
-            assert dresidual.shape == x.shape
-        else:
-            dresidual = None
-        dx, dw, db, dresidual_in = _layer_norm_bwd(
-            dy,
-            x,
-            weight,
-            bias,
-            ctx.eps,
-            mean,
-            rstd,
-            dresidual,
-            ctx.has_residual,
-            ctx.is_rms_norm,
-            x_dtype=ctx.x_dtype,
-        )
-        return (
-            dx.reshape(ctx.x_shape_og),
-            dw,
-            db,
-            dresidual_in.reshape(ctx.x_shape_og) if ctx.has_residual else None,
-            None,
-            None,
-            None,
-            None,
-        )
-
-
-def layer_norm_fn(
-    x,
-    weight,
-    bias,
-    residual=None,
-    eps=1e-6,
-    prenorm=False,
-    residual_in_fp32=False,
-    is_rms_norm=False,
-):
-    return LayerNormFn.apply(x, weight, bias, residual, eps, prenorm, residual_in_fp32, is_rms_norm)
-
-
-def rms_norm_fn(x, weight, bias, residual=None, prenorm=False, residual_in_fp32=False, eps=1e-6):
-    return LayerNormFn.apply(x, weight, bias, residual, eps, prenorm, residual_in_fp32, True)
-
-
-class RMSNorm(torch.nn.Module):
-    def __init__(self, hidden_size, eps=1e-5, device=None, dtype=None):
-        factory_kwargs = {"device": device, "dtype": dtype}
-        super().__init__()
-        self.eps = eps
-        self.weight = torch.nn.Parameter(torch.empty(hidden_size, **factory_kwargs))
-        self.register_parameter("bias", None)
-        self.reset_parameters()
-
-    def reset_parameters(self):
-        torch.nn.init.ones_(self.weight)
-
-    def forward(self, x, residual=None, prenorm=False, residual_in_fp32=False):
-        return rms_norm_fn(
-            x,
-            self.weight,
-            self.bias,
-            residual=residual,
-            eps=self.eps,
-            prenorm=prenorm,
-            residual_in_fp32=residual_in_fp32,
-        )
-
-
-class LayerNormLinearFn(torch.autograd.Function):
-    @staticmethod
-    @custom_fwd
-    def forward(
-        ctx,
-        x,
-        norm_weight,
-        norm_bias,
-        linear_weight,
-        linear_bias,
-        residual=None,
-        eps=1e-6,
-        prenorm=False,
-        residual_in_fp32=False,
-        is_rms_norm=False,
-    ):
-        x_shape_og = x.shape
-        # reshape input data into 2D tensor
-        x = x.reshape(-1, x.shape[-1])
-        if x.stride(-1) != 1:
-            x = x.contiguous()
-        if residual is not None:
-            assert residual.shape == x_shape_og
-            residual = residual.reshape(-1, residual.shape[-1])
-            if residual.stride(-1) != 1:
-                residual = residual.contiguous()
-        norm_weight = norm_weight.contiguous()
-        if norm_bias is not None:
-            norm_bias = norm_bias.contiguous()
-        residual_dtype = (
-            residual.dtype
-            if residual is not None
-            else (torch.float32 if residual_in_fp32 else None)
-        )
-        y, mean, rstd, residual_out = _layer_norm_fwd(
-            x,
-            norm_weight,
-            norm_bias,
-            eps,
-            residual,
-            out_dtype=None if not torch.is_autocast_enabled() else torch.get_autocast_gpu_dtype(),
-            residual_dtype=residual_dtype,
-            is_rms_norm=is_rms_norm,
-        )
-        y = y.reshape(x_shape_og)
-        dtype = torch.get_autocast_gpu_dtype() if torch.is_autocast_enabled() else y.dtype
-        linear_weight = linear_weight.to(dtype)
-        linear_bias = linear_bias.to(dtype) if linear_bias is not None else None
-        out = F.linear(y.to(linear_weight.dtype), linear_weight, linear_bias)
-        # We don't store y, will be recomputed in the backward pass to save memory
-        ctx.save_for_backward(residual_out, norm_weight, norm_bias, linear_weight, mean, rstd)
-        ctx.x_shape_og = x_shape_og
-        ctx.eps = eps
-        ctx.is_rms_norm = is_rms_norm
-        ctx.has_residual = residual is not None
-        ctx.prenorm = prenorm
-        ctx.x_dtype = x.dtype
-        ctx.linear_bias_is_none = linear_bias is None
-        return out if not prenorm else (out, residual_out.reshape(x_shape_og))
-
-    @staticmethod
-    @custom_bwd
-    def backward(ctx, dout, *args):
-        x, norm_weight, norm_bias, linear_weight, mean, rstd = ctx.saved_tensors
-        dout = dout.reshape(-1, dout.shape[-1])
-        dy = F.linear(dout, linear_weight.t())
-        dlinear_bias = None if ctx.linear_bias_is_none else dout.sum(0)
-        if dy.stride(-1) != 1:
-            dy = dy.contiguous()
-        assert dy.shape == x.shape
-        if ctx.prenorm:
-            dresidual = args[0]
-            dresidual = dresidual.reshape(-1, dresidual.shape[-1])
-            if dresidual.stride(-1) != 1:
-                dresidual = dresidual.contiguous()
-            assert dresidual.shape == x.shape
-        else:
-            dresidual = None
-        dx, dnorm_weight, dnorm_bias, dresidual_in, y = _layer_norm_bwd(
-            dy,
-            x,
-            norm_weight,
-            norm_bias,
-            ctx.eps,
-            mean,
-            rstd,
-            dresidual,
-            ctx.has_residual,
-            ctx.is_rms_norm,
-            x_dtype=ctx.x_dtype,
-            recompute_output=True,
-        )
-        dlinear_weight = torch.einsum("bo,bi->oi", dout, y)
-        return (
-            dx.reshape(ctx.x_shape_og),
-            dnorm_weight,
-            dnorm_bias,
-            dlinear_weight,
-            dlinear_bias,
-            dresidual_in.reshape(ctx.x_shape_og) if ctx.has_residual else None,
-            None,
-            None,
-            None,
-            None,
-        )
-
-
-def layer_norm_linear_fn(
-    x,
-    norm_weight,
-    norm_bias,
-    linear_weight,
-    linear_bias,
-    residual=None,
-    eps=1e-6,
-    prenorm=False,
-    residual_in_fp32=False,
-    is_rms_norm=False,
-):
-    return LayerNormLinearFn.apply(
-        x,
-        norm_weight,
-        norm_bias,
-        linear_weight,
-        linear_bias,
-        residual,
-        eps,
-        prenorm,
-        residual_in_fp32,
-        is_rms_norm,
-    )
diff --git a/based/models/mixers/mamba/ops/triton/selective_state_update.py b/based/models/mixers/mamba/ops/triton/selective_state_update.py
deleted file mode 100755
index fa95de7..0000000
--- a/based/models/mixers/mamba/ops/triton/selective_state_update.py
+++ /dev/null
@@ -1,192 +0,0 @@
-# Copyright (c) 2023, Tri Dao.
-
-"""We want triton==2.1.0 for this
-"""
-
-import math
-import torch
-import torch.nn.functional as F
-
-import triton
-import triton.language as tl
-
-from einops import rearrange, repeat
-
-
-@triton.heuristics({"HAS_DT_BIAS": lambda args: args["dt_bias_ptr"] is not None})
-@triton.heuristics({"HAS_D": lambda args: args["D_ptr"] is not None})
-@triton.heuristics({"HAS_Z": lambda args: args["z_ptr"] is not None})
-@triton.heuristics({"BLOCK_SIZE_DSTATE": lambda args: triton.next_power_of_2(args["dstate"])})
-@triton.jit
-def _selective_scan_update_kernel(
-    # Pointers to matrices
-    state_ptr, x_ptr, dt_ptr, dt_bias_ptr, A_ptr, B_ptr, C_ptr, D_ptr, z_ptr, out_ptr,
-    # Matrix dimensions
-    batch, dim, dstate,
-    # Strides
-    stride_state_batch, stride_state_dim, stride_state_dstate,
-    stride_x_batch, stride_x_dim,
-    stride_dt_batch, stride_dt_dim,
-    stride_dt_bias_dim,
-    stride_A_dim, stride_A_dstate,
-    stride_B_batch, stride_B_dstate,
-    stride_C_batch, stride_C_dstate,
-    stride_D_dim,
-    stride_z_batch, stride_z_dim,
-    stride_out_batch, stride_out_dim,
-    # Meta-parameters
-    DT_SOFTPLUS: tl.constexpr,
-    BLOCK_SIZE_M: tl.constexpr,
-    HAS_DT_BIAS: tl.constexpr,
-    HAS_D: tl.constexpr,
-    HAS_Z: tl.constexpr,
-    BLOCK_SIZE_DSTATE: tl.constexpr,
-):
-    pid_m = tl.program_id(axis=0)
-    pid_b = tl.program_id(axis=1)
-    state_ptr += pid_b * stride_state_batch
-    x_ptr += pid_b * stride_x_batch
-    dt_ptr += pid_b * stride_dt_batch
-    B_ptr += pid_b * stride_B_batch
-    C_ptr += pid_b * stride_C_batch
-    if HAS_Z:
-        z_ptr += pid_b * stride_z_batch
-    out_ptr += pid_b * stride_out_batch
-
-    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
-    offs_n = tl.arange(0, BLOCK_SIZE_DSTATE)
-    state_ptrs = state_ptr + (offs_m[:, None] * stride_state_dim + offs_n[None, :] * stride_state_dstate)
-    x_ptrs = x_ptr + offs_m * stride_x_dim
-    dt_ptrs = dt_ptr + offs_m * stride_dt_dim
-    if HAS_DT_BIAS:
-        dt_bias_ptrs = dt_bias_ptr + offs_m * stride_dt_bias_dim
-    A_ptrs = A_ptr + (offs_m[:, None] * stride_A_dim + offs_n[None, :] * stride_A_dstate)
-    B_ptrs = B_ptr + offs_n * stride_B_dstate
-    C_ptrs = C_ptr + offs_n * stride_C_dstate
-    if HAS_D:
-        D_ptrs = D_ptr + offs_m * stride_D_dim
-    if HAS_Z:
-        z_ptrs = z_ptr + offs_m * stride_z_dim
-    out_ptrs = out_ptr + offs_m * stride_out_dim
-
-    state = tl.load(state_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate), other=0.0)
-    x = tl.load(x_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)
-    dt = tl.load(dt_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)
-    if HAS_DT_BIAS:
-        dt += tl.load(dt_bias_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)
-    if DT_SOFTPLUS:
-        dt = tl.log(1.0 + tl.exp(dt))
-    A = tl.load(A_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate), other=0.0).to(tl.float32)
-    dA = tl.exp(A * dt[:, None])
-    B = tl.load(B_ptrs, mask=offs_n < dstate, other=0.0).to(tl.float32)
-    C = tl.load(C_ptrs, mask=offs_n < dstate, other=0.0).to(tl.float32)
-    if HAS_D:
-        D = tl.load(D_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)
-    if HAS_Z:
-        z = tl.load(z_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)
-
-    dB = B[None, :] * dt[:, None]
-    state = state * dA + dB * x[:, None]
-    tl.store(state_ptrs, state, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate))
-    out = tl.sum(state * C[None, :], axis=1)
-    if HAS_D:
-        out += x * D
-    if HAS_Z:
-        out *= z * tl.sigmoid(z)
-    tl.store(out_ptrs, out, mask=offs_m < dim)
-
-
-def selective_state_update(state, x, dt, A, B, C, D=None, z=None, dt_bias=None, dt_softplus=False):
-    """
-    Argument:
-        state: (batch, dim, dstate)
-        x: (batch, dim)
-        dt: (batch, dim)
-        A: (dim, dstate)
-        B: (batch, dstate)
-        C: (batch, dstate)
-        D: (dim,)
-        z: (batch, dim)
-        dt_bias: (dim,)
-    Return:
-        out: (batch, dim)
-    """
-    batch, dim, dstate = state.shape
-    assert x.shape == (batch, dim)
-    assert dt.shape == x.shape
-    assert A.shape == (dim, dstate)
-    assert B.shape == (batch, dstate)
-    assert C.shape == B.shape
-    if D is not None:
-        assert D.shape == (dim,)
-    if z is not None:
-        assert z.shape == x.shape
-    if dt_bias is not None:
-        assert dt_bias.shape == (dim,)
-    out = torch.empty_like(x)
-    grid = lambda META: (triton.cdiv(dim, META['BLOCK_SIZE_M']), batch)
-    z_strides = ((z.stride(0), z.stride(1)) if z is not None else (0, 0))
-    # We don't want autotune since it will overwrite the state
-    # We instead tune by hand.
-    BLOCK_SIZE_M, num_warps = ((32, 4) if dstate <= 16
-                               else ((16, 4) if dstate <= 32 else
-                                     ((8, 4) if dstate <= 64 else
-                                      ((4, 4) if dstate <= 128 else
-                                       ((4, 8))))))
-    with torch.cuda.device(x.device.index):
-        _selective_scan_update_kernel[grid](
-            state, x, dt, dt_bias, A, B, C, D, z, out,
-            batch, dim, dstate,
-            state.stride(0), state.stride(1), state.stride(2),
-            x.stride(0), x.stride(1),
-            dt.stride(0), dt.stride(1),
-            dt_bias.stride(0) if dt_bias is not None else 0,
-            A.stride(0), A.stride(1),
-            B.stride(0), B.stride(1),
-            C.stride(0), C.stride(1),
-            D.stride(0) if D is not None else 0,
-            z_strides[0], z_strides[1],
-            out.stride(0), out.stride(1),
-            dt_softplus,
-            BLOCK_SIZE_M,
-            num_warps=num_warps,
-        )
-    return out
-
-
-def selective_state_update_ref(state, x, dt, A, B, C, D=None, z=None, dt_bias=None, dt_softplus=False):
-    """
-    Argument:
-        state: (batch, dim, dstate)
-        x: (batch, dim)
-        dt: (batch, dim)
-        A: (dim, dstate)
-        B: (batch, dstate)
-        C: (batch, dstate)
-        D: (dim,)
-        z: (batch, dim)
-        dt_bias: (dim,)
-    Return:
-        out: (batch, dim)
-    """
-    batch, dim, dstate = state.shape
-    assert x.shape == (batch, dim)
-    assert dt.shape == x.shape
-    assert A.shape == (dim, dstate)
-    assert B.shape == (batch, dstate)
-    assert C.shape == B.shape
-    if D is not None:
-        assert D.shape == (dim,)
-    if z is not None:
-        assert z.shape == x.shape
-    if dt_bias is not None:
-        assert dt_bias.shape == (dim,)
-        dt = dt + dt_bias
-    dt = F.softplus(dt) if dt_softplus else dt
-    dA = torch.exp(rearrange(dt, "b d -> b d 1") * A)  # (batch, dim, dstate)
-    dB = rearrange(dt, "b d -> b d 1") * rearrange(B, "b n -> b 1 n")  # (batch, dim, dstate)
-    state.copy_(state * dA + dB * rearrange(x, "b d -> b d 1"))  # (batch, dim, dstate
-    out = torch.einsum("bdn,bn->bd", state.to(C.dtype), C)
-    if D is not None:
-        out += (x * D).to(out.dtype)
-    return (out if z is None else out * F.silu(z)).to(x.dtype)
diff --git a/based/models/mixers/mamba/utils/__init__.py b/based/models/mixers/mamba/utils/__init__.py
deleted file mode 100755
index e69de29..0000000
diff --git a/based/models/mixers/mamba/utils/generation.py b/based/models/mixers/mamba/utils/generation.py
deleted file mode 100755
index 75306ba..0000000
--- a/based/models/mixers/mamba/utils/generation.py
+++ /dev/null
@@ -1,374 +0,0 @@
-# Copyright (c) 2023, Albert Gu, Tri Dao.
-import gc
-import time
-from collections import namedtuple
-from dataclasses import dataclass, field
-from functools import partial
-from typing import Callable, Optional, Sequence, Union
-
-import torch
-import torch.nn.functional as F
-from einops import rearrange, repeat
-from torch import Tensor
-from torch.profiler import ProfilerActivity, profile, record_function
-from transformers.generation import GreedySearchDecoderOnlyOutput, SampleDecoderOnlyOutput, TextStreamer
-
-
-@dataclass
-class InferenceParams:
-    """Inference parameters that are passed to the main model in order
-    to efficienly calculate and store the context during inference."""
-
-    max_seqlen: int
-    max_batch_size: int
-    seqlen_offset: int = 0
-    batch_size_offset: int = 0
-    key_value_memory_dict: dict = field(default_factory=dict)
-    lengths_per_sample: Optional[Tensor] = None
-
-    def reset(self, max_seqlen, max_batch_size):
-        self.max_seqlen = max_seqlen
-        self.max_batch_size = max_batch_size
-        self.seqlen_offset = 0
-        if self.lengths_per_sample is not None:
-            self.lengths_per_sample.zero_()
-
-
-# https://github.com/NVIDIA/Megatron-LM/blob/0bb597b42c53355a567aba2a1357cc34b9d99ddd/megatron/text_generation/sampling.py
-# https://github.com/huggingface/transformers/blob/a44985b41cfa2de48a5e1de7f1f93b7483da25d1/src/transformers/generation/logits_process.py#L231
-def modify_logits_for_top_k_filtering(logits, top_k):
-    """Set the logits for none top-k values to -inf. Done in-place."""
-    indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]
-    logits.masked_fill_(indices_to_remove, float("-Inf"))
-
-
-# https://github.com/NVIDIA/Megatron-LM/blob/0bb597b42c53355a567aba2a1357cc34b9d99ddd/megatron/text_generation/sampling.py
-# https://github.com/huggingface/transformers/blob/a44985b41cfa2de48a5e1de7f1f93b7483da25d1/src/transformers/generation/logits_process.py#L170
-def modify_logits_for_top_p_filtering(logits, top_p):
-    """Set the logits for none top-p values to -inf. Done in-place."""
-    if top_p <= 0.0 or top_p >= 1.0:
-        return
-    # First sort and calculate cumulative sum of probabilities.
-    sorted_logits, sorted_indices = torch.sort(logits, descending=False)
-    cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)
-    # Remove tokens with cumulative top_p above the threshold (token with 0 are kept)
-    sorted_indices_to_remove = cumulative_probs <= (1 - top_p)
-    # scatter sorted tensors to original indexing
-    indices_to_remove = sorted_indices_to_remove.scatter(
-        1, sorted_indices, sorted_indices_to_remove
-    )
-    logits.masked_fill_(indices_to_remove, float("-inf"))
-
-
-def modify_logit_for_repetition_penalty(logits, prev_output_tokens, repetition_penalty=1.0):
-    """Apply repetition penalty. See https://arxiv.org/abs/1909.05858
-    logits: (batch_size, vocab_size)
-    prev_output_tokens: (batch_size, seq_len)
-    """
-    if repetition_penalty == 1.0:
-        return logits
-    score = torch.gather(logits, 1, prev_output_tokens)
-    # if score < 0 then repetition penalty has to be multiplied to reduce the previous token probability
-    score = torch.where(score < 0, score * repetition_penalty, score / repetition_penalty)
-    logits.scatter_(1, prev_output_tokens, score)
-    return logits
-
-
-def sample(logits, top_k=1, top_p=0.0, temperature=1.0):
-    """Sample from top-k logits.
-    Arguments:
-        logits: Tensor of shape (batch_size, vocab_size)
-    """
-    if top_k == 1:  # Short-circuit for greedy decoding
-        return logits.argmax(dim=-1)
-    else:
-        if top_p > 0.0:
-            assert top_p <= 1.0, "top-p should be in (0, 1]."
-        if top_k > 0:
-            top_k = min(top_k, logits.size(-1))  # Safety check
-            logits_top, indices = torch.topk(logits, top_k, dim=-1)
-            if temperature != 1.0:
-                logits_top /= temperature
-            modify_logits_for_top_p_filtering(logits_top, top_p)
-            return indices[
-                torch.arange(indices.shape[0], device=indices.device),
-                torch.multinomial(torch.softmax(logits_top, dim=-1), num_samples=1).squeeze(dim=-1),
-            ]
-        else:
-            # Clone so that when we modify for top_p we don't change the original logits
-            logits_top = logits / temperature if temperature != 1.0 else logits.clone()
-            modify_logits_for_top_p_filtering(logits_top, top_p)
-            return torch.multinomial(torch.softmax(logits_top, dim=-1), num_samples=1).squeeze(
-                dim=-1
-            )
-
-
-@torch.inference_mode()
-def decode(
-    input_ids,
-    model,
-    max_length,
-    top_k=1,
-    top_p=0.0,
-    temperature=1.0,
-    repetition_penalty=1.0,
-    eos_token_id=None,
-    teacher_outputs=None,
-    vocab_size=None,
-    cg=False,
-    enable_timing=False,
-    streamer: Optional[TextStreamer] = None,
-    stopping_criteria: any = None,
-    pad_token_id=None,
-    **kwargs
-):
-    """Decoding, either greedy or with top-k or top-p sampling.
-    If top-k = 0, don't limit the number of candidates (pure sampling).
-    Top-k and top-p can be used together. If top_k > 0 and top_p > 0, then top-k is applied first,
-    then top-p.
-    We assume that all sequences in the same batch have the same length.
-
-    Arguments:
-        input_ids: (batch, seq_len)
-        max_length: int
-        teacher_outputs (optional): (batch, seq_len). If provided, instead of sampling from the
-            logits, the next token is taken from the teacher_outputs. Useful for testing.
-    Returns: GreedySearchDecoderOnlyOutput or SampleDecoderOnlyOutput, with the following fields:
-        sequences: (batch, max_length)
-        scores: tuples of (batch, vocab_size)
-    """
-    if streamer is not None:
-        streamer.put(input_ids.cpu())
-
-    batch_size, seqlen_og = input_ids.shape
-    teacher_output_len = teacher_outputs.shape[1] if teacher_outputs is not None else 0
-    if cg:
-        if not hasattr(model, "_decoding_cache"):
-            model._decoding_cache = None
-        model._decoding_cache = update_graph_cache(
-            model,
-            model._decoding_cache,
-            batch_size,
-            seqlen_og,
-            max_length,
-        )
-        inference_params = model._decoding_cache.inference_params
-        inference_params.reset(max_length, batch_size)
-    else:
-        inference_params = InferenceParams(max_seqlen=max_length, max_batch_size=batch_size)
-
-    def get_logits(input_ids, inference_params):
-        decoding = inference_params.seqlen_offset > 0
-        if decoding:
-            position_ids = torch.full(
-                (batch_size, 1),
-                inference_params.seqlen_offset,
-                dtype=torch.long,
-                device=input_ids.device,
-            )
-        else:
-            position_ids = None
-        if not cg or not decoding:
-            logits = model(
-                input_ids,
-                position_ids=position_ids,
-                inference_params=inference_params,
-                num_last_tokens=1,
-            ).logits.squeeze(dim=1)
-        else:
-            logits = model._decoding_cache.run(
-                input_ids, position_ids, inference_params.seqlen_offset
-            ).squeeze(dim=1)
-        return logits[..., :vocab_size] if vocab_size is not None else logits
-
-    def sample_tokens(logits, inference_params):
-        if teacher_outputs is None or teacher_output_len <= inference_params.seqlen_offset:
-            token = sample(logits, top_k=top_k, top_p=top_p, temperature=temperature)
-        else:
-            token = teacher_outputs[:, inference_params.seqlen_offset]
-        # return rearrange(token, "b -> b 1")
-        return token.unsqueeze(1)
-
-    def should_stop(current_token, inference_params):
-        if inference_params.seqlen_offset == 0:
-            return False
-        if eos_token_id is not None and (current_token == eos_token_id).all():
-            return True
-        if inference_params.seqlen_offset >= max_length - 1:
-            return True
-        return False
-
-    start = torch.cuda.Event(enable_timing=enable_timing)
-    end = torch.cuda.Event(enable_timing=enable_timing)
-
-    if enable_timing:
-        start.record()
-    scores, sequences = [], [input_ids]
-    sequences_cat = input_ids
-    while not should_stop(sequences[-1], inference_params):
-        scores.append(get_logits(sequences[-1], inference_params))
-        inference_params.seqlen_offset += sequences[-1].shape[1]
-        if repetition_penalty == 1.0:
-            sampled_tokens = sample_tokens(scores[-1], inference_params)
-        else:
-            logits = modify_logit_for_repetition_penalty(
-                scores[-1].clone(), sequences_cat, repetition_penalty
-            )
-            sampled_tokens = sample_tokens(logits, inference_params)
-            sequences_cat = torch.cat([sequences_cat, sampled_tokens], dim=1)
-        sequences.append(sampled_tokens)
-        if streamer is not None:
-            streamer.put(sampled_tokens.cpu())
-    if streamer is not None:
-        streamer.end()
-    if enable_timing:
-        end.record()
-        torch.cuda.synchronize()
-        print(f"Prompt processing + decoding time: {(start.elapsed_time(end)):.0f}ms")
-    output_cls = GreedySearchDecoderOnlyOutput if top_k == 1 else SampleDecoderOnlyOutput
-    return output_cls(sequences=torch.cat(sequences, dim=1), scores=tuple(scores))
-
-
-class GenerationMixin:
-    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):
-        raise NotImplementedError
-
-    def generate(
-        self,
-        input_ids,
-        max_length,
-        top_k=1,
-        top_p=0.0,
-        temperature=1.0,
-        return_dict_in_generate=False,
-        output_scores=False,
-        **kwargs,
-    ):
-        output = decode(
-            input_ids, self, max_length, top_k=top_k, top_p=top_p, temperature=temperature, **kwargs
-        )
-        if not output_scores:
-            output.scores = None
-        return output if return_dict_in_generate else output.sequences
-
-
-@dataclass
-class DecodingCGCache:
-    max_batch_size: int = 0
-    max_seqlen: int = 0
-    device = None
-    dtype = None
-    callables: dict = field(default_factory=dict)
-    mempool = None
-    inference_params: Optional[InferenceParams] = None
-    run: Optional[Callable] = None
-
-
-@torch.inference_mode()
-def update_graph_cache(
-    model,
-    cache,
-    batch_size,
-    seqlen_og,
-    max_seqlen,
-    decoding_seqlens=(1,),
-    dtype=None,
-    n_warmups=2,
-):
-    if cache is None:
-        cache = DecodingCGCache()
-    param_example = next(iter(model.parameters()))
-    device = param_example.device
-    if dtype is None:
-        dtype = param_example.dtype
-    if (
-        (device, dtype) != (cache.device, cache.dtype)
-        or batch_size > cache.max_batch_size
-        or max_seqlen > cache.max_seqlen
-    ):  # Invalidate the cache
-        cache.callables = {}
-        cache.mempool = None
-        cache.inference_params = None
-        gc.collect()
-        cache.device, cache.dtype = device, dtype
-        cache.max_batch_size, cache.max_seqlen = batch_size, max_seqlen
-        assert hasattr(model, "allocate_inference_cache"), "CUDA graph decoding requires that the model has a method allocate_inference_cache"
-        inf_cache = model.allocate_inference_cache(batch_size, max_seqlen, dtype)
-        lengths_per_sample = torch.full((batch_size,), seqlen_og, dtype=torch.int32, device=device)
-        cache.inference_params = InferenceParams(
-            max_seqlen=max_seqlen,
-            max_batch_size=batch_size,
-            seqlen_offset=seqlen_og,
-            key_value_memory_dict=inf_cache,
-            lengths_per_sample=lengths_per_sample,
-        )
-        cache.mempool = torch.cuda.graphs.graph_pool_handle()
-    for decoding_seqlen in decoding_seqlens:
-        if (batch_size, decoding_seqlen) not in cache.callables:
-            cache.callables[batch_size, decoding_seqlen] = capture_graph(
-                model,
-                cache.inference_params,
-                batch_size,
-                max_seqlen,
-                decoding_seqlen=decoding_seqlen,
-                mempool=cache.mempool,
-                n_warmups=n_warmups,
-            )
-
-    def dispatch(input_ids, position_ids, seqlen):
-        batch_size, decoding_seqlen = input_ids.shape[:2]
-        return cache.callables[batch_size, decoding_seqlen](input_ids, position_ids, seqlen)
-
-    cache.run = dispatch
-    cache.inference_params.seqlen_offset = 0  # Reset so it's not confusing
-    return cache
-
-
-def capture_graph(
-    model, inference_params, batch_size, max_seqlen, decoding_seqlen=1, mempool=None, n_warmups=2
-):
-    device = next(iter(model.parameters())).device
-    input_ids = torch.full((batch_size, decoding_seqlen), 0, dtype=torch.long, device=device)
-    position_ids = torch.full((batch_size, decoding_seqlen), 0, dtype=torch.long, device=device)
-    seqlen_offset_og = inference_params.seqlen_offset
-    inference_params.seqlen_offset = max_seqlen - decoding_seqlen
-    inference_params.lengths_per_sample[:] = inference_params.seqlen_offset
-
-    # Warmup before capture
-    s = torch.cuda.Stream()
-    s.wait_stream(torch.cuda.current_stream())
-    with torch.cuda.stream(s):
-        for _ in range(n_warmups):
-            logits = model(
-                input_ids,
-                position_ids=position_ids,
-                inference_params=inference_params,
-                num_last_tokens=decoding_seqlen,
-            ).logits
-        s.synchronize()
-        # This might be needed for correctness if we run with NCCL_GRAPH_MIXING_SUPPORT=0,
-        # which requires that graph launch and non-captured launch to not overlap (I think,
-        # that's how I interpret the documentation). I'm not sure if this is required.
-        if torch.distributed.is_initialized():
-            torch.distributed.barrier()
-    torch.cuda.current_stream().wait_stream(s)
-    # Captures the graph
-    # To allow capture, automatically sets a side stream as the current stream in the context
-    graph = torch.cuda.CUDAGraph()
-    with torch.cuda.graph(graph, pool=mempool):
-        logits = model(
-            input_ids,
-            position_ids=position_ids,
-            inference_params=inference_params,
-            num_last_tokens=decoding_seqlen,
-        ).logits
-
-    def run(new_input_ids, new_position_ids, seqlen):
-        inference_params.lengths_per_sample[:] = seqlen
-        input_ids.copy_(new_input_ids)
-        position_ids.copy_(new_position_ids)
-        graph.replay()
-        return logits.clone()
-
-    inference_params.seqlen_offset = seqlen_offset_og
-    return run
diff --git a/based/models/mixers/mamba/utils/hf.py b/based/models/mixers/mamba/utils/hf.py
deleted file mode 100755
index 0d7555a..0000000
--- a/based/models/mixers/mamba/utils/hf.py
+++ /dev/null
@@ -1,23 +0,0 @@
-import json
-
-import torch
-
-from transformers.utils import WEIGHTS_NAME, CONFIG_NAME
-from transformers.utils.hub import cached_file
-
-
-def load_config_hf(model_name):
-    resolved_archive_file = cached_file(model_name, CONFIG_NAME, _raise_exceptions_for_missing_entries=False)
-    return json.load(open(resolved_archive_file))
-
-
-def load_state_dict_hf(model_name, device=None, dtype=None):
-    # If not fp32, then we don't want to load directly to the GPU
-    mapped_device = "cpu" if dtype not in [torch.float32, None] else device
-    resolved_archive_file = cached_file(model_name, WEIGHTS_NAME, _raise_exceptions_for_missing_entries=False)
-    return torch.load(resolved_archive_file, map_location=mapped_device)
-    # Convert dtype before moving to GPU to save memory
-    if dtype is not None:
-        state_dict = {k: v.to(dtype=dtype) for k, v in state_dict.items()}
-    state_dict = {k: v.to(device=device) for k, v in state_dict.items()}
-    return state_dict
diff --git a/based/models/mixers/mha.py b/based/models/mixers/mha.py
deleted file mode 100644
index f29c261..0000000
--- a/based/models/mixers/mha.py
+++ /dev/null
@@ -1,1016 +0,0 @@
-# Copyright (c) 2023, Tri Dao.
-
-import math
-from functools import partial
-
-import torch
-import torch.nn as nn
-from einops import rearrange, repeat
-
-from flash_attn.utils.distributed import get_dim_for_local_rank
-
-try:
-    from flash_attn import (
-        flash_attn_kvpacked_func,
-        flash_attn_qkvpacked_func,
-        flash_attn_varlen_kvpacked_func,
-        flash_attn_varlen_qkvpacked_func,
-        flash_attn_with_kvcache,
-    )
-except ImportError:
-    flash_attn_varlen_qkvpacked_func, flash_attn_varlen_kvpacked_func = None, None
-    flash_attn_qkvpacked_func, flash_attn_kvpacked_func = None, None
-    flash_attn_with_kvcache = None
-
-try:
-    from flash_attn.ops.fused_dense import ColumnParallelLinear, FusedDense, RowParallelLinear
-except ImportError:
-    FusedDense, ColumnParallelLinear, RowParallelLinear = None, None, None
-
-try:
-    from flash_attn.layers.rotary import RotaryEmbedding
-except ImportError:
-    RotaryEmbedding = None
-
-
-# From https://github.com/ofirpress/attention_with_linear_biases/blob/4b92f28a005ead2567abe2359f633e73e08f3833/fairseq/models/transformer.py#L742
-def get_alibi_slopes(nheads):
-    def get_slopes_power_of_2(nheads):
-        start = 2 ** (-(2 ** -(math.log2(nheads) - 3)))
-        ratio = start
-        return [start * ratio**i for i in range(nheads)]
-
-    if math.log2(nheads).is_integer():
-        return get_slopes_power_of_2(nheads)
-    else:
-        closest_power_of_2 = 2 ** math.floor(math.log2(nheads))
-        return (
-            get_slopes_power_of_2(closest_power_of_2)
-            + get_alibi_slopes(2 * closest_power_of_2)[0::2][: nheads - closest_power_of_2]
-        )
-
-
-class FlashSelfAttention(nn.Module):
-    """Implement the scaled dot product attention with softmax.
-    Arguments
-    ---------
-        softmax_scale: The temperature to use for the softmax attention.
-                      (default: 1/sqrt(d_keys) where d_keys is computed at
-                      runtime)
-        attention_dropout: The dropout rate to apply to the attention
-                           (default: 0.0)
-    """
-
-    def __init__(
-        self,
-        causal=False,
-        softmax_scale=None,
-        attention_dropout=0.0,
-        window_size=(-1, -1),
-        alibi_slopes=None,
-        deterministic=False,
-    ):
-        super().__init__()
-        assert flash_attn_varlen_qkvpacked_func is not None, "FlashAttention is not installed"
-        assert flash_attn_qkvpacked_func is not None, "FlashAttention is not installed"
-        self.causal = causal
-        self.softmax_scale = softmax_scale
-        self.drop = nn.Dropout(attention_dropout)
-        self.register_buffer("alibi_slopes", alibi_slopes, persistent=False)
-        self.window_size = window_size
-        self.deterministic = deterministic
-
-    def forward(self, qkv, causal=None, cu_seqlens=None, max_seqlen=None):
-        """Implements the multihead softmax attention.
-        Arguments
-        ---------
-            qkv: The tensor containing the query, key, and value.
-                If cu_seqlens is None and max_seqlen is None, then qkv has shape (B, S, 3, H, D).
-                If cu_seqlens is not None and max_seqlen is not None, then qkv has shape
-                (total, 3, H, D), where total is the sum of the sequence lengths in the batch.
-            causal: if passed, will override self.causal
-            cu_seqlens: (batch_size + 1,), dtype torch.int32. The cumulative sequence lengths
-                of the sequences in the batch, used to index into qkv.
-            max_seqlen: int. Maximum sequence length in the batch.
-        Returns:
-        --------
-            out: (total, H, D) if cu_seqlens is not None and max_seqlen is not None,
-                else (B, S, H, D).
-        """
-        assert qkv.dtype in [torch.float16, torch.bfloat16]
-        assert qkv.is_cuda
-        causal = self.causal if causal is None else causal
-        unpadded = cu_seqlens is not None
-        if unpadded:
-            assert cu_seqlens.dtype == torch.int32
-            assert max_seqlen is not None
-            assert isinstance(max_seqlen, int)
-            return flash_attn_varlen_qkvpacked_func(
-                qkv,
-                cu_seqlens,
-                max_seqlen,
-                self.drop.p if self.training else 0.0,
-                softmax_scale=self.softmax_scale,
-                causal=causal,
-                alibi_slopes=self.alibi_slopes,
-                window_size=self.window_size,
-                deterministic=self.deterministic,
-            )
-        else:
-            return flash_attn_qkvpacked_func(
-                qkv,
-                self.drop.p if self.training else 0.0,
-                softmax_scale=self.softmax_scale,
-                causal=causal,
-                alibi_slopes=self.alibi_slopes,
-                window_size=self.window_size,
-                deterministic=self.deterministic,
-            )
-
-
-class FlashCrossAttention(nn.Module):
-    """Implement the scaled dot product attention with softmax.
-    Arguments
-    ---------
-        softmax_scale: The temperature to use for the softmax attention.
-                      (default: 1/sqrt(d_keys) where d_keys is computed at
-                      runtime)
-        attention_dropout: The dropout rate to apply to the attention
-                           (default: 0.0)
-    """
-
-    def __init__(
-        self,
-        causal=False,
-        softmax_scale=None,
-        attention_dropout=0.0,
-        alibi_slopes=None,
-        window_size=(-1, -1),
-        deterministic=False,
-    ):
-        super().__init__()
-        assert flash_attn_varlen_kvpacked_func is not None, "FlashAttention is not installed"
-        assert flash_attn_kvpacked_func is not None, "FlashAttention is not installed"
-        self.causal = causal
-        self.softmax_scale = softmax_scale
-        self.drop = nn.Dropout(attention_dropout)
-        self.register_buffer("alibi_slopes", alibi_slopes, persistent=False)
-        self.window_size = window_size
-        self.deterministic = deterministic
-
-    def forward(
-        self,
-        q,
-        kv,
-        causal=None,
-        cu_seqlens=None,
-        max_seqlen=None,
-        cu_seqlens_k=None,
-        max_seqlen_k=None,
-    ):
-        """Implements the multihead softmax attention.
-        Arguments
-        ---------
-            q: The tensor containing the query. (B, Sq, H, D)
-            kv: The tensor containing the key and value. (B, Sk, 2, H_k, D)
-            causal: if passed, will override self.causal
-            cu_seqlens: (batch_size + 1,), dtype torch.int32. The cumulative sequence lengths
-                of the sequences in the batch, used to index into q.
-            max_seqlen: int. Maximum sequence length in the batch of q.
-            cu_seqlens_k: (batch_size + 1,), dtype torch.int32. The cumulative sequence lengths
-                of the sequences in the batch, used to index into kv.
-            max_seqlen_k: int. Maximum sequence length in the batch of k and v.
-        """
-        assert q.dtype in [torch.float16, torch.bfloat16]
-        assert q.is_cuda and kv.is_cuda
-        causal = self.causal if causal is None else causal
-        unpadded = cu_seqlens is not None
-        if unpadded:
-            assert cu_seqlens.dtype == torch.int32
-            assert max_seqlen is not None
-            assert isinstance(max_seqlen, int)
-            assert cu_seqlens_k is not None
-            assert cu_seqlens_k.dtype == torch.int32
-            assert max_seqlen_k is not None
-            assert isinstance(max_seqlen, int)
-            return flash_attn_varlen_kvpacked_func(
-                q,
-                kv,
-                cu_seqlens,
-                cu_seqlens_k,
-                max_seqlen,
-                max_seqlen_k,
-                self.drop.p if self.training else 0.0,
-                softmax_scale=self.softmax_scale,
-                causal=causal,
-                alibi_slopes=self.alibi_slopes,
-                window_size=self.window_size,
-                deterministic=self.deterministic,
-            )
-        else:
-            batch_size, seqlen_q = q.shape[0], q.shape[1]
-            seqlen_k = kv.shape[1]
-            assert kv.shape[0] == batch_size and kv.shape[4] == q.shape[3]
-            return flash_attn_kvpacked_func(
-                q,
-                kv,
-                self.drop.p if self.training else 0.0,
-                causal=causal,
-                softmax_scale=self.softmax_scale,
-                alibi_slopes=self.alibi_slopes,
-                window_size=self.window_size,
-                deterministic=self.deterministic,
-            )
-
-
-class SelfAttention(nn.Module):
-    """Implement the scaled dot product attention with softmax.
-    Arguments
-    ---------
-        softmax_scale: The temperature to use for the softmax attention.
-                      (default: 1/sqrt(d_keys) where d_keys is computed at
-                      runtime)
-        attention_dropout: The dropout rate to apply to the attention
-                           (default: 0.0)
-    """
-
-    def __init__(self, causal=False, softmax_scale=None, attention_dropout=0.0):
-        super().__init__()
-        self.causal = causal
-        self.softmax_scale = softmax_scale
-        self.drop = nn.Dropout(attention_dropout)
-
-    def forward(self, qkv, causal=None, key_padding_mask=None):
-        """Implements the multihead softmax attention.
-        Arguments
-        ---------
-            qkv: The tensor containing the query, key, and value. (B, S, 3, H, D)
-            causal: if passed, will override self.causal
-            key_padding_mask: boolean mask to apply to the attention weights. True means to keep,
-                False means to mask out. (B, S)
-        """
-        batch_size, seqlen = qkv.shape[0], qkv.shape[1]
-        causal = self.causal if causal is None else causal
-        q, k, v = qkv.unbind(dim=2)
-        softmax_scale = self.softmax_scale or 1.0 / math.sqrt(q.shape[-1])
-        scores = torch.einsum("bthd,bshd->bhts", q, k * softmax_scale)
-        if key_padding_mask is not None:
-            padding_mask = torch.full(
-                (batch_size, seqlen), -10000.0, dtype=scores.dtype, device=scores.device
-            )
-            padding_mask.masked_fill_(key_padding_mask, 0.0)
-            # TD [2022-09-30]: Adding is faster than masked_fill_ (idk why, just better kernel I guess)
-            scores = scores + rearrange(padding_mask, "b s -> b 1 1 s")
-        if causal:
-            # "triu_tril_cuda_template" not implemented for 'BFloat16'
-            # So we have to construct the mask in float
-            causal_mask = torch.triu(
-                torch.full((seqlen, seqlen), -10000.0, device=scores.device), 1
-            )
-            # TD [2022-09-30]: Adding is faster than masked_fill_ (idk why, just better kernel I guess)
-            scores = scores + causal_mask.to(dtype=scores.dtype)
-        attention = torch.softmax(scores, dim=-1, dtype=v.dtype)
-        attention_drop = self.drop(attention)
-        output = torch.einsum("bhts,bshd->bthd", attention_drop, v)
-        return output
-
-
-class CrossAttention(nn.Module):
-    """Implement the scaled dot product attention with softmax.
-    Arguments
-    ---------
-        softmax_scale: The temperature to use for the softmax attention.
-                      (default: 1/sqrt(d_keys) where d_keys is computed at
-                      runtime)
-        attention_dropout: The dropout rate to apply to the attention
-                           (default: 0.0)
-    """
-
-    def __init__(self, causal=False, softmax_scale=None, attention_dropout=0.0):
-        super().__init__()
-        self.causal = causal
-        self.softmax_scale = softmax_scale
-        self.drop = nn.Dropout(attention_dropout)
-
-    def forward(self, q, kv, causal=None, key_padding_mask=None):
-        """Implements the multihead softmax attention.
-        Arguments
-        ---------
-            q: The tensor containing the query. (B, Sq, H, D)
-            kv: The tensor containing the key and value. (B, Sk, 2, H_k, D)
-            causal: if passed, will override self.causal
-            key_padding_mask: boolean mask to apply to the attention weights. True means to keep,
-                False means to mask out. (B, Sk)
-        """
-        batch_size, seqlen_q = q.shape[0], q.shape[1]
-        causal = self.causal if causal is None else causal
-        seqlen_k = kv.shape[1]
-        assert kv.shape[0] == batch_size and kv.shape[4] == q.shape[3]
-        if kv.shape[3] != q.shape[2]:  # MQA/GQA
-            kv = repeat(kv, "... hkv d -> ... (hkv g) d", g=q.shape[2] // kv.shape[3])
-        k, v = kv.unbind(dim=2)
-        softmax_scale = self.softmax_scale or 1.0 / math.sqrt(q.shape[-1])
-        scores = torch.einsum("bthd,bshd->bhts", q, k * softmax_scale)
-        if key_padding_mask is not None:
-            padding_mask = torch.full(
-                (batch_size, seqlen_k), -10000.0, dtype=scores.dtype, device=scores.device
-            )
-            padding_mask.masked_fill_(key_padding_mask, 0.0)
-            # TD [2022-09-30]: Adding is faster than masked_fill_ (idk why, just better kernel I guess)
-            scores = scores + rearrange(padding_mask, "b s -> b 1 1 s")
-        if causal:
-            # causal mask needs to take into account the difference between seqlen_q and seqlen_k
-            row_idx = rearrange(
-                torch.arange(seqlen_q, device=q.device, dtype=torch.long), "s -> s 1"
-            )
-            col_idx = torch.arange(seqlen_k, device=kv.device, dtype=torch.long)
-            sk = (
-                seqlen_k
-                if key_padding_mask is None
-                else rearrange(key_padding_mask.sum(-1), "b -> b 1 1 1")
-            )
-            causal_mask = col_idx > row_idx + sk - seqlen_q
-            scores = scores.masked_fill(causal_mask, -10000.0)
-        attention = torch.softmax(scores, dim=-1, dtype=v.dtype)
-        attention_drop = self.drop(attention)
-        output = torch.einsum("bhts,bshd->bthd", attention_drop, v)
-        return output
-
-
-class LinearResidual(nn.Linear):
-    """Wrap nn.Linear to return the residual as well. For compatibility with FusedDense."""
-
-    def forward(self, input: torch.Tensor) -> torch.Tensor:
-        return super().forward(input), input
-
-
-def _update_kv_cache(kv, inference_params, layer_idx):
-    """kv: (batch_size, seqlen, 2, nheads, head_dim) or (batch_size, 1, 2, nheads, head_dim)"""
-    # Pre-allocate memory for key-values for inference.
-    num_heads, head_dim = kv.shape[-2:]
-    if layer_idx not in inference_params.key_value_memory_dict:
-        kv_cache = torch.empty(
-            inference_params.max_batch_size,
-            inference_params.max_seqlen,
-            2,
-            num_heads,
-            head_dim,
-            dtype=kv.dtype,
-            device=kv.device,
-        )
-        inference_params.key_value_memory_dict[layer_idx] = kv_cache
-    else:
-        kv_cache = inference_params.key_value_memory_dict[layer_idx]
-    # Adjust key and value for inference
-    batch_start = inference_params.batch_size_offset
-    batch_end = batch_start + kv.shape[0]
-    sequence_start = inference_params.seqlen_offset
-    sequence_end = sequence_start + kv.shape[1]
-    assert batch_end <= kv_cache.shape[0]
-    assert sequence_end <= kv_cache.shape[1]
-    assert kv_cache is not None
-    kv_cache[batch_start:batch_end, sequence_start:sequence_end, ...] = kv
-    return kv_cache[batch_start:batch_end, :sequence_end, ...]
-
-
-class MHA(nn.Module):
-    """Multi-head self-attention and cross-attention"""
-
-    def __init__(
-        self,
-        embed_dim,
-        num_heads,
-        num_heads_kv=None,
-        cross_attn=False,
-        qkv_proj_bias=True,
-        out_proj_bias=True,
-        dropout=0.0,
-        softmax_scale=None,
-        causal=False,
-        layer_idx=None,
-        dwconv=False,
-        rotary_emb_dim=0,
-        rotary_emb_base=10000.0,
-        rotary_emb_scale_base=None,
-        rotary_emb_interleaved=False,
-        use_alibi=False,
-        window_size=(-1, -1),
-        fused_bias_fc=False,
-        use_flash_attn=False,
-        return_residual=False,
-        checkpointing=False,
-        device=None,
-        dtype=None,
-    ) -> None:
-        """
-        num_heads_kv: can be used to toggle MQA / GQA. If None, use num_heads.
-        return_residual: whether to return the input x along with the output. This is for
-            performance reason: for post-norm architecture, returning the input allows us
-            to fuse the backward of nn.Linear with the residual connection.
-        """
-        factory_kwargs = {"device": device, "dtype": dtype}
-        super().__init__()
-        self.embed_dim = embed_dim
-        self.cross_attn = cross_attn
-        self.causal = causal
-        self.layer_idx = layer_idx
-        self.dwconv = dwconv
-        self.rotary_emb_dim = rotary_emb_dim
-        self.use_flash_attn = use_flash_attn
-        self.return_residual = return_residual
-        self.checkpointing = checkpointing
-        if use_alibi:
-            assert use_flash_attn, "ALiBi code path requires flash_attn"
-            alibi_slopes = torch.tensor(get_alibi_slopes(num_heads), device=device)
-        else:
-            alibi_slopes = None
-        if window_size != (-1, -1):
-            assert use_flash_attn, "Local (sliding window) attention code path requires flash_attn"
-
-        self.num_heads = num_heads
-        self.num_heads_kv = num_heads_kv if num_heads_kv is not None else num_heads
-        assert (
-            self.num_heads % self.num_heads_kv == 0
-        ), "num_heads must be divisible by num_heads_kv"
-        assert self.embed_dim % num_heads == 0, "embed_dim must be divisible by num_heads"
-        self.head_dim = self.embed_dim // num_heads
-        qkv_dim = self.head_dim * (self.num_heads + 2 * self.num_heads_kv)
-        kv_dim = 2 * self.head_dim * self.num_heads_kv
-
-        if self.rotary_emb_dim > 0:
-            assert not cross_attn, "MHA with rotary embedding does not support cross-attention yet"
-            assert RotaryEmbedding is not None, "rotary_emb is not installed"
-            self.rotary_emb = RotaryEmbedding(
-                self.rotary_emb_dim,
-                base=rotary_emb_base,
-                scale_base=rotary_emb_scale_base,
-                interleaved=rotary_emb_interleaved,
-                device=device,
-            )
-
-        if fused_bias_fc and FusedDense is None:
-            raise ImportError("fused_dense is not installed")
-        linear_cls = nn.Linear if not fused_bias_fc else FusedDense
-        linear_resid_cls = (
-            LinearResidual if not fused_bias_fc else partial(FusedDense, return_residual=True)
-        )
-        wqkv_cls = linear_cls if not self.return_residual else linear_resid_cls
-        inner_attn_cls = (
-            partial(FlashSelfAttention, alibi_slopes=alibi_slopes, window_size=window_size)
-            if use_flash_attn
-            else SelfAttention
-        )
-        inner_cross_attn_cls = (
-            partial(FlashCrossAttention, alibi_slopes=alibi_slopes, window_size=window_size)
-            if use_flash_attn
-            else CrossAttention
-        )
-        if not self.cross_attn:
-            self.Wqkv = wqkv_cls(embed_dim, qkv_dim, bias=qkv_proj_bias, **factory_kwargs)
-        else:
-            self.Wq = linear_cls(embed_dim, embed_dim, bias=qkv_proj_bias, **factory_kwargs)
-            self.Wkv = wqkv_cls(embed_dim, kv_dim, bias=qkv_proj_bias, **factory_kwargs)
-        if self.dwconv:
-            if self.num_heads_kv == self.num_heads:
-                self.dwconv_qkv = nn.Conv1d(
-                    qkv_dim, qkv_dim, kernel_size=3, padding=2, groups=qkv_dim
-                )
-            else:
-                self.dwconv_q = nn.Conv1d(
-                    embed_dim, embed_dim, kernel_size=3, padding=2, groups=embed_dim
-                )
-                self.dwconv_kv = nn.Conv1d(kv_dim, kv_dim, kernel_size=3, padding=2, groups=kv_dim)
-        self.inner_attn = inner_attn_cls(
-            causal=causal,
-            softmax_scale=softmax_scale,
-            attention_dropout=dropout,
-        )
-        self.inner_cross_attn = inner_cross_attn_cls(
-            causal=causal, softmax_scale=softmax_scale, attention_dropout=dropout
-        )
-        self.out_proj = linear_cls(embed_dim, embed_dim, bias=out_proj_bias, **factory_kwargs)
-
-    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None):
-        dtype = self.out_proj.weight.dtype if dtype is None else dtype
-        device = self.out_proj.weight.device
-        return torch.empty(
-            batch_size,
-            max_seqlen,
-            2,
-            self.num_heads_kv,
-            self.head_dim,
-            dtype=dtype,
-            device=device,
-        )
-
-    def _update_kv_cache(self, kv, inference_params):
-        """kv: (batch_size, seqlen, 2, nheads, head_dim) or (batch_size, 1, 2, nheads, head_dim)"""
-        assert not self.dwconv, "Generation does not support dwconv yet"
-        assert self.layer_idx is not None, "Generation requires layer_idx in the constructor"
-        return _update_kv_cache(kv, inference_params, self.layer_idx)
-
-    def _apply_rotary_update_kvcache_attention(self, q, kv, inference_params):
-        """
-        Fast path that combine 3 steps: apply rotary to Q and K, update kv cache, and apply attention.
-        q: (batch_size, seqlen_q, nheads, head_dim)
-        kv: (batch_size, seqlen_k, 2, nheads_kv, head_dim)
-        """
-        assert inference_params is not None and inference_params.seqlen_offset > 0
-        assert self.use_flash_attn
-        if self.rotary_emb_dim > 0:
-            assert self.rotary_emb.scale is None, "This code path does not support xPos"
-            self.rotary_emb._update_cos_sin_cache(
-                inference_params.max_seqlen, device=q.device, dtype=q.dtype
-            )
-            rotary_cos, rotary_sin = self.rotary_emb._cos_cached, self.rotary_emb._sin_cached
-        else:
-            rotary_cos, rotary_sin = None, None
-        batch = q.shape[0]
-        kv_cache = inference_params.key_value_memory_dict[self.layer_idx][:batch]
-        cache_seqlens = (
-            inference_params.lengths_per_sample[:batch]
-            if inference_params.lengths_per_sample is not None
-            else inference_params.seqlen_offset
-        )
-        alibi_slopes = getattr(self.inner_cross_attn, "alibi_slopes", None)
-        context = flash_attn_with_kvcache(
-            q,
-            kv_cache[:, :, 0],
-            kv_cache[:, :, 1],
-            kv[:, :, 0],
-            kv[:, :, 1],
-            rotary_cos=rotary_cos,
-            rotary_sin=rotary_sin,
-            cache_seqlens=cache_seqlens,
-            softmax_scale=self.inner_cross_attn.softmax_scale,
-            causal=self.inner_cross_attn.causal,
-            rotary_interleaved=self.rotary_emb.interleaved if self.rotary_emb_dim > 0 else False,
-            alibi_slopes=alibi_slopes,
-        )
-        return context
-
-    def _update_kvcache_attention(self, q, kv, inference_params):
-        """Write kv to inference_params, then do attention"""
-        if (
-            inference_params.seqlen_offset == 0
-            or flash_attn_with_kvcache is None
-            or not self.use_flash_attn
-        ):
-            # TODO: this only uses seqlen_offset and not lengths_per_sample.
-            kv = self._update_kv_cache(kv, inference_params)
-            return self.inner_cross_attn(q, kv)
-        else:
-            batch = q.shape[0]
-            kv_cache = inference_params.key_value_memory_dict[self.layer_idx][:batch]
-            cache_seqlens = (
-                inference_params.lengths_per_sample[:batch]
-                if inference_params.lengths_per_sample is not None
-                else inference_params.seqlen_offset
-            )
-            alibi_slopes = getattr(self.inner_cross_attn, "alibi_slopes", None)
-            return flash_attn_with_kvcache(
-                q,
-                kv_cache[:, :, 0],
-                kv_cache[:, :, 1],
-                kv[:, :, 0],
-                kv[:, :, 1],
-                cache_seqlens=cache_seqlens,
-                softmax_scale=self.inner_cross_attn.softmax_scale,
-                causal=self.inner_cross_attn.causal,
-                alibi_slopes=alibi_slopes,
-            )
-
-    def forward(
-        self,
-        x,
-        x_kv=None,
-        key_padding_mask=None,
-        cu_seqlens=None,
-        max_seqlen=None,
-        mixer_subset=None,
-        inference_params=None,
-        **kwargs,
-    ):
-        """
-        Arguments:
-            x: (batch, seqlen, hidden_dim) (where hidden_dim = num heads * head dim) if
-                cu_seqlens is None and max_seqlen is None, else (total, hidden_dim) where total
-                is the is the sum of the sequence lengths in the batch.
-            x_kv: (batch, seqlen, hidden_dim), only applicable for cross-attention. If None, use x.
-            cu_seqlens: (batch_size + 1,), dtype torch.int32. The cumulative sequence lengths
-                of the sequences in the batch, used to index into x. Only applicable when using
-                FlashAttention.
-            max_seqlen: int. Maximum sequence length in the batch.
-            key_padding_mask: boolean mask, True means to keep, False means to mask out.
-                (batch, seqlen). Only applicable when not using FlashAttention.
-            mixer_subset: for cross-attention only. If not None, will take a subset of x
-                before applying the query projection. Useful for e.g., ViT where we only care
-                about the CLS token in the last layer.
-            inference_params: for generation. Adapted from Megatron-LM (and Apex)
-            https://github.com/NVIDIA/apex/blob/3ff1a10f72ec07067c4e44759442329804ac5162/apex/transformer/testing/standalone_transformer_lm.py#L470
-        """
-        if cu_seqlens is not None:
-            assert max_seqlen is not None
-            assert key_padding_mask is None
-            assert self.use_flash_attn
-            assert not self.dwconv
-            assert self.rotary_emb_dim == 0
-        if key_padding_mask is not None:
-            assert cu_seqlens is None
-            assert max_seqlen is None
-            assert not self.use_flash_attn
-        if inference_params is not None:
-            assert key_padding_mask is None
-            assert cu_seqlens is None and max_seqlen is None
-            assert not self.dwconv
-
-        kwargs = (
-            {"cu_seqlens": cu_seqlens, "max_seqlen": max_seqlen, **kwargs}
-            if self.use_flash_attn
-            else {"key_padding_mask": key_padding_mask, **kwargs}
-        )
-        seqlen_offset = (
-            0
-            if inference_params is None
-            else (
-                inference_params.lengths_per_sample
-                if inference_params.lengths_per_sample is not None
-                else inference_params.seqlen_offset
-            )
-        )
-        rotary_max_seqlen = inference_params.max_seqlen if inference_params is not None else None
-        batch, seqlen = x.shape[:2]
-        if not self.cross_attn and self.num_heads_kv == self.num_heads:
-            assert x_kv is None and mixer_subset is None
-            if not self.return_residual:
-                qkv = self.Wqkv(x)
-            else:
-                qkv, x = self.Wqkv(x)
-            if self.dwconv:
-                qkv = rearrange(
-                    self.dwconv_qkv(rearrange(qkv, "b s d -> b d s"))[..., :-2], "b d s -> b s d"
-                ).contiguous()
-            qkv = rearrange(qkv, "... (three h d) -> ... three h d", three=3, d=self.head_dim)
-            if (
-                inference_params is None
-                or inference_params.seqlen_offset == 0
-                or (self.rotary_emb_dim == 0 or self.rotary_emb_dim % 16 != 0)
-                or not self.use_flash_attn
-            ):
-                if self.rotary_emb_dim > 0:
-                    qkv = self.rotary_emb(
-                        qkv, seqlen_offset=seqlen_offset, max_seqlen=rotary_max_seqlen
-                    )
-                if inference_params is None:
-                    if not self.checkpointing:
-                        context = self.inner_attn(qkv, **kwargs)
-                    else:
-                        context = torch.utils.checkpoint.checkpoint(self.inner_attn, qkv, **kwargs)
-                else:
-                    context = self._update_kvcache_attention(
-                        qkv[:, :, 0], qkv[:, :, 1:], inference_params
-                    )
-            else:
-                context = self._apply_rotary_update_kvcache_attention(
-                    qkv[:, :, 0], qkv[:, :, 1:], inference_params
-                )
-        else:
-            if self.cross_attn:
-                if not self.return_residual:
-                    q = self.Wq(x if mixer_subset is None else x[:, mixer_subset])
-                    kv = self.Wkv(x_kv if x_kv is not None else x)
-                else:
-                    if x_kv is not None:
-                        kv, x_kv = self.Wkv(x_kv)
-                    else:
-                        kv, x = self.Wkv(x)
-                    q = self.Wq(x if mixer_subset is None else x[:, mixer_subset])
-            else:
-                assert self.num_heads_kv != self.num_heads
-                if not self.return_residual:
-                    qkv = self.Wqkv(x)
-                else:
-                    qkv, x = self.Wqkv(x)
-                q = qkv[..., : self.num_heads * self.head_dim]
-                kv = qkv[..., self.num_heads * self.head_dim :]
-            q = rearrange(q, "... (h d) -> ... h d", d=self.head_dim)
-            kv = rearrange(kv, "... (two hkv d) -> ... two hkv d", two=2, d=self.head_dim)
-            if self.dwconv:
-                q = rearrange(
-                    self.dwconv_q(rearrange(q, "b s d -> b d s"))[..., :-2], "b d s -> b s d"
-                ).contiguous()
-                kv = rearrange(
-                    self.dwconv_kv(rearrange(kv, "b s d -> b d s"))[..., :-2], "b d s -> b s d"
-                ).contiguous()
-            if (
-                inference_params is None
-                or inference_params.seqlen_offset == 0
-                or (self.rotary_emb_dim == 0 or self.rotary_emb_dim % 16 != 0)
-                or not self.use_flash_attn
-            ):
-                if self.rotary_emb_dim > 0:
-                    q, kv = self.rotary_emb(
-                        q, kv, seqlen_offset=seqlen_offset, max_seqlen=rotary_max_seqlen
-                    )
-                if inference_params is None:
-                    if not self.checkpointing:
-                        context = self.inner_cross_attn(q, kv, **kwargs)
-                    else:
-                        context = torch.utils.checkpoint.checkpoint(
-                            self.inner_cross_attn, q, kv, **kwargs
-                        )
-                else:
-                    context = self._update_kvcache_attention(q, kv, inference_params)
-            else:
-                context = self._apply_rotary_update_kvcache_attention(q, kv, inference_params)
-        out = self.out_proj(rearrange(context, "... h d -> ... (h d)"))
-        return out if not self.return_residual else (out, x)
-
-
-class ParallelMHA(nn.Module):
-    """Multi-head self-attention and cross-attention"""
-
-    def __init__(
-        self,
-        embed_dim,
-        num_heads,
-        process_group,
-        num_heads_kv=None,
-        qkv_proj_bias=True,
-        out_proj_bias=True,
-        dropout=0.0,
-        softmax_scale=None,
-        causal=False,
-        layer_idx=None,
-        rotary_emb_dim=0,
-        rotary_emb_base=10000.0,
-        rotary_emb_scale_base=None,
-        rotary_emb_interleaved=False,
-        use_alibi=False,
-        window_size=(-1, -1),
-        use_flash_attn=False,
-        checkpointing=False,
-        sequence_parallel=True,
-        device=None,
-        dtype=None,
-    ) -> None:
-        factory_kwargs = {"device": device, "dtype": dtype}
-        super().__init__()
-        self.embed_dim = embed_dim
-        self.causal = causal
-        self.layer_idx = layer_idx
-        self.rotary_emb_dim = rotary_emb_dim
-        self.use_flash_attn = use_flash_attn
-        self.checkpointing = checkpointing
-        self.process_group = process_group
-        self.world_size = process_group.size()
-        self.local_rank = torch.distributed.get_rank(process_group)
-
-        self.num_heads = num_heads
-        assert self.embed_dim % self.num_heads == 0, "embed_dim must be divisible by num_heads"
-
-        self.num_heads_kv = num_heads_kv if num_heads_kv is not None else num_heads
-        assert (
-            self.num_heads % self.num_heads_kv == 0
-        ), "num_heads must be divisible by num_heads_kv"
-
-        self.num_heads_per_rank = get_dim_for_local_rank(
-            self.num_heads, self.world_size, self.local_rank
-        )
-        self.num_heads_kv_per_rank = get_dim_for_local_rank(
-            self.num_heads_kv, self.world_size, self.local_rank
-        )
-        self.head_dim = self.embed_dim // num_heads
-        qkv_dim = self.head_dim * (self.num_heads + 2 * self.num_heads_kv)
-
-        if use_alibi:
-            assert use_flash_attn, "ALiBi code path requires flash_attn"
-            num_heads_local = math.ceil(self.num_heads / self.world_size)
-            alibi_slopes = torch.tensor(
-                get_alibi_slopes(num_heads)[
-                    self.local_rank * num_heads_local : (self.local_rank + 1) * num_heads_local
-                ],
-                device=device,
-            )
-        else:
-            alibi_slopes = None
-        if window_size != (-1, -1):
-            assert use_flash_attn, "Local (sliding window) attention code path requires flash_attn"
-
-        if self.rotary_emb_dim > 0:
-            assert RotaryEmbedding is not None, "rotary_emb is not installed"
-            self.rotary_emb = RotaryEmbedding(
-                self.rotary_emb_dim,
-                base=rotary_emb_base,
-                scale_base=rotary_emb_scale_base,
-                interleaved=rotary_emb_interleaved,
-                device=device,
-            )
-
-        if ColumnParallelLinear is None or RowParallelLinear is None:
-            raise ImportError("fused_dense is not installed")
-        self.Wqkv = ColumnParallelLinear(
-            embed_dim,
-            qkv_dim,
-            process_group,
-            bias=qkv_proj_bias,
-            sequence_parallel=sequence_parallel,
-            multiple_of=self.head_dim * (self.num_heads // self.num_heads_kv + 2),
-            **factory_kwargs,
-        )
-        inner_attn_cls = (
-            partial(FlashSelfAttention, alibi_slopes=alibi_slopes, window_size=window_size)
-            if use_flash_attn
-            else SelfAttention
-        )
-        inner_cross_attn_cls = (
-            partial(FlashCrossAttention, alibi_slopes=alibi_slopes, window_size=window_size)
-            if use_flash_attn
-            else CrossAttention
-        )
-        self.inner_attn = inner_attn_cls(
-            causal=causal, softmax_scale=softmax_scale, attention_dropout=dropout
-        )
-        self.inner_cross_attn = inner_cross_attn_cls(
-            causal=causal, softmax_scale=softmax_scale, attention_dropout=dropout
-        )
-        self.out_proj = RowParallelLinear(
-            embed_dim,
-            embed_dim,
-            process_group,
-            bias=out_proj_bias,
-            sequence_parallel=sequence_parallel,
-            multiple_of=self.head_dim,
-            **factory_kwargs,
-        )
-
-    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None):
-        dtype = self.out_proj.weight.dtype if dtype is None else dtype
-        device = self.out_proj.weight.device
-        return torch.empty(
-            batch_size,
-            max_seqlen,
-            2,
-            self.num_heads_kv_per_rank,
-            self.head_dim,
-            dtype=dtype,
-            device=device,
-        )
-
-    def _update_kv_cache(self, kv, inference_params):
-        """kv: (batch_size, seqlen, 2, nheads, head_dim) or (batch_size, 1, 2, nheads, head_dim)"""
-        assert self.layer_idx is not None, "Generation requires layer_idx in the constructor"
-        return _update_kv_cache(kv, inference_params, self.layer_idx)
-
-    def _apply_rotary_update_kvcache_attention(self, q, kv, inference_params):
-        """
-        Fast path that combine 3 steps: apply rotary to Q and K, update kv cache, and apply attention.
-        q: (batch_size, seqlen_q, nheads, head_dim)
-        kv: (batch_size, seqlen_k, 2, nheads_kv, head_dim)
-        """
-        assert inference_params is not None and inference_params.seqlen_offset > 0
-        assert self.use_flash_attn
-        if self.rotary_emb_dim > 0:
-            assert self.rotary_emb.scale is None, "This code path does not support xPos"
-            self.rotary_emb._update_cos_sin_cache(
-                inference_params.max_seqlen, device=q.device, dtype=q.dtype
-            )
-            rotary_cos, rotary_sin = self.rotary_emb._cos_cached, self.rotary_emb._sin_cached
-        else:
-            rotary_cos, rotary_sin = None, None
-        batch = q.shape[0]
-        kv_cache = inference_params.key_value_memory_dict[self.layer_idx][:batch]
-        cache_seqlens = (
-            inference_params.lengths_per_sample[:batch]
-            if inference_params.lengths_per_sample is not None
-            else inference_params.seqlen_offset
-        )
-        alibi_slopes = getattr(self.inner_cross_attn, "alibi_slopes", None)
-        context = flash_attn_with_kvcache(
-            q,
-            kv_cache[:, :, 0],
-            kv_cache[:, :, 1],
-            kv[:, :, 0],
-            kv[:, :, 1],
-            rotary_cos=rotary_cos,
-            rotary_sin=rotary_sin,
-            cache_seqlens=cache_seqlens,
-            softmax_scale=self.inner_cross_attn.softmax_scale,
-            causal=self.inner_cross_attn.causal,
-            rotary_interleaved=self.rotary_emb.interleaved if self.rotary_emb_dim > 0 else False,
-            alibi_slopes=alibi_slopes,
-        )
-        return context
-
-    def _update_kvcache_attention(self, q, kv, inference_params):
-        """Write kv to inference_params, then do attention"""
-        if inference_params.seqlen_offset == 0 or not self.use_flash_attn:
-            # TODO: this only uses seqlen_offset and not lengths_per_sample.
-            kv = self._update_kv_cache(kv, inference_params)
-            return self.inner_cross_attn(q, kv)
-        else:
-            batch = q.shape[0]
-            kv_cache = inference_params.key_value_memory_dict[self.layer_idx][:batch]
-            cache_seqlens = (
-                inference_params.lengths_per_sample[:batch]
-                if inference_params.lengths_per_sample is not None
-                else inference_params.seqlen_offset
-            )
-            alibi_slopes = getattr(self.inner_cross_attn, "alibi_slopes", None)
-            context = flash_attn_with_kvcache(
-                q,
-                kv_cache[:, :, 0],
-                kv_cache[:, :, 1],
-                kv[:, :, 0],
-                kv[:, :, 1],
-                cache_seqlens=cache_seqlens,
-                softmax_scale=self.inner_cross_attn.softmax_scale,
-                causal=self.inner_cross_attn.causal,
-                alibi_slopes=alibi_slopes,
-            )
-            return context
-
-    def forward(self, x, seqlen=None, inference_params=None, **kwargs):
-        """
-        Arguments:
-            x: (batch, seqlen, hidden_dim) (where hidden_dim = num heads * head dim) if seqlen=None.
-                If seqlen is not None, x is (batch * seqlen, hidden_dim). This is so that when we
-                split x during sequence parallel, we split the batch * seqlen dimension
-                (in case batch is small).
-        """
-        qkv = self.Wqkv(x)
-        if seqlen is not None:
-            qkv = rearrange(qkv, "(b s) ... -> b s ...", s=seqlen)
-        seqlen_offset = (
-            0
-            if inference_params is None
-            else (
-                inference_params.lengths_per_sample
-                if inference_params.lengths_per_sample is not None
-                else inference_params.seqlen_offset
-            )
-        )
-        rotary_max_seqlen = inference_params.max_seqlen if inference_params is not None else None
-        if self.num_heads_kv == self.num_heads:
-            qkv = rearrange(qkv, "b s (three h d) -> b s three h d", three=3, d=self.head_dim)
-            if (
-                inference_params is None
-                or inference_params.seqlen_offset == 0
-                or (self.rotary_emb_dim == 0 or self.rotary_emb_dim % 16 != 0)
-                or not self.use_flash_attn
-            ):
-                if self.rotary_emb_dim > 0:
-                    qkv = self.rotary_emb(
-                        qkv, seqlen_offset=seqlen_offset, max_seqlen=rotary_max_seqlen
-                    )
-                if inference_params is None:
-                    if not self.checkpointing:
-                        context = self.inner_attn(qkv, **kwargs)
-                    else:
-                        context = torch.utils.checkpoint.checkpoint(self.inner_attn, qkv, **kwargs)
-                else:
-                    context = self._update_kvcache_attention(
-                        qkv[:, :, 0], qkv[:, :, 1:], inference_params
-                    )
-            else:
-                context = self._apply_rotary_update_kvcache_attention(
-                    qkv[:, :, 0], qkv[:, :, 1:], inference_params
-                )
-        else:
-            q = rearrange(
-                qkv[..., : self.num_heads_per_rank * self.head_dim],
-                "... (h d) -> ... h d",
-                d=self.head_dim,
-            )
-            kv = rearrange(
-                qkv[..., self.num_heads_per_rank * self.head_dim :],
-                "... (two hkv d) -> ... two hkv d",
-                two=2,
-                d=self.head_dim,
-            )
-            if (
-                inference_params is None
-                or inference_params.seqlen_offset == 0
-                or (self.rotary_emb_dim == 0 or self.rotary_emb_dim % 16 != 0)
-                or not self.use_flash_attn
-            ):
-                if self.rotary_emb_dim > 0:
-                    q, kv = self.rotary_emb(
-                        q, kv, seqlen_offset=seqlen_offset, max_seqlen=rotary_max_seqlen
-                    )
-                if inference_params is None:
-                    if not self.checkpointing:
-                        context = self.inner_cross_attn(q, kv, **kwargs)
-                    else:
-                        context = torch.utils.checkpoint.checkpoint(
-                            self.inner_cross_attn, q, kv, **kwargs
-                        )
-                else:
-                    context = self._update_kvcache_attention(q, kv, inference_params)
-            else:
-                context = self._apply_rotary_update_kvcache_attention(q, kv, inference_params)
-        context = rearrange(context, "b s h d -> b s (h d)")
-        if seqlen is not None:
-            context = rearrange(context, "b s d -> (b s) d")
-        out = self.out_proj(context)
-        return out
\ No newline at end of file
diff --git a/based/models/mixers/optim.py b/based/models/mixers/optim.py
deleted file mode 100755
index 2373df3..0000000
--- a/based/models/mixers/optim.py
+++ /dev/null
@@ -1,18 +0,0 @@
-
-from torch import nn
-
-class OptimModule(nn.Module):
-    """ Interface for Module that allows registering buffers/parameters with configurable optimizer hyperparameters """
-
-    def register(self, name, tensor, lr=None, wd=0.0):
-        """Register a tensor with a configurable learning rate and 0 weight decay"""
-
-        if lr == 0.0:
-            self.register_buffer(name, tensor)
-        else:
-            self.register_parameter(name, nn.Parameter(tensor))
-
-            optim = {}
-            if lr is not None: optim["lr"] = lr
-            if wd is not None: optim["weight_decay"] = wd
-            setattr(getattr(self, name), "_optim", optim)
\ No newline at end of file
diff --git a/based/models/mixers/slide_attention.py b/based/models/mixers/slide_attention.py
deleted file mode 100644
index 5184f71..0000000
--- a/based/models/mixers/slide_attention.py
+++ /dev/null
@@ -1,601 +0,0 @@
-# Copyright (c) 2023, Tri Dao.
-# Adapted by Simran Arora and Sabri Eyuboglu.
-
-import math
-from functools import partial
-
-import torch
-import torch.nn as nn
-from einops import rearrange, repeat
-
-from flash_attn.utils.distributed import get_dim_for_local_rank
-
-from flash_attn import (
-    flash_attn_kvpacked_func,
-    flash_attn_qkvpacked_func,
-    flash_attn_varlen_kvpacked_func,
-    flash_attn_varlen_qkvpacked_func,
-    flash_attn_with_kvcache,
-    flash_attn_func,
-)
-
-
-import inspect
-_flash_supports_window_size = "window_size" in list(inspect.signature(flash_attn_func).parameters)
-assert _flash_supports_window_size, "flash_attn_func does not support window_size"
-
-
-try:
-    from flash_attn.ops.fused_dense import ColumnParallelLinear, FusedDense, RowParallelLinear
-except ImportError:
-    FusedDense, ColumnParallelLinear, RowParallelLinear = None, None, None
-
-try:
-    from flash_attn.layers.rotary import RotaryEmbedding
-except ImportError:
-    RotaryEmbedding = None
-
-
-class FlashSelfAttention(nn.Module):
-    """Implement the scaled dot product attention with softmax.
-    Arguments
-    ---------
-        softmax_scale: The temperature to use for the softmax attention.
-                      (default: 1/sqrt(d_keys) where d_keys is computed at
-                      runtime)
-        attention_dropout: The dropout rate to apply to the attention
-                           (default: 0.0)
-    """
-
-    def __init__(self, causal=True, softmax_scale=None, attention_dropout=0.0, window_size=None):
-        super().__init__()
-        assert flash_attn_varlen_qkvpacked_func is not None, "FlashAttention is not installed"
-        assert flash_attn_qkvpacked_func is not None, "FlashAttention is not installed"
-        self.causal = causal
-        self.softmax_scale = softmax_scale
-        self.drop = nn.Dropout(attention_dropout)
-        self.window_size = window_size
-
-    def forward(self, qkv, causal=True, cu_seqlens=None, max_seqlen=None, **kwargs):
-        """Implements the multihead softmax attention.
-        Arguments
-        ---------
-            qkv: The tensor containing the query, key, and value.
-                If cu_seqlens is None and max_seqlen is None, then qkv has shape (B, S, 3, H, D).
-                If cu_seqlens is not None and max_seqlen is not None, then qkv has shape
-                (total, 3, H, D), where total is the sum of the sequence lengths in the batch.
-            causal: if passed, will override self.causal
-            cu_seqlens: (batch_size + 1,), dtype torch.int32. The cumulative sequence lengths
-                of the sequences in the batch, used to index into qkv.
-            max_seqlen: int. Maximum sequence length in the batch.
-        Returns:
-        --------
-            out: (total, H, D) if cu_seqlens is not None and max_seqlen is not None,
-                else (B, S, H, D).
-        """
-        in_dtype = qkv.dtype
-        if qkv.dtype not in [torch.float16, torch.bfloat16]:
-            qkv = qkv.to(torch.bfloat16)
-
-        assert qkv.dtype in [torch.float16, torch.bfloat16]
-        assert qkv.is_cuda
-        causal = True # Changed (01152023)
-        unpadded = cu_seqlens is not None
-
-        if self.window_size is not None:
-            if unpadded:
-                assert cu_seqlens.dtype == torch.int32
-                assert max_seqlen is not None
-                assert isinstance(max_seqlen, int)
-                return flash_attn_varlen_qkvpacked_func(
-                    qkv,
-                    cu_seqlens,
-                    max_seqlen,
-                    self.drop.p if self.training else 0.0,
-                    softmax_scale=self.softmax_scale,
-                    causal=causal,
-                    window_size=self.window_size ,    # [i - window_size[0], i + window_size[1]]
-                ).to(in_dtype)
-            else:
-                return flash_attn_qkvpacked_func(
-                    qkv,
-                    self.drop.p if self.training else 0.0,
-                    softmax_scale=self.softmax_scale,
-                    causal=causal,
-                    window_size=self.window_size,  
-                ).to(in_dtype)
-        else:
-            assert 0, print("Using windows")
-            if unpadded:
-                assert cu_seqlens.dtype == torch.int32
-                assert max_seqlen is not None
-                assert isinstance(max_seqlen, int)
-                return flash_attn_varlen_qkvpacked_func(
-                    qkv,
-                    cu_seqlens,
-                    max_seqlen,
-                    self.drop.p if self.training else 0.0,
-                    softmax_scale=self.softmax_scale,
-                    causal=causal,
-                ).to(in_dtype)
-            else:
-                return flash_attn_qkvpacked_func(
-                    qkv,
-                    self.drop.p if self.training else 0.0,
-                    softmax_scale=self.softmax_scale,
-                    causal=causal,
-                ).to(in_dtype)
-
-
-class FlashCrossAttention(nn.Module):
-    """Implement the scaled dot product attention with softmax.
-    Arguments
-    ---------
-        softmax_scale: The temperature to use for the softmax attention.
-                      (default: 1/sqrt(d_keys) where d_keys is computed at
-                      runtime)
-        attention_dropout: The dropout rate to apply to the attention
-                           (default: 0.0)
-    """
-
-    def __init__(self, causal=True, softmax_scale=None, attention_dropout=0.0, window_size=None):
-        super().__init__()
-        assert flash_attn_varlen_kvpacked_func is not None, "FlashAttention is not installed"
-        assert flash_attn_kvpacked_func is not None, "FlashAttention is not installed"
-        self.causal = causal
-        self.softmax_scale = softmax_scale
-        self.drop = nn.Dropout(attention_dropout)
-        self.window_size = window_size
-
-    def forward(
-        self,
-        q,
-        kv,
-        causal=None,
-        cu_seqlens=None,
-        max_seqlen=None,
-        cu_seqlens_k=None,
-        max_seqlen_k=None,
-    ):
-        """Implements the multihead softmax attention.
-        Arguments
-        ---------
-            q: The tensor containing the query. (B, Sq, H, D)
-            kv: The tensor containing the key and value. (B, Sk, 2, H_k, D)
-            causal: if passed, will override self.causal
-            cu_seqlens: (batch_size + 1,), dtype torch.int32. The cumulative sequence lengths
-                of the sequences in the batch, used to index into q.
-            max_seqlen: int. Maximum sequence length in the batch of q.
-            cu_seqlens_k: (batch_size + 1,), dtype torch.int32. The cumulative sequence lengths
-                of the sequences in the batch, used to index into kv.
-            max_seqlen_k: int. Maximum sequence length in the batch of k and v.
-        """
-        in_dtype = q.dtype
-        if q.dtype not in [torch.float16, torch.bfloat16]:
-            q = q.to(torch.bfloat16)
-            kv = kv.to(torch.bfloat16)
-        assert q.dtype in [torch.float16, torch.bfloat16]
-        assert q.is_cuda and kv.is_cuda
-        causal = True #self.causal if causal is None else causal
-        unpadded = cu_seqlens is not None
-        if self.window_size is not None:
-            if unpadded:
-                assert cu_seqlens.dtype == torch.int32
-                assert max_seqlen is not None
-                assert isinstance(max_seqlen, int)
-                assert cu_seqlens_k is not None
-                assert cu_seqlens_k.dtype == torch.int32
-                assert max_seqlen_k is not None
-                assert isinstance(max_seqlen, int)
-                return flash_attn_varlen_kvpacked_func(
-                    q,
-                    kv,
-                    cu_seqlens,
-                    cu_seqlens_k,
-                    max_seqlen,
-                    max_seqlen_k,
-                    self.drop.p if self.training else 0.0,
-                    softmax_scale=self.softmax_scale,
-                    causal=True,
-                    window_size=self.window_size,
-                ).to(in_dtype)
-            else:
-                batch_size, seqlen_q = q.shape[0], q.shape[1]
-                seqlen_k = kv.shape[1]
-                assert kv.shape[0] == batch_size and kv.shape[4] == q.shape[3]
-                return flash_attn_kvpacked_func(
-                    q,
-                    kv,
-                    self.drop.p if self.training else 0.0,
-                    causal=True,
-                    softmax_scale=self.softmax_scale,
-                    window_size=self.window_size,
-                ).to(in_dtype)
-        else:
-            assert 0, print("Using windows")
-            if unpadded:
-                assert cu_seqlens.dtype == torch.int32
-                assert max_seqlen is not None
-                assert isinstance(max_seqlen, int)
-                assert cu_seqlens_k is not None
-                assert cu_seqlens_k.dtype == torch.int32
-                assert max_seqlen_k is not None
-                assert isinstance(max_seqlen, int)
-                return flash_attn_varlen_kvpacked_func(
-                    q,
-                    kv,
-                    cu_seqlens,
-                    cu_seqlens_k,
-                    max_seqlen,
-                    max_seqlen_k,
-                    self.drop.p if self.training else 0.0,
-                    softmax_scale=self.softmax_scale,
-                    causal=True,
-                ).to(in_dtype)
-            else:
-                batch_size, seqlen_q = q.shape[0], q.shape[1]
-                seqlen_k = kv.shape[1]
-                assert kv.shape[0] == batch_size and kv.shape[4] == q.shape[3]
-                return flash_attn_kvpacked_func(
-                    q,
-                    kv,
-                    self.drop.p if self.training else 0.0,
-                    causal=True,
-                    softmax_scale=self.softmax_scale,
-                ).to(in_dtype)
-
-
-class LinearResidual(nn.Linear):
-    """Wrap nn.Linear to return the residual as well. For compatibility with FusedDense."""
-
-    def forward(self, input: torch.Tensor) -> torch.Tensor:
-        return super().forward(input), input
-
-
-def _update_kv_cache(kv, inference_params, layer_idx):
-    """kv: (batch_size, seqlen, 2, nheads, head_dim) or (batch_size, 1, 2, nheads, head_dim)"""
-    # Pre-allocate memory for key-values for inference.
-    num_heads, head_dim = kv.shape[-2:]
-    if layer_idx not in inference_params.key_value_memory_dict:
-        kv_cache = torch.empty(
-            inference_params.max_batch_size,
-            inference_params.max_seqlen,
-            2,
-            num_heads,
-            head_dim,
-            dtype=kv.dtype,
-            device=kv.device,
-        )
-        inference_params.key_value_memory_dict[layer_idx] = kv_cache
-    else:
-        kv_cache = inference_params.key_value_memory_dict[layer_idx]
-    # Adjust key and value for inference
-    batch_start = inference_params.batch_size_offset
-    batch_end = batch_start + kv.shape[0]
-    sequence_start = inference_params.seqlen_offset
-    sequence_end = sequence_start + kv.shape[1]
-    assert batch_end <= (kv_cache.shape[0] if kv_cache is not None else kv_cache.shape[0])
-    assert sequence_end <= (kv_cache.shape[1] if kv_cache is not None else kv_cache.shape[2])
-    assert kv_cache is not None
-    kv_cache[batch_start:batch_end, sequence_start:sequence_end, ...] = kv
-    return kv_cache[batch_start:batch_end, :sequence_end, ...]
-
-
-class SlidingAttention(nn.Module):
-    """Multi-head self-attention and cross-attention"""
-
-    def __init__(
-        self,
-        embed_dim,
-        num_heads,
-        num_heads_kv=None,
-        cross_attn=False,
-        qkv_proj_bias=True,
-        out_proj_bias=True,
-        dropout=0.0,
-        softmax_scale=None,
-        causal=True,
-        layer_idx=None,
-        rotary_emb_dim=0,
-        rotary_emb_base=10000.0,
-        rotary_emb_scale_base=None,
-        rotary_emb_interleaved=False,
-        fused_bias_fc=False,
-        use_flash_attn=True,
-        return_residual=False,
-        checkpointing=False,
-        window_size=128,
-        device=None,
-        dtype=None,
-    ) -> None:
-        """
-        num_heads_kv: can be used to toggle MQA / GQA. If None, use num_heads.
-        return_residual: whether to return the input x along with the output. This is for
-            performance reason: for post-norm architecture, returning the input allows us
-            to fuse the backward of nn.Linear with the residual connection.
-        """
-        # SA hardcode
-        use_flash_attn = True
-        causal = True
-
-        factory_kwargs = {"device": device, "dtype": dtype}
-        super().__init__()
-        self.embed_dim = embed_dim
-        self.cross_attn = cross_attn
-        self.causal = causal
-        self.layer_idx = layer_idx
-        self.rotary_emb_dim = rotary_emb_dim
-        self.use_flash_attn = use_flash_attn
-        self.return_residual = return_residual
-        self.checkpointing = checkpointing
-
-        if window_size is None:
-            assert 0, print("Using windows")
-            self.window = None
-        else:
-            self.window = (window_size//2, 0)
-
-        self.num_heads = num_heads
-        self.num_heads_kv = num_heads_kv if num_heads_kv is not None else num_heads
-        assert (self.num_heads % self.num_heads_kv == 0), "num_heads must be divisible by num_heads_kv"
-        assert self.embed_dim % num_heads == 0, "embed_dim must be divisible by num_heads"
-        self.head_dim = self.embed_dim // num_heads
-        qkv_dim = self.head_dim * (self.num_heads + 2 * self.num_heads_kv)
-        kv_dim = 2 * self.head_dim * self.num_heads_kv
-
-        if self.rotary_emb_dim > 0:
-            assert not cross_attn, "MHA with rotary embedding does not support cross-attention yet"
-            assert RotaryEmbedding is not None, "rotary_emb is not installed"
-            self.rotary_emb = RotaryEmbedding(
-                self.rotary_emb_dim,
-                base=rotary_emb_base,
-                scale_base=rotary_emb_scale_base,
-                interleaved=rotary_emb_interleaved,
-                device=device,
-            )
-
-        if fused_bias_fc and FusedDense is None: raise ImportError("fused_dense is not installed")
-        linear_cls = nn.Linear if not fused_bias_fc else FusedDense
-        linear_resid_cls = (
-            LinearResidual if not fused_bias_fc else partial(FusedDense, return_residual=True)
-        )
-        wqkv_cls = linear_cls if not self.return_residual else linear_resid_cls
-        inner_attn_cls = FlashSelfAttention
-        inner_cross_attn_cls = FlashCrossAttention 
-        self.Wqkv = wqkv_cls(embed_dim, qkv_dim, bias=qkv_proj_bias, **factory_kwargs)
-        self.inner_attn = inner_attn_cls(
-            causal=causal, softmax_scale=softmax_scale, attention_dropout=dropout, window_size=self.window
-        )
-        self.inner_cross_attn = inner_cross_attn_cls(
-            causal=causal, softmax_scale=softmax_scale, attention_dropout=dropout, window_size=self.window
-        )
-        self.out_proj = linear_cls(embed_dim, embed_dim, bias=out_proj_bias, **factory_kwargs)
-
-    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None):
-        dtype = self.out_proj.weight.dtype if dtype is None else dtype
-        device = self.out_proj.weight.device
-        return torch.empty(batch_size,max_seqlen,2,self.num_heads_kv,self.head_dim,dtype=dtype,device=device,)
-
-    def _update_kv_cache(self, kv, inference_params):
-        """kv: (batch_size, seqlen, 2, nheads, head_dim) or (batch_size, 1, 2, nheads, head_dim)"""
-        assert self.layer_idx is not None, "Generation requires layer_idx in the constructor"
-        return _update_kv_cache(kv, inference_params, self.layer_idx)
-
-    def _apply_rotary_update_kvcache_attention(self, q, kv, inference_params):
-        """
-        Fast path that combine 3 steps: apply rotary to Q and K, update kv cache, and apply attention.
-        q: (batch_size, seqlen_q, nheads, head_dim)
-        kv: (batch_size, seqlen_k, 2, nheads_kv, head_dim)
-        """
-        assert inference_params is not None and inference_params.seqlen_offset > 0
-        assert self.use_flash_attn, "This code path only supports FlashAttention"
-        batch = q.shape[0]
-        kv_cache = inference_params.key_value_memory_dict[self.layer_idx][:batch]
-        in_dtype = q.dtype
-        if q.dtype not in [torch.float16, torch.bfloat16]:
-            q = q.to(torch.bfloat16)
-            kv_cache = kv_cache.to(torch.bfloat16)
-            kv = kv.to(torch.bfloat16)
-
-        if self.rotary_emb_dim > 0:
-            assert self.rotary_emb.scale is None, "This code path does not support xPos"
-            self.rotary_emb._update_cos_sin_cache(
-                inference_params.max_seqlen, device=q.device, dtype=q.dtype
-            )
-            rotary_cos, rotary_sin = self.rotary_emb._cos_cached, self.rotary_emb._sin_cached
-        else:
-            rotary_cos, rotary_sin = None, None
-
-        cache_seqlens = (
-            inference_params.lengths_per_sample[:batch]
-            if inference_params.lengths_per_sample is not None
-            else inference_params.seqlen_offset
-        )
-        context = flash_attn_with_kvcache(
-            q,
-            kv_cache[:, :, 0],
-            kv_cache[:, :, 1],
-            kv[:, :, 0],
-            kv[:, :, 1],
-            rotary_cos=rotary_cos,
-            rotary_sin=rotary_sin,
-            cache_seqlens=cache_seqlens,
-            softmax_scale=self.inner_cross_attn.softmax_scale,
-            causal=self.inner_cross_attn.causal,
-            rotary_interleaved=self.rotary_emb.interleaved if self.rotary_emb_dim > 0 else False,
-            window_size=self.window,
-        ).to(in_dtype)
-        return context
-
-    def _update_kvcache_attention(self, q, kv, inference_params):
-        """Write kv to inference_params, then do attention"""
-        if (
-            inference_params.seqlen_offset == 0
-            or flash_attn_with_kvcache is None
-            or not self.use_flash_attn
-        ):
-            # TODO: this only uses seqlen_offset and not lengths_per_sample.
-            kv = self._update_kv_cache(kv, inference_params)
-            return self.inner_cross_attn(q, kv)
-        else:
-            batch = q.shape[0]
-            kv_cache = inference_params.key_value_memory_dict[self.layer_idx][:batch]
-            cache_seqlens = (
-                inference_params.lengths_per_sample[:batch]
-                if inference_params.lengths_per_sample is not None
-                else inference_params.seqlen_offset
-            )
-            in_dtype = q.dtype
-            if q.dtype not in [torch.float16, torch.bfloat16]:
-                q = q.to(torch.float16)
-                kv_cache = kv_cache.to(torch.float16)
-                kv = kv.to(torch.float16)
-            if self.window is None:
-                assert 0, print("Using windows")
-                return flash_attn_with_kvcache(
-                    q,
-                    kv_cache[:, :, 0],
-                    kv_cache[:, :, 1],
-                    kv[:, :, 0],
-                    kv[:, :, 1],
-                    cache_seqlens=cache_seqlens,
-                    softmax_scale=self.inner_cross_attn.softmax_scale,
-                    causal=self.inner_cross_attn.causal,
-                ).to(in_dtype)
-            else:
-                # SA adjusted this
-                # kv_cache[:, inference_params.seqlen_offset-1:inference_params.seqlen_offset, 0] =  kv[:, :, 0]
-                # kv_cache[:, inference_params.seqlen_offset-1:inference_params.seqlen_offset, 1] =  kv[:, :, 1]
-                # kv_cache[:, inference_params.seqlen_offset-self.window[0]:inference_params.seqlen_offset, 0],
-                # kv_cache[:, inference_params.seqlen_offset-self.window[0]:inference_params.seqlen_offset, 1],
-                return flash_attn_with_kvcache(
-                    q,
-                    kv_cache[:, :, 0],
-                    kv_cache[:, :, 1],
-                    kv[:, :, 0],
-                    kv[:, :, 1],
-                    cache_seqlens=cache_seqlens,
-                    softmax_scale=self.inner_cross_attn.softmax_scale,
-                    causal=self.inner_cross_attn.causal,
-                    window_size=self.window,
-                ).to(in_dtype)
-
-    def forward(
-        self,
-        x,
-        x_kv=None,
-        key_padding_mask=None,
-        cu_seqlens=None,
-        max_seqlen=None,
-        mixer_subset=None,
-        inference_params=None,
-        **kwargs,
-    ):
-        """
-        Arguments:
-            x: (batch, seqlen, hidden_dim) (where hidden_dim = num heads * head dim) if
-                cu_seqlens is None and max_seqlen is None, else (total, hidden_dim) where total
-                is the is the sum of the sequence lengths in the batch.
-            x_kv: (batch, seqlen, hidden_dim), only applicable for cross-attention. If None, use x.
-            cu_seqlens: (batch_size + 1,), dtype torch.int32. The cumulative sequence lengths
-                of the sequences in the batch, used to index into x. Only applicable when using
-                FlashAttention.
-            max_seqlen: int. Maximum sequence length in the batch.
-            key_padding_mask: boolean mask, True means to keep, False means to mask out.
-                (batch, seqlen). Only applicable when not using FlashAttention.
-            mixer_subset: for cross-attention only. If not None, will take a subset of x
-                before applying the query projection. Useful for e.g., ViT where we only care
-                about the CLS token in the last layer.
-            inference_params: for generation. Adapted from Megatron-LM (and Apex)
-            https://github.com/NVIDIA/apex/blob/3ff1a10f72ec07067c4e44759442329804ac5162/apex/transformer/testing/standalone_transformer_lm.py#L470
-        """
-
-        if cu_seqlens is not None:
-            assert max_seqlen is not None
-            assert key_padding_mask is None
-            assert self.use_flash_attn
-            assert self.rotary_emb_dim == 0
-        if key_padding_mask is not None:
-            assert cu_seqlens is None
-            assert max_seqlen is None
-            assert not self.use_flash_attn
-        if inference_params is not None:
-            assert key_padding_mask is None
-            assert cu_seqlens is None and max_seqlen is None
-
-        kwargs = (
-            {"cu_seqlens": cu_seqlens, "max_seqlen": max_seqlen, **kwargs}
-            if self.use_flash_attn
-            else {"key_padding_mask": key_padding_mask, **kwargs}
-        )
-        seqlen_offset = (
-            0
-            if inference_params is None
-            else (
-                inference_params.lengths_per_sample
-                if inference_params.lengths_per_sample is not None
-                else inference_params.seqlen_offset
-            )
-        )
-        rotary_max_seqlen = inference_params.max_seqlen if inference_params is not None else None
-        batch, seqlen = x.shape[:2]
-        if not self.cross_attn and self.num_heads_kv == self.num_heads:
-            assert x_kv is None and mixer_subset is None
-            if not self.return_residual: qkv = self.Wqkv(x)
-            else: qkv, x = self.Wqkv(x)
-            qkv = rearrange(qkv, "... (three h d) -> ... three h d", three=3, d=self.head_dim)
-            if (
-                inference_params is None
-                or inference_params.seqlen_offset == 0
-                or (self.rotary_emb_dim == 0 or self.rotary_emb_dim % 16 != 0)
-                or not self.use_flash_attn
-            ):
-                if self.rotary_emb_dim > 0:
-                    qkv = self.rotary_emb(
-                        qkv, seqlen_offset=seqlen_offset, max_seqlen=rotary_max_seqlen
-                    )
-                if inference_params is None:
-                    if not self.checkpointing:
-                        context = self.inner_attn(qkv, **kwargs)
-                    else:
-                        context = torch.utils.checkpoint.checkpoint(self.inner_attn, qkv, **kwargs)
-                else:
-                    context = self._update_kvcache_attention(
-                        qkv[:, :, 0], qkv[:, :, 1:], inference_params
-                    )
-            else:
-                context = self._apply_rotary_update_kvcache_attention(
-                    qkv[:, :, 0], qkv[:, :, 1:], inference_params
-                )
-        else:
-            assert self.num_heads_kv != self.num_heads
-            if not self.return_residual: qkv = self.Wqkv(x)
-            else: qkv, x = self.Wqkv(x)
-            q = qkv[..., : self.num_heads * self.head_dim]
-            kv = qkv[..., self.num_heads * self.head_dim :]
-            q = rearrange(q, "... (h d) -> ... h d", d=self.head_dim)
-            kv = rearrange(kv, "... (two hkv d) -> ... two hkv d", two=2, d=self.head_dim)
-            if (
-                inference_params is None
-                or inference_params.seqlen_offset == 0
-                or (self.rotary_emb_dim == 0 or self.rotary_emb_dim % 16 != 0)
-                or not self.use_flash_attn
-            ):
-                if self.rotary_emb_dim > 0:
-                    q, kv = self.rotary_emb(
-                        q, kv, seqlen_offset=seqlen_offset, max_seqlen=rotary_max_seqlen
-                    )
-                if inference_params is None:
-                    if not self.checkpointing:
-                        context = self.inner_cross_attn(q, kv, **kwargs)
-                    else:
-                        context = torch.utils.checkpoint.checkpoint(
-                            self.inner_cross_attn, q, kv, **kwargs
-                        )
-                else:
-                    context = self._update_kvcache_attention(q, kv, inference_params)
-            else:
-                context = self._apply_rotary_update_kvcache_attention(q, kv, inference_params)
-        out = self.out_proj(rearrange(context, "... h d -> ... (h d)"))
-        return out if not self.return_residual else (out, x)
-
diff --git a/based/models/mlp.py b/based/models/mlp.py
deleted file mode 100755
index 54790ab..0000000
--- a/based/models/mlp.py
+++ /dev/null
@@ -1,216 +0,0 @@
-# Copyright (c) 2023, Tri Dao.
-
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-from torch.distributed import ProcessGroup
-
-
-try:
-    from flash_attn.ops.activations import swiglu
-except ImportError:
-    swiglu = None
-
-try:
-    from flash_attn.ops.fused_dense import ColumnParallelLinear, RowParallelLinear
-except ImportError:
-    ColumnParallelLinear, RowParallelLinear = None, None
-
-try:
-    from flash_attn.ops.fused_dense import FusedMLP, ParallelFusedMLP
-except ImportError:
-    FusedMLP, ParallelFusedMLP = None, None
-
-
-class Mlp(nn.Module):
-    def __init__(
-        self,
-        in_features,
-        hidden_features=None,
-        out_features=None,
-        activation=F.gelu,
-        bias1=True,
-        bias2=True,
-        return_residual=False,
-        device=None,
-        dtype=None,
-    ):
-        factory_kwargs = {"device": device, "dtype": dtype}
-        super().__init__()
-        out_features = out_features if out_features is not None else in_features
-        hidden_features = hidden_features if hidden_features is not None else in_features * 4
-        self.return_residual = return_residual
-        self.fc1 = nn.Linear(in_features, hidden_features, bias=bias1, **factory_kwargs)
-        self.activation = activation
-        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias2, **factory_kwargs)
-
-    def forward(self, x):
-        y = self.fc1(x)
-        y = self.activation(y)
-        y = self.fc2(y)
-        return y if not self.return_residual else (y, x)
-
-
-class ParallelMLP(nn.Module):
-    def __init__(
-        self,
-        in_features,
-        hidden_features=None,
-        out_features=None,
-        activation=F.gelu,
-        process_group: ProcessGroup = None,
-        sequence_parallel=True,
-        bias1=True,
-        bias2=True,
-        device=None,
-        dtype=None,
-    ):
-        factory_kwargs = {"device": device, "dtype": dtype}
-        super().__init__()
-        assert ColumnParallelLinear is not None, "Need to install fused_dense"
-        assert RowParallelLinear is not None, "Need to install fused_dense"
-        out_features = out_features if out_features is not None else in_features
-        hidden_features = hidden_features if hidden_features is not None else in_features * 4
-        self.fc1 = ColumnParallelLinear(
-            in_features,
-            hidden_features,
-            process_group,
-            bias=bias1,
-            sequence_parallel=sequence_parallel,
-            **factory_kwargs,
-        )
-        self.activation = activation
-        self.fc2 = RowParallelLinear(
-            hidden_features,
-            out_features,
-            process_group,
-            bias=bias2,
-            sequence_parallel=sequence_parallel,
-            **factory_kwargs,
-        )
-
-    def forward(self, x):
-        y = self.fc1(x)
-        y = self.activation(y)
-        y = self.fc2(y)
-        return y
-
-
-class GatedMlp(nn.Module):
-    def __init__(
-        self,
-        in_features,
-        hidden_features=None,
-        out_features=None,
-        activation=F.sigmoid,
-        bias1=True,
-        bias2=True,
-        multiple_of=256,
-        return_residual=False,
-        device=None,
-        dtype=None,
-        mlp_type='base',
-        ff_mult=2,
-    ):
-        factory_kwargs = {"device": device, "dtype": dtype}
-        super().__init__()
-        self.mlp_type = mlp_type
-        if self.mlp_type == 'base':
-            out_features = out_features if out_features is not None else in_features
-            hidden_features = (
-                hidden_features if hidden_features is not None else int(8 * in_features / 3)
-            )
-            hidden_features = (hidden_features + multiple_of - 1) // multiple_of * multiple_of
-            self.return_residual = return_residual
-            self.fc1 = nn.Linear(in_features, 2 * hidden_features, bias=bias1, **factory_kwargs)
-            self.activation = activation
-            self.fc2 = nn.Linear(hidden_features, out_features, bias=bias2, **factory_kwargs)
-        else:
-            # print(f"Alternate gated mlp!")
-            # SA: This is the MLP type used in the NeoX Eleuther repo
-            out_features = out_features if out_features is not None else in_features
-            hidden_features = int(2 * in_features * ff_mult / 3)
-            hidden_features = (hidden_features + multiple_of - 1) // multiple_of * multiple_of
-            self.return_residual = return_residual
-            self.activation = activation
-            self.fc1 = nn.Linear(in_features, hidden_features, bias=bias1, **factory_kwargs)
-            self.fc3 = nn.Linear(in_features, hidden_features, bias=bias1, **factory_kwargs)
-            self.fc2 = nn.Linear(hidden_features, in_features, bias=bias2, **factory_kwargs)
-
-
-    def forward(self, x):
-        if self.mlp_type == 'base':
-            y = self.fc1(x)
-            if self.activation == F.sigmoid:  # Special case for GLU
-                y = F.glu(y, dim=-1)
-            elif self.activation == F.silu and swiglu is not None:  # Special case for SwiGLU
-                y, gate = y.chunk(2, dim=-1)
-                y = swiglu(gate, y)
-            else:
-                y, gate = y.chunk(2, dim=-1)
-                y = y * self.activation(gate)
-            y = self.fc2(y)
-        else:
-            w1_out = self.fc1(x)
-            w3_out = self.fc3(x)
-            y = self.fc2(self.activation(w1_out) * w3_out)
-        return y if not self.return_residual else (y, x)
-
-
-class ParallelGatedMlp(nn.Module):
-    """Parallel GatedMlp"""
-
-    def __init__(
-        self,
-        in_features,
-        process_group,
-        hidden_features=None,
-        out_features=None,
-        activation=F.sigmoid,
-        bias1=True,
-        bias2=True,
-        multiple_of=256,
-        sequence_parallel=True,
-        device=None,
-        dtype=None,
-        mlp_type='base'
-    ):
-        factory_kwargs = {"device": device, "dtype": dtype}
-        super().__init__()
-        out_features = out_features if out_features is not None else in_features
-        self.mlp_type = mlp_type
-        if self.mlp_type == 'base':
-            hidden_features = (hidden_features if hidden_features is not None else int(8 * in_features / 3))
-        else:
-            # print(f"{self.mlp_type} gated mlp!")
-            hidden_features = int(2 * in_features * 2 / 3)
-        hidden_features = (hidden_features + multiple_of - 1) // multiple_of * multiple_of
-        if ColumnParallelLinear is None or RowParallelLinear is None:
-            raise ImportError("fused_dense is not installed")
-        self.fc1 = ColumnParallelLinear(
-            in_features,
-            2 * hidden_features,
-            process_group,
-            bias=bias1,
-            sequence_parallel=sequence_parallel,
-            **factory_kwargs,
-        )
-        self.activation = activation
-        self.fc2 = RowParallelLinear(
-            hidden_features,
-            out_features,
-            process_group,
-            bias=bias2,
-            sequence_parallel=sequence_parallel,
-            **factory_kwargs,
-        )
-
-    def forward(self, x):
-        y = self.fc1(x)
-        if self.activation == F.sigmoid:  # Special case for GLU
-            y = F.glu(y, dim=-1)
-        else:
-            y, gate = y.chunk(2, dim=-1)
-            y = y * self.activation(gate)
-        y = self.fc2(y)
-        return y
diff --git a/based/models/modules/__init__.py b/based/models/modules/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/based/models/modules/seq_common.py b/based/models/modules/seq_common.py
deleted file mode 100755
index 4d0469d..0000000
--- a/based/models/modules/seq_common.py
+++ /dev/null
@@ -1,342 +0,0 @@
-import math
-from functools import partial
-from collections import namedtuple
-
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-from torch.nn.modules.utils import _pair
-
-import hydra
-
-from einops import reduce, rearrange
-
-
-def pooling(x, pooling_mode='CLS', key_padding_mask=None, batch_first=True):
-    if pooling_mode not in ['MEAN', 'SUM', 'CLS', 'LAST', 'FLATTEN']:
-        raise NotImplementedError(f'pooling_mode must be MEAN, SUM, CLS, LAST, FLATTEN')
-    if pooling_mode in ['MEAN', 'SUM']:
-        if key_padding_mask is not None:
-            mask = rearrange(~key_padding_mask.bool_matrix,
-                             'b s -> b s 1' if batch_first else 'b s -> s b 1')
-            x = x.masked_fill(mask, 0)
-        s = reduce(x, 'b s ... -> b ...' if batch_first else 's b ... -> b ...', 'sum')
-        if pooling_mode == 'SUM':
-            return s
-        else:
-            if key_padding_mask is None:
-                return s / x.shape[1 if batch_first else 0]
-            else:
-                lengths = rearrange(key_padding_mask._lengths, 'b -> b 1')
-                return s / lengths
-    elif pooling_mode == 'CLS':
-        return x[:, 0] if batch_first else x[0]
-    elif pooling_mode == 'LAST':
-        if key_padding_mask is None:
-            return x[:, -1] if batch_first else x[-1]
-        else:
-            lengths = key_padding_mask._lengths
-            if batch_first:
-                batch_size = x.shape[0]
-                return x[torch.arange(batch_size, device=x.device), lengths - 1]
-            else:
-                batch_size = x.shape[1]
-                return x[lengths - 1, torch.arange(batch_size, device=x.device)]
-    elif pooling_mode == 'FLATTEN':
-        return rearrange(x, 'b ... -> b (...)' if batch_first else 's b ... -> b (s ...)')
-
-
-class ClassificationHeadLinear(nn.Module):
-    """Head for sentence-level classification tasks."""
-
-    def __init__(self, d_model, num_classes, pooling_mode='MEAN',
-                 batch_first=False, **kwargs):
-        super().__init__()
-        assert pooling_mode in ['MEAN', 'SUM', 'CLS', 'LAST', 'FLATTEN'], 'pooling_mode not supported'
-        self.pooling_mode = pooling_mode
-        self.batch_first = batch_first
-        self.out_proj = nn.Linear(d_model, num_classes)
-
-    def forward(self, hidden_states, key_padding_mask=None, **kwargs):
-        """
-            hidden_states: (B, S, D) if batch_first else (S, B, D)
-        """
-        hidden_states = pooling(hidden_states, pooling_mode=self.pooling_mode,
-                                key_padding_mask=key_padding_mask, batch_first=self.batch_first)
-        hidden_states = self.out_proj(hidden_states)
-        return hidden_states
-
-
-# Adapted from https://github.com/huggingface/transformers/blob/master/src/transformers/models/reformer/modeling_reformer.py
-class ClassificationHead(nn.Module):
-    """Head for sentence-level classification tasks."""
-
-    def __init__(self, d_model, d_inner, num_classes, dropout=0.0, pooling_mode='MEAN',
-                 batch_first=False):
-        super().__init__()
-        assert pooling_mode in ['MEAN', 'SUM', 'CLS', 'LAST', 'FLATTEN'], 'pooling_mode not supported'
-        self.pooling_mode = pooling_mode
-        self.batch_first = batch_first
-        self.dense = nn.Linear(d_model, d_inner)
-        self.dropout = nn.Dropout(dropout)
-        self.out_proj = nn.Linear(d_inner, num_classes)
-
-    def forward(self, hidden_states, key_padding_mask=None, **kwargs):
-        """
-            hidden_states: (B, S, D) if batch_first else (S, B, D)
-        """
-        hidden_states = pooling(hidden_states, pooling_mode=self.pooling_mode,
-                                key_padding_mask=key_padding_mask, batch_first=self.batch_first)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.dense(hidden_states)
-        # Huggingface uses tanh instead of relu
-        hidden_states = torch.relu(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.out_proj(hidden_states)
-        return hidden_states
-
-
-class ClassificationHeadDual(nn.Module):
-    """Head for sentence-level classification tasks."""
-
-    def __init__(self, d_model, d_inner, num_classes, dropout=0.0, pooling_mode='MEAN',
-                 batch_first=False, interaction='NLI'):
-        super().__init__()
-        assert pooling_mode in ['MEAN', 'SUM', 'CLS'], 'pooling_mode not supported'
-        assert interaction in [None, 'NLI'], 'interaction not supported'
-        self.pooling_mode = pooling_mode
-        self.batch_first = batch_first
-        self.interaction = interaction
-        self.dense = nn.Linear(d_model * (4 if self.interaction == 'NLI' else 2), d_inner)
-        self.dropout = nn.Dropout(dropout)
-        self.out_proj = nn.Linear(d_inner, num_classes)
-
-    def forward(self, hidden_states1, hidden_states2,
-                key_padding_mask1=None, key_padding_mask2=None, **kwargs):
-        """
-            hidden_states: (B, S, D) if batch_first else (S, B, D)
-        """
-        x1 = pooling(hidden_states1, pooling_mode=self.pooling_mode,
-                     key_padding_mask=key_padding_mask1, batch_first=self.batch_first)
-        x2 = pooling(hidden_states2, pooling_mode=self.pooling_mode,
-                     key_padding_mask=key_padding_mask2, batch_first=self.batch_first)
-        hidden_states = (torch.cat([x1, x2, x1 * x2, x1 - x2], dim=-1) if self.interaction == 'NLI'
-                         else torch.cat([x1, x2], dim=-1))
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.dense(hidden_states)
-        # Huggingface uses tanh instead of relu
-        hidden_states = torch.relu(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.out_proj(hidden_states)
-        return hidden_states
-
-
-class LMHead(nn.Module):
-
-    def __init__(self, d_model, num_classes, batch_first=True, bias=True):
-        super().__init__()
-        self.lm_head = nn.Linear(d_model, num_classes, bias=bias)
-
-    def forward(self, hidden_states, **kwargs):
-        """
-            hidden_states: (B, S, D) if batch_first else (S, B, D)
-        """
-        CausalLMOutput = namedtuple('CausalLMOutput', ['logits'])
-        return CausalLMOutput(self.lm_head(hidden_states))
-
-
-def sinusoidal_init_(tensor):
-    """
-        tensor: (max_len, d_model)
-    """
-    max_len, d_model = tensor.shape
-    position = rearrange(torch.arange(0.0, max_len), 's -> s 1')
-    div_term = torch.exp(-math.log(10000.0) * torch.arange(0.0, d_model, 2.0) / d_model)
-    tensor[:, 0::2] = torch.sin(position * div_term)
-    tensor[:, 1::2] = torch.cos(position * div_term)
-    return tensor
-
-
-# Adapted from https://github.com/pytorch/examples/blob/master/word_language_model/model.py
-class PositionalEncoding(nn.Module):
-    r"""Inject some information about the relative or absolute position of the tokens
-        in the sequence. The positional encodings have the same dimension as
-        the embeddings, so that the two can be summed. Here, we use sine and cosine
-        functions of different frequencies.
-    .. math::
-        \text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))
-        \text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))
-        \text{where pos is the word position and i is the embed idx)
-    Args:
-        d_model: the embed dim (required).
-        dropout: the dropout value (default=0.1).
-        max_len: the max. length of the incoming sequence (default=5000).
-    Examples:
-        >>> pos_encoder = PositionalEncoding(d_model)
-    """
-
-    def __init__(self, d_model, dropout=0.1, max_len=5000, batch_first=False, initializer=None):
-        super().__init__()
-        self.batch_first = batch_first
-        self.dropout = nn.Dropout(p=dropout)
-        pe = torch.empty(max_len, d_model)
-        if initializer is None:
-            sinusoidal_init_(pe)
-            pe = rearrange(pe, 's d -> 1 s d' if self.batch_first else 's d -> s 1 d')
-            self.register_buffer('pe', pe)
-        else:
-            hydra.utils.call(initializer, pe)
-            pe = rearrange(pe, 's d -> 1 s d' if self.batch_first else 's d -> s 1 d')
-            self.pe = nn.Parameter(pe)
-
-    def forward(self, x):
-        r"""Inputs of forward function
-        Args:
-            x: the sequence fed to the positional encoder model (required).
-        Shape:
-            x: [sequence length, batch size, embed dim] if not batch_first else [B, S, D]
-            output: [sequence length, batch size, embed dim] if not batch_first else [B, S, D]
-        Examples:
-            >>> output = pos_encoder(x)
-        """
-        x = x + (self.pe[:, :x.size(1)] if self.batch_first else self.pe[:x.size(0)])
-        return self.dropout(x)
-
-
-# Adapted from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/mlp.py
-class Mlp(nn.Module):
-    """ MLP as used in Vision Transformer, MLP-Mixer and related networks
-    """
-    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU,
-                 act_fn=None, drop=0., device=None, dtype=None):
-        """TD [2021-10-27] act_fn takes precedence over act_layer if set.
-        This is to support Pytorch 1.10 Transformer interface that construct the activation
-        *function*, not the activation *layer*.
-        """
-        factory_kwargs = {'device': device, 'dtype': dtype}
-        super().__init__()
-        out_features = out_features or in_features
-        hidden_features = hidden_features or in_features
-        drop_probs = _pair(drop)
-        self.fc1 = nn.Linear(in_features, hidden_features, **factory_kwargs)
-        self.act = act_layer() if act_fn is None else act_fn
-        self.drop1 = nn.Dropout(drop_probs[0])
-        self.fc2 = nn.Linear(hidden_features, out_features, **factory_kwargs)
-        self.drop2 = nn.Dropout(drop_probs[1])
-
-    def forward(self, x):
-        x = self.fc1(x)
-        x = self.act(x)
-        x = self.drop1(x)
-        x = self.fc2(x)
-        x = self.drop2(x)
-        return x
-
-
-class MlpBig(nn.Module):
-    """ MLP as used in Vision Transformer, MLP-Mixer and related networks
-    """
-    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU,
-                 act_fn=None, drop=0., device=None, dtype=None):
-        """Copied from Mlp above. If num_layers > 2, add more Mlp layers, doubling each time.
-        """
-        factory_kwargs = {'device': device, 'dtype': dtype}
-        super().__init__()
-        out_features = out_features or in_features
-        hidden_features = hidden_features or in_features
-        cur_hidden_features = hidden_features
-        layers = []
-        for _ in range(4):
-            layers.append(nn.Linear(in_features, cur_hidden_features, **factory_kwargs))
-            layers.append(act_layer())
-            layers.append(nn.Dropout(drop))
-            in_features = cur_hidden_features
-            cur_hidden_features *= 2
-        layers.append(nn.Linear(in_features, out_features, **factory_kwargs))
-        layers.append(nn.Dropout(drop))
-        self.fwd = nn.Sequential(*layers)
-
-    def forward(self, x):
-        return self.fwd(x)
-
-class GluMlp(nn.Module):
-    """ MLP w/ GLU style gating
-    See: https://arxiv.org/abs/1612.08083, https://arxiv.org/abs/2002.05202
-    """
-    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.Sigmoid, drop=0.):
-        super().__init__()
-        out_features = out_features or in_features
-        hidden_features = hidden_features or in_features
-        assert hidden_features % 2 == 0
-        self.fc1 = nn.Linear(in_features, hidden_features)
-        self.act = act_layer()
-        self.fc2 = nn.Linear(hidden_features // 2, out_features)
-        self.drop = nn.Dropout(drop)
-
-    def init_weights(self):
-        # override init of fc1 w/ gate portion set to weight near zero, bias=1
-        fc1_mid = self.fc1.bias.shape[0] // 2
-        nn.init.ones_(self.fc1.bias[fc1_mid:])
-        nn.init.normal_(self.fc1.weight[fc1_mid:], std=1e-6)
-
-    def forward(self, x):
-        x = self.fc1(x)
-        x, gates = x.chunk(2, dim=-1)
-        x = x * self.act(gates)
-        x = self.drop(x)
-        x = self.fc2(x)
-        x = self.drop(x)
-        return x
-
-
-class GatedMlp(nn.Module):
-    """ MLP as used in gMLP
-    """
-    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU,
-                 gate_layer=None, drop=0.):
-        super().__init__()
-        out_features = out_features or in_features
-        hidden_features = hidden_features or in_features
-        self.fc1 = nn.Linear(in_features, hidden_features)
-        self.act = act_layer()
-        if gate_layer is not None:
-            assert hidden_features % 2 == 0
-            self.gate = gate_layer(hidden_features)
-            hidden_features = hidden_features // 2  # FIXME base reduction on gate property?
-        else:
-            self.gate = nn.Identity()
-        self.fc2 = nn.Linear(hidden_features, out_features)
-        self.drop = nn.Dropout(drop)
-
-    def forward(self, x):
-        x = self.fc1(x)
-        x = self.act(x)
-        x = self.drop(x)
-        x = self.gate(x)
-        x = self.fc2(x)
-        x = self.drop(x)
-        return x
-
-
-class ConvMlp(nn.Module):
-    """ MLP using 1x1 convs that keeps spatial dims
-    """
-    def __init__(
-            self, in_features, hidden_features=None, out_features=None, act_layer=nn.ReLU, norm_layer=None, drop=0.):
-        super().__init__()
-        out_features = out_features or in_features
-        hidden_features = hidden_features or in_features
-        self.fc1 = nn.Conv2d(in_features, hidden_features, kernel_size=1, bias=True)
-        self.norm = norm_layer(hidden_features) if norm_layer else nn.Identity()
-        self.act = act_layer()
-        self.fc2 = nn.Conv2d(hidden_features, out_features, kernel_size=1, bias=True)
-        self.drop = nn.Dropout(drop)
-
-    def forward(self, x):
-        x = self.fc1(x)
-        x = self.norm(x)
-        x = self.act(x)
-        x = self.drop(x)
-        x = self.fc2(x)
-        return x
-
diff --git a/based/models/transformer/__init__.py b/based/models/transformer/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/based/models/transformer/gpt.py b/based/models/transformer/gpt.py
deleted file mode 100644
index 17aa72d..0000000
--- a/based/models/transformer/gpt.py
+++ /dev/null
@@ -1,1054 +0,0 @@
-# Copyright (c) 2024, Tri Dao.
-
-import logging
-import math
-import re
-from collections import OrderedDict, namedtuple
-from collections.abc import Sequence
-from functools import partial
-from typing import Dict, List
-
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-from einops import rearrange
-from transformers import GPT2Config
-
-from flash_attn.models.bigcode import remap_state_dict_hf_bigcode
-from flash_attn.models.falcon import remap_state_dict_hf_falcon
-from flash_attn.models.gpt_neox import remap_state_dict_hf_gpt_neox
-from flash_attn.models.gptj import remap_state_dict_hf_gptj
-from flash_attn.models.llama import remap_state_dict_hf_llama
-from flash_attn.models.opt import remap_state_dict_hf_opt
-from flash_attn.modules.block import Block, ParallelBlock
-from flash_attn.modules.embedding import GPT2Embeddings, ParallelGPT2Embeddings
-from flash_attn.modules.mha import MHA, ParallelMHA
-from flash_attn.modules.mlp import (
-    FusedMLP,
-    GatedMlp,
-    Mlp,
-    ParallelFusedMLP,
-    ParallelGatedMlp,
-    ParallelMLP,
-)
-from flash_attn.ops.activations import sqrelu_fwd
-from flash_attn.utils.distributed import (
-    all_gather,
-    all_gather_raw,
-    get_dim_for_local_rank,
-    sync_shared_params,
-)
-from based.generation import GenerationMixin, NaiveGenerationMixin
-from flash_attn.utils.pretrained import state_dict_from_pretrained
-
-try:
-    from flash_attn.ops.fused_dense import ColumnParallelLinear
-except ImportError:
-    ColumnParallelLinear = None
-
-try:
-    from flash_attn.ops.triton.mlp import FusedDenseSqreluDense
-except ImportError:
-    FusedDenseSqreluDense = None
-
-try:
-    from flash_attn.ops.triton.layer_norm import layer_norm_fn, RMSNorm
-except ImportError:
-    layer_norm_fn, RMSNorm = None, None
-
-logger = logging.getLogger(__name__)
-
-
-def create_mixer_cls(config, layer_idx=None, process_group=None, device=None, dtype=None):
-    factory_kwargs = {"device": device, "dtype": dtype}
-    head_dim = getattr(config, "head_dim", config.hidden_size // config.num_attention_heads)
-    attn_scale_power = 0.5 if not getattr(config, "mup_scale_qk_dot_by_d", False) else 1.0
-    softmax_scale = 1.0 if not config.scale_attn_weights else (head_dim ** (-attn_scale_power))
-    softmax_scale *= getattr(config, "mup_attn_multiplier", 1.0)
-    if config.scale_attn_by_inverse_layer_idx:
-        assert layer_idx is not None
-        softmax_scale /= float(layer_idx + 1)
-    dwconv = getattr(config, "attn_dwconv", False)
-    if dwconv:
-        assert process_group is None, "TensorParallel MHA does not support dwconv yet"
-    qkv_proj_bias = getattr(config, "qkv_proj_bias", True)
-    out_proj_bias = getattr(config, "out_proj_bias", True)
-    rotary_emb_dim = int(getattr(config, "rotary_emb_fraction", 0.0) * head_dim)
-    rotary_emb_base = getattr(config, "rotary_emb_base", 10000.0)
-    rotary_emb_scale_base = getattr(config, "rotary_emb_scale_base", None)
-    rotary_emb_interleaved = getattr(config, "rotary_emb_interleaved", False)
-    use_alibi = getattr(config, "use_alibi", False)
-    window_size = getattr(config, "window_size", (-1, -1))
-    use_flash_attn = getattr(config, "use_flash_attn", False)
-    fused_bias_fc = getattr(config, "fused_bias_fc", False)
-    if not fused_bias_fc:
-        assert process_group is None, "TensorParallel MHA requires fused_bias_fc"
-    mha_cls = MHA if process_group is None else ParallelMHA
-    serial_kwargs = (
-        {"fused_bias_fc": fused_bias_fc, "dwconv": dwconv} if process_group is None else {}
-    )
-    parallel_kwargs = (
-        {
-            "process_group": process_group,
-            "sequence_parallel": getattr(config, "sequence_parallel", True),
-        }
-        if process_group is not None
-        else {}
-    )
-    num_heads_kv = getattr(config, "n_head_kv", None)
-    mixer_cls = partial(
-        mha_cls,
-        num_heads=config.num_attention_heads,
-        num_heads_kv=num_heads_kv,
-        qkv_proj_bias=qkv_proj_bias,
-        out_proj_bias=out_proj_bias,
-        dropout=config.attn_pdrop,
-        softmax_scale=softmax_scale,
-        causal=True,
-        layer_idx=layer_idx,
-        rotary_emb_dim=rotary_emb_dim,
-        rotary_emb_base=rotary_emb_base,
-        rotary_emb_scale_base=rotary_emb_scale_base,
-        rotary_emb_interleaved=rotary_emb_interleaved,
-        use_alibi=use_alibi,
-        window_size=window_size,
-        use_flash_attn=use_flash_attn,
-        **serial_kwargs,
-        **parallel_kwargs,
-        **factory_kwargs,
-    )
-    return mixer_cls
-
-
-def create_mlp_cls(config, layer_idx=None, process_group=None, device=None, dtype=None, multiple_of=256):
-    factory_kwargs = {"device": device, "dtype": dtype}
-    mlp_fc1_bias = getattr(config, "mlp_fc1_bias", True)
-    mlp_fc2_bias = getattr(config, "mlp_fc2_bias", True)
-    fused_mlp = getattr(config, "fused_mlp", False)
-    if fused_mlp:
-        assert config.activation_function in [
-            "gelu_new",
-            "gelu_fast",
-            "gelu_approx",
-            "gelu_pytorch_tanh",
-            "relu",
-            "sqrelu",
-        ]
-    fused_dense_sqrelu_dense = getattr(config, "fused_dense_sqrelu_dense", False)
-    if fused_dense_sqrelu_dense:
-        assert config.activation_function == "sqrelu", (
-            "fused_dense_sqrelu_dense only " "supports approximate activation_function sqrelu"
-        )
-    assert not (fused_dense_sqrelu_dense and fused_mlp)
-    if not fused_mlp and not fused_dense_sqrelu_dense:
-        assert config.activation_function in [
-            "gelu",
-            "gelu_new",
-            "gelu_fast",
-            "gelu_approx",
-            "gelu_pytorch_tanh",
-            "relu",
-            "sqrelu",
-            "glu",
-            "swiglu",
-            "geglu",
-        ]
-        if config.activation_function in ["glu", "swiglu", "geglu"]:
-            activation = (
-                F.sigmoid
-                if config.activation_function == "glu"
-                else (F.silu if config.activation_function == "swiglu" else F.gelu)
-            )
-            mlp_cls = GatedMlp if process_group is None else ParallelGatedMlp
-            parallel_kwargs = (
-                {
-                    "process_group": process_group,
-                    "sequence_parallel": getattr(config, "sequence_parallel", True),
-                }
-                if process_group is not None
-                else {}
-            )
-
-            # SE (02-18): Need to replace this with 256, 
-            # mlp_multiple_of = getattr(config, "mlp_multiple_of", 128)
-            mlp_cls = partial(
-                mlp_cls,
-                hidden_features=config.n_inner,
-                activation=activation,
-                bias1=mlp_fc1_bias,
-                bias2=mlp_fc2_bias,
-                multiple_of=multiple_of,
-                **parallel_kwargs,
-                **factory_kwargs,
-            )
-        else:
-            if config.activation_function == "relu":
-                activation = partial(F.relu, inplace=True)
-            elif config.activation_function == "sqrelu":
-                activation = sqrelu_fwd
-            else:
-                approximate = (
-                    "tanh"
-                    if config.activation_function
-                    in ["gelu_new", "gelu_fast", "gelu_approx", "gelu_pytorch_tanh"]
-                    else "none"
-                )
-                activation = partial(F.gelu, approximate=approximate)
-            mlp_cls = Mlp if process_group is None else ParallelMLP
-            parallel_kwargs = (
-                {
-                    "process_group": process_group,
-                    "sequence_parallel": getattr(config, "sequence_parallel", True),
-                }
-                if process_group is not None
-                else {}
-            )
-            mlp_cls = partial(
-                mlp_cls,
-                hidden_features=config.n_inner,
-                activation=activation,
-                bias1=mlp_fc1_bias,
-                bias2=mlp_fc2_bias,
-                **parallel_kwargs,
-                **factory_kwargs,
-            )
-    else:
-        mlp_checkpoint_lvl = getattr(config, "mlp_checkpoint_lvl", 0)
-        # mlp_checkpoint_lvl could be a list, which contains the checkpoint_lvl for each layer
-        if isinstance(mlp_checkpoint_lvl, Sequence):
-            assert layer_idx is not None
-            mlp_checkpoint_lvl = mlp_checkpoint_lvl[layer_idx]
-        if fused_mlp:
-            if FusedMLP is None:
-                raise ImportError("fused_dense is not installed")
-            activation = (
-                "gelu_approx"
-                if config.activation_function
-                in ["gelu_new", "gelu_fast", "gelu_approx", "gelu_pytorch_tanh"]
-                else config.activation_function
-            )
-            mlp_cls = FusedMLP if process_group is None else ParallelFusedMLP
-            parallel_kwargs = (
-                {
-                    "process_group": process_group,
-                    "sequence_parallel": getattr(config, "sequence_parallel", True),
-                }
-                if process_group is not None
-                else {}
-            )
-            mlp_cls = partial(
-                mlp_cls,
-                hidden_features=config.n_inner,
-                activation=activation,
-                checkpoint_lvl=mlp_checkpoint_lvl,
-                bias1=mlp_fc1_bias,
-                bias2=mlp_fc2_bias,
-                **parallel_kwargs,
-                **factory_kwargs,
-            )
-        elif fused_dense_sqrelu_dense:
-            if process_group is not None:
-                assert fused_mlp, "Tensor Parallel is not implemented for FusedDenseSqreluDense"
-            assert FusedDenseSqreluDense is not None
-            mlp_cls = partial(
-                FusedDenseSqreluDense,
-                hidden_features=config.n_inner,
-                checkpoint_lvl=mlp_checkpoint_lvl,
-                **factory_kwargs,
-            )
-        else:
-            raise RuntimeError("MLP type not supported")
-    return mlp_cls
-
-
-def create_block(config, layer_idx=None, process_group=None, device=None, dtype=None, multiple_of=256):
-    factory_kwargs = {"device": device, "dtype": dtype}
-    sequence_parallel = getattr(config, "sequence_parallel", True)
-    mixer_cls = create_mixer_cls(config, layer_idx, process_group=process_group, **factory_kwargs)
-    mlp_cls = create_mlp_cls(config, layer_idx, process_group=process_group,multiple_of=multiple_of, **factory_kwargs)
-    use_rms_norm = getattr(config, "rms_norm", False)
-    norm_cls = partial(
-        nn.LayerNorm if not use_rms_norm else RMSNorm,
-        eps=config.layer_norm_epsilon,
-        **factory_kwargs,
-    )
-    # TD [2022-07-30]: Force residual in fp32, seems to make fp16 training more stable
-    residual_in_fp32 = getattr(config, "residual_in_fp32", False)
-    resid_dropout1 = config.resid_pdrop if layer_idx is None or layer_idx > 0 else config.embd_pdrop
-    prenorm = getattr(config, "prenorm", True)
-    parallel_block = getattr(config, "parallel_block", False)
-    if not parallel_block:
-        block = Block(
-            config.hidden_size,
-            mixer_cls,
-            mlp_cls,
-            norm_cls=norm_cls,
-            prenorm=prenorm,
-            resid_dropout1=resid_dropout1,
-            resid_dropout2=config.resid_pdrop,
-            fused_dropout_add_ln=getattr(config, "fused_dropout_add_ln", False),
-            residual_in_fp32=residual_in_fp32,
-            sequence_parallel=sequence_parallel and process_group is not None,
-            mark_shared_params=process_group is not None,
-        )
-    else:
-        assert prenorm
-        block = ParallelBlock(
-            config.hidden_size,
-            mixer_cls,
-            mlp_cls,
-            norm_cls=norm_cls,
-            resid_dropout1=resid_dropout1,
-            resid_dropout2=config.resid_pdrop,
-            tied_norm=getattr(config, "parallel_block_tied_norm", False),
-            fused_dropout_add_ln=getattr(config, "fused_dropout_add_ln", False),
-            residual_in_fp32=residual_in_fp32,
-            sequence_parallel=sequence_parallel and process_group is not None,
-            mark_shared_params=process_group is not None,
-        )
-    block.layer_idx = layer_idx
-    return block
-
-
-class GPTPreTrainedModel(nn.Module):
-    """An abstract class to handle weights initialization and
-    a simple interface for dowloading and loading pretrained models.
-    """
-
-    def __init__(self, config, *inputs, **kwargs):
-        super().__init__()
-        if not isinstance(config, GPT2Config):
-            raise ValueError(
-                "Parameter config in `{}(config)` should be an instance of class `GPT2Config`. "
-                "To create a model from a Google pretrained model use "
-                "`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`".format(
-                    self.__class__.__name__, self.__class__.__name__
-                )
-            )
-        self.config = config
-
-    
-            
-    @classmethod
-    def from_pretrained_hf(cls, pretrained_model_name, device=None, **kwargs):
-        from based.utils.hf import load_config_hf
-
-        config_data = load_config_hf(pretrained_model_name)
-        config = GPT2Config(**config_data)
-        try:
-            model = GPTLMHeadModel(config=config, device=device, dtype=torch.float16)
-        except:
-            model = GPTLMHeadModel(config=config, device=device, dtype=torch.float16, multiple_of=128)
-        state_dict = state_dict_from_pretrained(pretrained_model_name, dtype=torch.float16)
-        # remove the 'model.' prefix from the keys
-        state_dict = {re.sub("^model\.", "", k): v for k, v in state_dict.items()}
-        # remove Unexpected key(s) in state_dict: "train_metrics.num-tokens.count", "val_metrics.num-tokens.count", "test_metrics.num-tokens.count". from the state_dict
-        state_dict = {k: v for k, v in state_dict.items() if "metrics" not in k}
-        model.load_state_dict(state_dict)
-       
-        return model.to(device=device)
-
-
-
-# https://github.com/huggingface/transformers/blob/c28d04e9e252a1a099944e325685f14d242ecdcd/src/transformers/models/gpt2/modeling_gpt2.py#L454
-def _init_weights(
-    module, n_layer, initializer_range=0.02, mup_width_scale=1.0, rescale_prenorm_residual=True
-):
-    mup_init_scale = math.sqrt(mup_width_scale)
-    if isinstance(module, nn.Linear):
-        nn.init.normal_(module.weight, std=initializer_range * mup_init_scale)
-        optim_cfg = getattr(module.weight, "_optim", {})
-        optim_cfg.update({"lr_multiplier": mup_width_scale})
-        setattr(module.weight, "_optim", optim_cfg)
-        if module.bias is not None:
-            nn.init.zeros_(module.bias)
-    elif isinstance(module, nn.Embedding):
-        nn.init.normal_(module.weight, std=initializer_range)
-
-    if rescale_prenorm_residual:
-        # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:
-        #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale
-        #   > the weights of residual layers at initialization by a factor of 1/N where N is the # of residual layers.
-        #   >   -- GPT-2 :: https://openai.com/blog/better-language-models/
-        #
-        # Reference (Megatron-LM): https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py
-        for name, p in module.named_parameters():
-            if name in ["out_proj.weight", "fc2.weight"]:
-                # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block
-                nn.init.normal_(
-                    p, mean=0.0, std=initializer_range * mup_init_scale / math.sqrt(2 * n_layer)
-                )
-
-
-class GPTModel(GPTPreTrainedModel):
-    def __init__(self, config: GPT2Config, process_group=None, device=None, dtype=None, multiple_of=256):
-        super().__init__(config)
-        factory_kwargs = {"device": device, "dtype": dtype}
-        self.process_group = process_group
-        self.sequence_parallel = getattr(config, "sequence_parallel", True)
-        assert config.activation_function in [
-            "gelu",
-            "gelu_new",
-            "gelu_fast",
-            "gelu_approx",
-            "gelu_pytorch_tanh",
-            "relu",
-            "sqrelu",
-            "glu",
-            "swiglu",
-            "geglu",
-        ]
-        pad_vocab_size_multiple = getattr(config, "pad_vocab_size_multiple", 1)
-        vocab_size = (
-            math.ceil(config.vocab_size / pad_vocab_size_multiple) * pad_vocab_size_multiple
-        )
-        self.embeddings_multiplier = getattr(config, "mup_embeddings_multiplier", 1.0)
-        # TD [2022-07-30]: Force residual in fp32, seems to make fp16 training more stable
-        self.residual_in_fp32 = getattr(config, "residual_in_fp32", False)
-        # These 2 options are for OPT-350m
-        self.prenorm = getattr(config, "prenorm", True)
-        use_rms_norm = getattr(config, "rms_norm", False)
-        word_embed_proj_dim = getattr(config, "word_embed_proj_dim", None)
-        # For GPT-J, GPT-NeoX
-        self.parallel_block = getattr(config, "parallel_block", False)
-
-        if process_group is None:
-            self.embeddings = GPT2Embeddings(
-                config.hidden_size,
-                vocab_size,
-                config.max_position_embeddings,
-                word_embed_proj_dim=word_embed_proj_dim,
-                **factory_kwargs,
-            )
-        else:
-            self.embeddings = ParallelGPT2Embeddings(
-                config.hidden_size,
-                vocab_size,
-                config.max_position_embeddings,
-                process_group=process_group,
-                sequence_parallel=self.sequence_parallel,
-                **factory_kwargs,
-            )
-
-        # We change the order of dropout, residual and layer norm:
-        # Instead of LN -> Attn / MLP -> Dropout -> Add, we do:
-        # Dropout -> Add -> LN -> Attn / MLP, returning both the residual branch (output of Add) and
-        # the main branch (output of MLP). The model definition is unchanged, but the mapping of the
-        # nn.Dropout probabilities are changed.
-        # This is for performance reason: we can fuse dropout + add + layer_norm.
-        self.layers = nn.ModuleList(
-            [
-                create_block(config, layer_idx=i, process_group=process_group, multiple_of=multiple_of, **factory_kwargs)
-                for i in range(config.num_hidden_layers)
-            ]
-        )
-        rotary_emb_fraction = getattr(config, "rotary_emb_fraction", 0.0)
-        if rotary_emb_fraction > 0.0:  # Tie all the RotaryEmbedding modules to share the same cos/sin cache
-            for layer in self.layers[1:]:
-                layer.mixer.rotary_emb = self.layers[0].mixer.rotary_emb
-
-        self.fused_dropout_add_ln = getattr(config, "fused_dropout_add_ln", False)
-        if self.fused_dropout_add_ln:
-            if layer_norm_fn is None:
-                raise ImportError("Triton is not installed")
-        if self.prenorm:
-            self.drop_f = nn.Dropout(config.resid_pdrop)
-            norm_cls = nn.LayerNorm if not use_rms_norm else RMSNorm
-            self.ln_f = norm_cls(
-                config.hidden_size, eps=config.layer_norm_epsilon, **factory_kwargs
-            )
-        if process_group is not None:
-            for p in self.ln_f.parameters():
-                # Mark the norm parameters as "shared_params" so that we sync their values at init.
-                p._shared_params = True
-                # Mark the norm params as "sequence_parallel" so we run all-reduce on their grads.
-                if self.sequence_parallel:
-                    p._sequence_parallel = True
-
-        self.apply(
-            partial(
-                _init_weights,
-                n_layer=config.num_hidden_layers,
-                initializer_range=config.initializer_range,
-                mup_width_scale=getattr(config, "mup_width_scale", 1.0),
-            )
-        )
-        self.tie_weights()
-
-    def tie_weights(self):
-        if self.process_group is not None:
-            sync_shared_params(self, self.process_group)
-
-    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):
-        return {
-            i: layer.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs)
-            for i, layer in enumerate(self.layers)
-        }
-
-    def forward(self, input_ids, position_ids=None, inference_params=None):
-        # If using Tensor Parallel with sequence parallel, we combine the batch and the seqlen
-        # dimensions so that we can split on it easily, in case of small batch size.
-        # Only the attention layers need to know the seqlen.
-        embedding_kwargs = (
-            {"combine_batch_seqlen_dim": True}
-            if self.process_group is not None and self.sequence_parallel
-            else {}
-        )
-        hidden_states = self.embeddings(input_ids, position_ids=position_ids, **embedding_kwargs)
-        if self.embeddings_multiplier != 1.0:
-            hidden_states = hidden_states * self.embeddings_multiplier
-        if self.parallel_block:
-            hidden_states2 = None
-        residual = None
-        mixer_kwargs = (
-            {"seqlen": input_ids.shape[1]}
-            if self.process_group is not None and self.sequence_parallel
-            else {}
-        )
-        if inference_params is not None:
-            mixer_kwargs["inference_params"] = inference_params
-        for layer in self.layers:
-            if self.prenorm:
-                if not self.parallel_block:
-                    hidden_states, residual = layer(
-                        hidden_states, residual, mixer_kwargs=mixer_kwargs
-                    )
-                else:
-                    hidden_states, hidden_states2, residual = layer(
-                        hidden_states, hidden_states2, residual, mixer_kwargs=mixer_kwargs
-                    )
-            else:
-                hidden_states = layer(hidden_states, mixer_kwargs=mixer_kwargs)
-        if self.prenorm:
-            if not self.fused_dropout_add_ln:
-                dropped = self.drop_f(hidden_states)
-                if not self.parallel_block:
-                    residual = (dropped + residual) if residual is not None else dropped
-                else:
-                    dropped2 = self.drop_f(hidden_states2)
-                    residual = (
-                        (residual + dropped + dropped2)
-                        if residual is not None
-                        else dropped + dropped2
-                    )
-                hidden_states = self.ln_f(residual.to(dtype=self.ln_f.weight.dtype))
-            else:
-                # Set prenorm=False here since we don't need the residual
-                hidden_states = layer_norm_fn(
-                    hidden_states,
-                    self.ln_f.weight,
-                    self.ln_f.bias,
-                    residual=residual,
-                    x1=None if not self.parallel_block else hidden_states2,
-                    eps=self.ln_f.eps,
-                    dropout_p=self.drop_f.p if self.training else 0.0,
-                    prenorm=False,
-                    is_rms_norm=isinstance(self.ln_f, RMSNorm)
-                )
-        return hidden_states
-
-
-class GPTLMHeadModel(GPTPreTrainedModel, GenerationMixin, NaiveGenerationMixin):
-    def __init__(self, config: GPT2Config, process_group=None, device=None, dtype=None, multiple_of=256):
-        factory_kwargs = {"device": device, "dtype": dtype}
-        super().__init__(config)
-        self.process_group = process_group
-        self.transformer = GPTModel(config, process_group=process_group, multiple_of=multiple_of, **factory_kwargs)
-        self.tie_word_embeddings = getattr(config, "tie_word_embeddings", True)
-        lm_head_bias = getattr(config, "lm_head_bias", False)
-        pad_vocab_size_multiple = getattr(config, "pad_vocab_size_multiple", 1)
-        vocab_size = (
-            math.ceil(config.vocab_size / pad_vocab_size_multiple) * pad_vocab_size_multiple
-        )
-        # This option is for OPT-350m
-        word_embed_proj_dim = getattr(config, "word_embed_proj_dim", None)
-        embed_dim = config.n_embd if word_embed_proj_dim is None else word_embed_proj_dim
-        if word_embed_proj_dim is not None:
-            self.project_out = nn.Linear(config.n_embd, embed_dim, bias=False, **factory_kwargs)
-        else:
-            self.project_out = None
-        mup_width_scale = getattr(config, "mup_width_scale", 1.0)
-        mup_output_multiplier = getattr(config, "mup_output_multiplier", 1.0)
-        self.output_scale = mup_output_multiplier * mup_width_scale
-        if process_group is None:
-            self.lm_head = nn.Linear(embed_dim, vocab_size, bias=lm_head_bias, **factory_kwargs)
-        else:
-            if ColumnParallelLinear is None:
-                raise ImportError("fused_dense_lib is not installed")
-            self.lm_head = ColumnParallelLinear(
-                embed_dim,
-                vocab_size,
-                process_group,
-                bias=lm_head_bias,
-                sequence_parallel=getattr(config, "sequence_parallel", True),
-                **factory_kwargs,
-            )
-        self.norm_head = getattr(config, "norm_head", False)
-        # Initialize weights and apply final processing
-        self.apply(
-            partial(
-                _init_weights,
-                n_layer=config.num_hidden_layers,
-                initializer_range=config.initializer_range,
-                mup_width_scale=mup_width_scale,
-            )
-        )
-        self.tie_weights()
-
-    def tie_weights(self):
-        if self.tie_word_embeddings:
-            self.lm_head.weight = self.transformer.embeddings.word_embeddings.weight
-        if self.process_group is not None:
-            sync_shared_params(self, self.process_group)
-
-    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):
-        return self.transformer.allocate_inference_cache(
-            batch_size, max_seqlen, dtype=dtype, **kwargs
-        )
-
-    def forward(self, input_ids, position_ids=None, inference_params=None, num_last_tokens=0):
-        """
-        input_ids: (batch, seqlen) int tensor
-        inference_params: for generation. Adapted from Megatron-LM (and Apex)
-        https://github.com/NVIDIA/apex/blob/3ff1a10f72ec07067c4e44759442329804ac5162/apex/transformer/testing/standalone_transformer_lm.py#L470
-        num_last_tokens: if > 0, only return the logits for the last n tokens
-        """
-        assert (
-            input_ids.ndim == 2
-        ), f"Expected `input_ids` to have shape [b, slen], but got shape {input_ids.shape}"
-        b, slen = input_ids.shape
-        hidden_states = self.transformer(
-            input_ids, position_ids=position_ids, inference_params=inference_params
-        )
-        if inference_params is not None:
-            assert hidden_states.ndim == 3, "sequence_parallel is not supported in generation mode"
-        if num_last_tokens > 0:
-            hidden_states = hidden_states[:, -num_last_tokens:]
-        if self.project_out is not None:
-            hidden_states = self.project_out(hidden_states)
-        if self.output_scale != 1.0:
-            hidden_states = hidden_states * self.output_scale
-        if not self.norm_head:
-            lm_logits = self.lm_head(hidden_states)
-        else:
-            lm_head_weight = F.normalize(self.lm_head.weight)
-            if isinstance(self.lm_head, ColumnParallelLinear) and self.lm_head.sequence_parallel:
-                hidden_states = all_gather(hidden_states, self.lm_head.process_group)
-            lm_logits = F.linear(hidden_states, lm_head_weight, bias=self.lm_head.bias)
-        # During inference, we want the full logit for sampling
-        if isinstance(self.lm_head, ColumnParallelLinear) and inference_params is not None:
-            lm_logits, _ = all_gather_raw(lm_logits, self.lm_head.process_group)
-            lm_logits = rearrange(lm_logits, "(n b) ... d -> b ... (n d)", b=b)
-        CausalLMOutput = namedtuple("CausalLMOutput", ["logits"])
-        return CausalLMOutput(logits=lm_logits)
-
-    def load_state_dict(self, state_dict, strict=True):
-        # Remapping from our checkpoints that used a different ordering of layers in the block
-        # Previous: Attn / MLP -> Dropout -> Add -> LN
-        # Current: Dropout -> Add -> LN -> Attn / MLP
-        if "transformer.ln_0.weight" in state_dict:
-            n_layers = len(self.transformer.layers)
-            ln_weight = state_dict.pop(f"transformer.layers.{n_layers - 1}.norm2.weight")
-            ln_bias = state_dict.pop(f"transformer.layers.{n_layers - 1}.norm2.bias")
-            state_dict["transformer.ln_f.weight"] = ln_weight
-            state_dict["transformer.ln_f.bias"] = ln_bias
-            for l in reversed(range(n_layers)):
-                ln_weight = state_dict.pop(f"transformer.layers.{l}.norm1.weight")
-                ln_bias = state_dict.pop(f"transformer.layers.{l}.norm1.bias")
-                state_dict[f"transformer.layers.{l}.norm2.weight"] = ln_weight
-                state_dict[f"transformer.layers.{l}.norm2.bias"] = ln_bias
-                if l > 0:
-                    ln_weight = state_dict.pop(f"transformer.layers.{l - 1}.norm2.weight")
-                    ln_bias = state_dict.pop(f"transformer.layers.{l - 1}.norm2.bias")
-                    state_dict[f"transformer.layers.{l}.norm1.weight"] = ln_weight
-                    state_dict[f"transformer.layers.{l}.norm1.bias"] = ln_bias
-            ln_weight = state_dict.pop("transformer.ln_0.weight")
-            ln_bias = state_dict.pop("transformer.ln_0.bias")
-            state_dict[f"transformer.layers.0.norm1.weight"] = ln_weight
-            state_dict[f"transformer.layers.0.norm1.bias"] = ln_bias
-        return super().load_state_dict(state_dict, strict=strict)
-
-
-def shard_state_dict_tp(state_dict, config, world_size, rank):
-    """Convert the state_dict of a standard GPT model to the state_dict of a GPT model
-    with tensor parallel.
-
-    This function modifies state_dict in place.
-    """
-    pad_vocab_size_multiple = getattr(config, "pad_vocab_size_multiple", 1)
-    vocab_size = math.ceil(config.vocab_size / pad_vocab_size_multiple) * pad_vocab_size_multiple
-    assert vocab_size % world_size == 0
-    assert config.hidden_size % world_size == 0
-    inner_dim = config.n_inner if config.n_inner is not None else 4 * config.hidden_size
-    assert inner_dim % world_size == 0
-
-    n_head = config.n_head
-    n_head_kv = getattr(config, "n_head_kv", n_head)
-
-    embed_dim = config.hidden_size
-    head_dim = embed_dim // n_head
-
-    def shard_first_dim(state_dict, key):
-        if key in state_dict:
-            x = state_dict[key]
-            dim = x.shape[0] // world_size
-            state_dict[key] = x[rank * dim : (rank + 1) * dim]
-
-    def shard_last_dim(state_dict, key, multiple_of=1):
-        if key in state_dict:
-            x = state_dict[key]
-            dim_each_rank = [
-                get_dim_for_local_rank(x.size(-1), world_size, local_rank, multiple_of)
-                for local_rank in range(world_size)
-            ]
-            beg, end = tuple(sum(dim_each_rank[:pos]) for pos in (rank, rank + 1))
-            state_dict[key] = x[..., beg:end]
-
-    def shard_gatedmlp_fc1_dim(state_dict, key):
-        if key in state_dict:
-            x = state_dict[key]
-            dim = x.shape[0] // world_size // 2
-            state_dict[key] = rearrange(
-                rearrange(x, "(two o) ... -> two o ...", two=2)[:, rank * dim : (rank + 1) * dim],
-                "two o ... -> (two o) ...",
-            )
-
-    def shard_qkv_headdim(state_dict, key):
-        if key in state_dict:
-            n_head_each_rank = [
-                get_dim_for_local_rank(n_head, world_size, local_rank)
-                for local_rank in range(world_size)
-            ]
-            n_head_kv_each_rank = [
-                get_dim_for_local_rank(n_head_kv, world_size, local_rank)
-                for local_rank in range(world_size)
-            ]
-
-            beg_n_head = sum(n_head_each_rank[:rank])
-            end_n_head = sum(n_head_each_rank[: rank + 1])
-
-            beg_n_head_kv = sum(n_head_kv_each_rank[:rank])
-            end_n_head_kv = sum(n_head_kv_each_rank[: rank + 1])
-
-            if n_head_kv == n_head:
-                x = rearrange(state_dict[key], "(three d) ... -> three d ...", three=3)
-                state_dict[key] = rearrange(
-                    x[:, beg_n_head * head_dim : end_n_head * head_dim],
-                    "three d ... -> (three d) ...",
-                )
-            else:
-                x = rearrange(
-                    state_dict[key],
-                    "(nheadqkv headdim) ... -> nheadqkv headdim ...",
-                    nheadqkv=n_head + 2 * n_head_kv,
-                )
-                state_dict[key] = rearrange(
-                    torch.cat(
-                        [
-                            x[beg_n_head:end_n_head],
-                            x[n_head + beg_n_head_kv : n_head + end_n_head_kv],
-                            x[
-                                n_head
-                                + n_head_kv
-                                + beg_n_head_kv : n_head
-                                + n_head_kv
-                                + end_n_head_kv
-                            ],
-                        ],
-                        dim=0,
-                    ),
-                    "nheadqkv headdim ... -> (nheadqkv headdim) ...",
-                )
-
-    shard_first_dim(state_dict, "transformer.embeddings.word_embeddings.weight")
-    if "lm_head.weight" in state_dict:
-        shard_first_dim(state_dict, "lm_head.weight")
-    if "transformer.embeddings.position_embeddings.weight" in state_dict:
-        shard_last_dim(state_dict, "transformer.embeddings.position_embeddings.weight")
-    for i in range(config.num_hidden_layers):
-        shard_qkv_headdim(state_dict, f"transformer.layers.{i}.mixer.Wqkv.weight")
-        shard_qkv_headdim(state_dict, f"transformer.layers.{i}.mixer.Wqkv.bias")
-        shard_last_dim(
-            state_dict, f"transformer.layers.{i}.mixer.out_proj.weight", multiple_of=head_dim
-        )
-        if rank != 0:
-            state_dict.pop(f"transformer.layers.{i}.mixer.out_proj.bias", None)
-        if config.activation_function in ["glu", "swiglu", "geglu"]:
-            shard_gatedmlp_fc1_dim(state_dict, f"transformer.layers.{i}.mlp.fc1.weight")
-            shard_gatedmlp_fc1_dim(state_dict, f"transformer.layers.{i}.mlp.fc1.bias")
-        else:
-            shard_first_dim(state_dict, f"transformer.layers.{i}.mlp.fc1.weight")
-            shard_first_dim(state_dict, f"transformer.layers.{i}.mlp.fc1.bias")
-        shard_last_dim(state_dict, f"transformer.layers.{i}.mlp.fc2.weight")
-        if rank != 0:
-            state_dict.pop(f"transformer.layers.{i}.mlp.fc2.bias", None)
-    return state_dict
-
-
-def combine_state_dicts_tp(state_dicts: List[Dict[str, torch.Tensor]], config: GPT2Config):
-    """Convert the list of sharded state_dict of a GPT model with tensor parallel to
-    the state_dict of a standard GPT model.
-
-    This function is meant to be the "reverse" of shard_state_dict_tp.
-
-    Precondition:
-        - state_dicts should be ordered in the same way as the shards were created.
-    """
-    world_size = len(state_dicts)
-    keys = state_dicts[0].keys()
-    pad_vocab_size_multiple = getattr(config, "pad_vocab_size_multiple", 1)
-    vocab_size = math.ceil(config.vocab_size / pad_vocab_size_multiple) * pad_vocab_size_multiple
-    assert vocab_size % world_size == 0
-    assert config.hidden_size % world_size == 0
-    inner_dim = config.n_inner if config.n_inner is not None else 4 * config.hidden_size
-    assert inner_dim % world_size == 0
-    assert config.hidden_size % config.n_head == 0
-    headdim = config.hidden_size // config.n_head
-
-    # Sometimes the word embeddings are sharded on the 0th dim, sometimes on the 1st dim.
-    # vocab_size // world_size coordinates are nonzero.
-    def combine_word_embeddings(state_dicts, state_dict, key):
-        dim = 0 if state_dicts[0][key].shape[0] == vocab_size // world_size else 1
-        state_dict[key] = torch.cat([s[key] for s in state_dicts], dim=dim)
-
-    def combine_dim(state_dicts, state_dict, key, dim=-1):
-        if key in state_dict:
-            state_dict[key] = torch.cat([s[key] for s in state_dicts], dim=dim)
-
-    def combine_qkv_headdim(state_dicts, state_dict, key):
-        n_head = config.n_head
-        n_head_kv = getattr(config, "n_head_kv", n_head)
-        if key in state_dict:
-            if n_head_kv == n_head:
-                xs = [
-                    rearrange(s[key], "(three d) ... -> three d ...", three=3) for s in state_dicts
-                ]
-                state_dict[key] = rearrange(torch.cat(xs, dim=1), "three d ... -> (three d) ...")
-            else:
-                n_head_each_rank = [
-                    get_dim_for_local_rank(n_head, world_size, local_rank)
-                    for local_rank in range(world_size)
-                ]
-                n_head_kv_each_rank = [
-                    get_dim_for_local_rank(n_head_kv, world_size, local_rank)
-                    for local_rank in range(world_size)
-                ]
-                xs = [
-                    rearrange(
-                        s[key],
-                        "(nheadqkv headdim) ... -> nheadqkv headdim ...",
-                        nheadqkv=rank_n_head + 2 * rank_n_head_kv,
-                        headdim=headdim,
-                    )
-                    for s, rank_n_head, rank_n_head_kv in zip(
-                        state_dicts, n_head_each_rank, n_head_kv_each_rank
-                    )
-                ]
-                wq = torch.cat([x[: n_head_each_rank[rank]] for rank, x in enumerate(xs)], dim=0)
-                wk = torch.cat(
-                    [
-                        x[
-                            n_head_each_rank[rank] : n_head_each_rank[rank]
-                            + n_head_kv_each_rank[rank]
-                        ]
-                        for rank, x in enumerate(xs)
-                    ],
-                    dim=0,
-                )
-                wv = torch.cat(
-                    [
-                        x[n_head_each_rank[rank] + n_head_kv_each_rank[rank] :]
-                        for rank, x in enumerate(xs)
-                    ],
-                    dim=0,
-                )
-                wqkv = torch.cat(
-                    [wq, wk, wv],
-                    dim=0,
-                )
-                state_dict[key] = rearrange(
-                    wqkv,
-                    "nheadqkv headdim ... -> (nheadqkv headdim) ...",
-                )
-
-    def combine_gated_mlp(state_dicts, state_dict, key):
-        if key in state_dict:
-            xs = [rearrange(s[key], "(two d) ... -> two d ...", two=2) for s in state_dicts]
-            state_dict[key] = rearrange(torch.cat(xs, dim=1), "two d ... -> (two d) ...")
-
-    state_dict = state_dicts[0].copy()  # don't modify state_dict[0] inplace
-    combine_word_embeddings(
-        state_dicts, state_dict, "transformer.embeddings.word_embeddings.weight"
-    )
-    if "lm_head.weight" in state_dict:
-        combine_word_embeddings(state_dicts, state_dict, "lm_head.weight")
-    if "transformer.embeddings.position_embeddings.weight" in state_dict:
-        combine_dim(
-            state_dicts, state_dict, "transformer.embeddings.position_embeddings.weight", -1
-        )
-    mlp_combine_fn = (
-        combine_gated_mlp
-        if config.activation_function in ["glu", "swiglu", "geglu"]
-        else partial(combine_dim, dim=0)
-    )
-    for i in range(config.num_hidden_layers):
-        combine_qkv_headdim(state_dicts, state_dict, f"transformer.layers.{i}.mixer.Wqkv.weight")
-        combine_qkv_headdim(state_dicts, state_dict, f"transformer.layers.{i}.mixer.Wqkv.bias")
-        combine_dim(state_dicts, state_dict, f"transformer.layers.{i}.mixer.out_proj.weight", -1)
-        mlp_combine_fn(state_dicts, state_dict, f"transformer.layers.{i}.mlp.fc1.weight")
-        combine_dim(state_dicts, state_dict, f"transformer.layers.{i}.mlp.fc1.bias", 0)
-        combine_dim(state_dicts, state_dict, f"transformer.layers.{i}.mlp.fc2.weight", -1)
-    return state_dict
-
-
-def remap_state_dict_hf_gpt2(state_dict, config):
-    # Word embedding and position embedding
-    def key_mapping_pos_emb(key):
-        return re.sub(r"^wpe.", "transformer.embeddings.position_embeddings.", key)
-
-    state_dict = OrderedDict((key_mapping_pos_emb(k), v) for k, v in state_dict.items())
-    word_embeddings = state_dict.pop("wte.weight")
-    # It's possible that vocab_size is padded to be a multiple of 8, for example.
-    pad_vocab_size_multiple = getattr(config, "pad_vocab_size_multiple", 1)
-    vocab_size = math.ceil(config.vocab_size / pad_vocab_size_multiple) * pad_vocab_size_multiple
-    state_dict["transformer.embeddings.word_embeddings.weight"] = F.pad(
-        word_embeddings, (0, 0, 0, vocab_size - word_embeddings.shape[0])
-    )
-    state_dict["lm_head.weight"] = state_dict["transformer.embeddings.word_embeddings.weight"]
-
-    # LayerNorm
-    def key_mapping_ln(key):
-        key = re.sub(r"^ln_f.(weight|bias)", r"transformer.ln_f.\1", key)
-        key = re.sub(r"^h.(\d+).ln_(1|2).(weight|bias)", r"transformer.layers.\1.norm\2.\3", key)
-        return key
-
-    state_dict = OrderedDict((key_mapping_ln(k), v) for k, v in state_dict.items())
-
-    # MLP
-    for d in range(config.num_hidden_layers):
-        W1 = state_dict.pop(f"h.{d}.mlp.c_fc.weight")
-        state_dict[f"transformer.layers.{d}.mlp.fc1.weight"] = W1.t()
-        W2 = state_dict.pop(f"h.{d}.mlp.c_proj.weight")
-        state_dict[f"transformer.layers.{d}.mlp.fc2.weight"] = W2.t()
-
-    def key_mapping_mlp(key):
-        key = re.sub(r"^h.(\d+).mlp.c_fc.bias", r"transformer.layers.\1.mlp.fc1.bias", key)
-        key = re.sub(r"^h.(\d+).mlp.c_proj.bias", r"transformer.layers.\1.mlp.fc2.bias", key)
-        return key
-
-    state_dict = OrderedDict((key_mapping_mlp(k), v) for k, v in state_dict.items())
-
-    # Attention
-    for d in range(config.num_hidden_layers):
-        state_dict.pop(f"h.{d}.attn.bias")  # We don't store this bias
-        Wqkv = state_dict.pop(f"h.{d}.attn.c_attn.weight")
-        state_dict[f"transformer.layers.{d}.mixer.Wqkv.weight"] = Wqkv.t()
-        Wout = state_dict.pop(f"h.{d}.attn.c_proj.weight")
-        state_dict[f"transformer.layers.{d}.mixer.out_proj.weight"] = Wout.t()
-
-    def key_mapping_attn(key):
-        key = re.sub(r"^h.(\d+).attn.c_attn.bias", r"transformer.layers.\1.mixer.Wqkv.bias", key)
-        key = re.sub(
-            r"^h.(\d+).attn.c_proj.bias", r"transformer.layers.\1.mixer.out_proj.bias", key
-        )
-        return key
-
-    state_dict = OrderedDict((key_mapping_attn(k), v) for k, v in state_dict.items())
-
-    return state_dict
-
-
-def remap_state_dict_megatron(state_dict, config):
-    def key_mapping_transformer(key):
-        key = re.sub(r"^language_model.encoder.", "transformer.", key)
-        key = re.sub(r"^language_model.", "transformer.", key)
-        return key
-
-    state_dict = OrderedDict((key_mapping_transformer(k), v) for k, v in state_dict.items())
-
-    # Word embedding and position embedding
-    def key_mapping_pos_emb(key):
-        return re.sub(r"^wpe.", "transformer.embeddings.position_embeddings.", key)
-
-    state_dict = OrderedDict((key_mapping_pos_emb(k), v) for k, v in state_dict.items())
-    word_embeddings = state_dict.pop("transformer.embedding.word_embeddings.weight")
-    # It's possible that vocab_size is padded to be a multiple of 8, for example.
-    pad_vocab_size_multiple = getattr(config, "pad_vocab_size_multiple", 1)
-    vocab_size = (
-        math.ceil(word_embeddings.shape[0] / pad_vocab_size_multiple) * pad_vocab_size_multiple
-    )
-    state_dict["transformer.embeddings.word_embeddings.weight"] = F.pad(
-        word_embeddings, (0, 0, 0, vocab_size - word_embeddings.shape[0])
-    )
-    state_dict["lm_head.weight"] = state_dict["transformer.embeddings.word_embeddings.weight"]
-
-    # LayerNorm
-    def key_mapping_ln(key):
-        key = re.sub(r"^transformer.final_layernorm.(weight|bias)", r"transformer.ln_f.\1", key)
-        key = re.sub(
-            r"^transformer.layers.(\d+).input_layernorm.(weight|bias)",
-            r"transformer.layers.\1.norm1.\2",
-            key,
-        )
-        key = re.sub(
-            r"^transformer.layers.(\d+).post_attention_layernorm.(weight|bias)",
-            r"transformer.layers.\1.norm2.\2",
-            key,
-        )
-        return key
-
-    state_dict = OrderedDict((key_mapping_ln(k), v) for k, v in state_dict.items())
-
-    # MLP
-    def key_mapping_mlp(key):
-        key = re.sub(
-            r"^transformer.layers.(\d+).mlp.dense_h_to_4h.(weight|bias)",
-            r"transformer.layers.\1.mlp.fc1.\2",
-            key,
-        )
-        key = re.sub(
-            r"^transformer.layers.(\d+).mlp.dense_4h_to_h.(weight|bias)",
-            r"transformer.layers.\1.mlp.fc2.\2",
-            key,
-        )
-        return key
-
-    state_dict = OrderedDict((key_mapping_mlp(k), v) for k, v in state_dict.items())
-
-    # Attention
-    def key_mapping_attn(key):
-        key = re.sub(
-            r"^transformer.layers.(\d+).self_attention.rotary_emb.inv_freq",
-            r"transformer.layers.\1.mixer.rotary_emb.inv_freq",
-            key,
-        )
-        key = re.sub(
-            r"^transformer.layers.(\d+).self_attention.query_key_value.(weight|bias)",
-            r"transformer.layers.\1.mixer.Wqkv.\2",
-            key,
-        )
-        key = re.sub(
-            r"^transformer.layers.(\d+).self_attention.dense.(weight|bias)",
-            r"transformer.layers.\1.mixer.out_proj.\2",
-            key,
-        )
-        return key
-
-    state_dict = OrderedDict((key_mapping_attn(k), v) for k, v in state_dict.items())
-    # Megatron stores Wqkv as ((nheads 3 headdim), hidden_dim)
-    # while we store Wqkv as ((3 nheads headdim), hidden_dim)
-    headdim = config.hidden_size // config.num_attention_heads
-    for d in range(config.num_hidden_layers):
-        Wqkv = state_dict.pop(f"transformer.layers.{d}.mixer.Wqkv.weight")
-        state_dict[f"transformer.layers.{d}.mixer.Wqkv.weight"] = rearrange(
-            Wqkv,
-            "(nheads three headdim) ... -> (three nheads headdim) ...",
-            three=3,
-            headdim=headdim,
-        )
-        bqkv = state_dict.pop(f"transformer.layers.{d}.mixer.Wqkv.bias")
-        state_dict[f"transformer.layers.{d}.mixer.Wqkv.bias"] = rearrange(
-            bqkv, "(nheads three headdim) -> (three nheads headdim)", three=3, headdim=headdim
-        )
-
-    return state_dict
\ No newline at end of file
diff --git a/based/ops/__init__.py b/based/ops/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/based/ops/activations.py b/based/ops/activations.py
deleted file mode 100644
index b00063b..0000000
--- a/based/ops/activations.py
+++ /dev/null
@@ -1,135 +0,0 @@
-# Copied from https://github.com/mlcommons/training_results_v1.1/blob/main/NVIDIA/benchmarks/bert/implementations/pytorch/model/layers/activations.py
-import math
-
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-
-# 1/sqrt(2*pi)-> 0.3989423
-# 1/sqrt(2)   -> 0.70710678
-# sqrt(2/pi)  -> 0.79788456
-
-# this function is tanh approximation of gelu
-# actual gelu is:
-# x * 0.5 * (1.0 + torch.erf(x * 0.70710678))
-@torch.jit.script
-def bias_gelu(y, bias):
-    x = bias + y
-    return (x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))).to(dtype=y.dtype)
-
-
-# gradient of tanh approximation of gelu
-# gradient of actual gelu is:
-# 0.5 * (1. + torch.erf(x * 0.70710678)) + 0.3989423 * x * torch.exp(-0.5 * x * x)
-@torch.jit.script
-def bias_gelu_back(g, y, bias):
-    """Assume that y has shape (B, D) and bias has shape (D)"""
-    x = bias + y
-    tanh_out = torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x))
-    # sqrt(2/pi) * 3 * 0.044715 -> 0.1070322243
-    ff = 0.5 * x * ((1 - tanh_out * tanh_out) * (0.79788456 + 0.1070322243 * x * x)) + 0.5 * (
-        1 + tanh_out
-    )
-    grad_y = ff * g
-    return grad_y.to(dtype=y.dtype), grad_y.sum(dim=(0), dtype=bias.dtype)
-
-
-class GeLUFunction(torch.autograd.Function):
-    @staticmethod
-    # bias is an optional argument
-    def forward(ctx, input, bias):
-        ctx.save_for_backward(input, bias)
-        return bias_gelu(input, bias)
-
-    @staticmethod
-    def backward(ctx, grad_output):
-        input, bias = ctx.saved_tensors
-        tmp = bias_gelu_back(grad_output, input, bias)
-        return tmp, tmp
-
-
-bias_gelu_impl = GeLUFunction.apply
-
-# this function is tanh approximation of gelu
-# actual gelu is:
-# x * 0.5 * (1.0 + torch.erf(x * 0.70710678))
-@torch.jit.script
-def gelu_fwd(x):
-    return (x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))).to(dtype=x.dtype)
-
-
-# gradient of tanh approximation of gelu
-# gradient of actual gelu is:
-# 0.5 * (1. + torch.erf(x * 0.70710678)) + 0.3989423 * x * torch.exp(-0.5 * x * x)
-@torch.jit.script
-def gelu_bwd(g, x):
-    tanh_out = torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x))
-    # sqrt(2/pi) * 3 * 0.044715 -> 0.1070322243
-    ff = 0.5 * x * ((1 - tanh_out * tanh_out) * (0.79788456 + 0.1070322243 * x * x)) + 0.5 * (
-        1 + tanh_out
-    )
-    return (ff * g).to(dtype=x.dtype)
-
-
-class FastGeLUFunction(torch.autograd.Function):
-    @staticmethod
-    # bias is an optional argument
-    def forward(ctx, input):
-        ctx.save_for_backward(input)
-        return gelu_fwd(input)
-
-    @staticmethod
-    def backward(ctx, grad_output):
-        (input,) = ctx.saved_tensors
-        tmp = gelu_bwd(grad_output, input)
-        return tmp
-
-
-fast_gelu_impl = FastGeLUFunction.apply
-
-
-@torch.jit.script
-def relu_bwd(g, x):
-    return torch.where(x >= 0, g, 0.0).to(dtype=x.dtype)
-
-
-@torch.jit.script
-def sqrelu_fwd(x):
-    r = F.relu(x)
-    return (r * r).to(dtype=x.dtype)
-
-
-@torch.jit.script
-def sqrelu_bwd(g, x):
-    return (2.0 * g * F.relu(x)).to(dtype=x.dtype)
-
-
-swiglu_fwd_codestring = """
-template <typename T> T swiglu_fwd(T x, T y) {
-    return float(x) * float(y) / (1.0f + ::exp(-float(x)));
-}
-"""
-swiglu_bwd_codestring = """
-template <typename T> T swiglu_bwd(T x, T y, T g, T& dx, T& dy) {
-    float x_sigmoid = 1.0f / (1.0f + ::exp(-float(x)));
-    dx = x_sigmoid * (1 + float(x) * (1.0f - x_sigmoid)) * float(g) * float(y);
-    dy = float(x) * x_sigmoid * float(g);
-}
-"""
-swiglu_fwd = torch.cuda.jiterator._create_jit_fn(swiglu_fwd_codestring)
-swiglu_bwd = torch.cuda.jiterator._create_multi_output_jit_fn(swiglu_bwd_codestring, num_outputs=2)
-
-
-class SwiGLUFunction(torch.autograd.Function):
-
-    @staticmethod
-    def forward(ctx, x, y):
-        ctx.save_for_backward(x, y)
-        return swiglu_fwd(x, y)
-
-    @staticmethod
-    def backward(ctx, dout):
-        x, y = ctx.saved_tensors
-        return swiglu_bwd(x, y, dout)
-
-swiglu = SwiGLUFunction.apply
diff --git a/based/ops/fused_dense.py b/based/ops/fused_dense.py
deleted file mode 100644
index 1e45b8e..0000000
--- a/based/ops/fused_dense.py
+++ /dev/null
@@ -1,688 +0,0 @@
-# Copyright (c) 2023, Tri Dao.
-# Inspired by https://github.com/NVIDIA/apex/blob/master/apex/fused_dense/fused_dense.py
-# We make it work with pytorch amp and with bfloat16.
-# The TensorParallel linear modules are inspired by https://github.com/NVIDIA/apex/blob/master/apex/transformer/tensor_parallel/layers.py
-from functools import partial
-from typing import Optional
-
-# import fused_dense_cuda  # from apex
-import fused_dense_lib as fused_dense_cuda
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-from torch import Tensor
-from torch.cuda.amp import custom_bwd, custom_fwd
-from torch.distributed import ProcessGroup
-
-from flash_attn.ops.activations import gelu_bwd, relu_bwd, sqrelu_bwd, sqrelu_fwd
-from flash_attn.utils.distributed import (
-    all_gather_raw,
-    all_reduce,
-    all_reduce_raw,
-    reduce_scatter,
-    reduce_scatter_raw,
-)
-
-
-class FusedDenseFunc(torch.autograd.Function):
-    @staticmethod
-    @custom_fwd
-    def forward(
-        ctx, x, weight, bias, return_residual=False, process_group=None, sequence_parallel=True
-    ):
-        """
-        If process_group is not None and sequence_parallel=True, we're doing Tensor Parallel
-        with sequence parallelism: we do an all_gather_raw of x before doing the matmul.
-        """
-        ctx.compute_weight_gradient = weight.requires_grad
-        ctx.return_residual = return_residual
-        ctx.process_group = process_group
-        ctx.sequence_parallel = sequence_parallel
-
-        if torch.is_autocast_enabled():
-            x = x.to(dtype=torch.get_autocast_gpu_dtype())
-        x = x.contiguous()
-        if process_group is not None and sequence_parallel:
-            # We want to kick off the all_gather early, before weight dtype conversion
-            total_x, handle_x = all_gather_raw(x, process_group, async_op=True)
-        else:
-            total_x = x
-
-        if torch.is_autocast_enabled():
-            weight = weight.to(dtype=torch.get_autocast_gpu_dtype())
-            bias = bias.to(dtype=torch.get_autocast_gpu_dtype()) if bias is not None else None
-        weight = weight.contiguous()
-        if process_group is not None and sequence_parallel:
-            handle_x.wait()
-        batch_shape, n = total_x.shape[:-1], total_x.shape[-1]
-        batch_dim = batch_shape.numel()
-        # https://github.com/pytorch/pytorch/blob/5b51849b48a7dbccd297286cc0110def4706f9e7/aten/src/ATen/native/cuda/Blas.cpp#L174
-        if min(batch_dim, n, *weight.shape) > 65535 * 32:
-            raise RuntimeError("fused_dense only supports matrix dims <= 2M")
-        output = F.linear(total_x, weight, bias)
-        if ctx.compute_weight_gradient:
-            ctx.save_for_backward(x, weight)
-        else:
-            ctx.save_for_backward(weight)
-        return output if not return_residual else (output, x)
-
-    @staticmethod
-    @custom_bwd
-    def backward(ctx, grad_output, *args):
-        grad_output = grad_output.contiguous()
-        if ctx.return_residual:
-            (grad_input,) = args
-            grad_input = grad_input.contiguous()
-        process_group = ctx.process_group
-        sequence_parallel = ctx.sequence_parallel
-        if ctx.compute_weight_gradient:
-            x, weight = ctx.saved_tensors
-            if process_group is not None and sequence_parallel:
-                total_x, handle_x = all_gather_raw(x, process_group, async_op=True)
-            else:
-                total_x = x
-        else:
-            (weight,) = ctx.saved_tensors
-            total_x = None
-        batch_shape = grad_output.shape[:-1]
-        batch_dim = batch_shape.numel()
-        grad_output = grad_output.reshape(batch_dim, grad_output.shape[-1])
-        if ctx.needs_input_grad[0]:
-            if not ctx.return_residual:
-                grad_input = F.linear(grad_output, weight.t())
-            else:
-                grad_input = torch.addmm(
-                    grad_input.reshape(batch_dim, grad_input.shape[-1]), grad_output, weight
-                )
-            grad_input = grad_input.reshape(*batch_shape, grad_input.shape[-1])
-            if process_group is not None:
-                reduce_fn = reduce_scatter_raw if sequence_parallel else all_reduce_raw
-                grad_input, handle_grad_input = reduce_fn(grad_input, process_group, async_op=True)
-        else:
-            grad_input = None
-        if ctx.needs_input_grad[1]:
-            assert ctx.compute_weight_gradient
-            if process_group is not None and sequence_parallel:
-                handle_x.wait()
-            grad_weight, grad_bias = fused_dense_cuda.linear_bias_wgrad(
-                total_x.reshape(batch_dim, total_x.shape[-1]), grad_output, ctx.needs_input_grad[2]
-            )
-        else:
-            grad_weight = None
-            grad_bias = grad_output if ctx.needs_input_grad[2] else None
-        if process_group is not None and ctx.needs_input_grad[0]:
-            handle_grad_input.wait()
-        return grad_input, grad_weight, grad_bias, None, None, None
-
-
-def fused_dense_func(
-    x: Tensor,
-    weight: Tensor,
-    bias: Optional[Tensor] = None,
-    return_residual: bool = False,
-    process_group: Optional[ProcessGroup] = None,
-    sequence_parallel: bool = True,
-):
-    dtype_eligible = x.dtype in [torch.float16, torch.bfloat16] or (
-        x.dtype == torch.float32 and torch.is_autocast_enabled()
-    )
-    if x.is_cuda and weight.is_cuda and (bias is None or bias.is_cuda) and dtype_eligible:
-        return FusedDenseFunc.apply(
-            x, weight, bias, return_residual, process_group, sequence_parallel
-        )
-    else:
-        assert process_group is None
-        out = F.linear(x, weight, bias)
-        return out if not return_residual else (out, x)
-
-
-class FusedDense(nn.Linear):
-    def __init__(
-        self,
-        in_features: int,
-        out_features: int,
-        bias: bool = True,
-        return_residual: bool = False,
-        device=None,
-        dtype=None,
-    ) -> None:
-        super().__init__(in_features, out_features, bias=bias, device=device, dtype=dtype)
-        self.return_residual = return_residual
-
-    def forward(self, x, process_group=None):
-        """
-        If process_group is not None, we're doing Tensor Parallel with sequence parallelism:
-        we do an all_gather of x before doing the matmul.
-        """
-        return fused_dense_func(
-            x,
-            self.weight,
-            self.bias,
-            return_residual=self.return_residual,
-            process_group=process_group,
-        )
-
-
-class ColumnParallelLinear(nn.Linear):
-    def __init__(
-        self,
-        in_features: int,
-        out_features: int,
-        process_group: ProcessGroup,
-        bias: bool = True,
-        sequence_parallel=True,
-        multiple_of=1,
-        device=None,
-        dtype=None,
-    ) -> None:
-        world_size = torch.distributed.get_world_size(process_group)
-        if out_features % multiple_of:
-            raise ValueError(f"out_features ({out_features}) must be a multiple of {multiple_of}")
-        multiple = out_features // multiple_of
-        # We want to split @multiple across world_size, but it could be an uneven split
-        div = multiple // world_size
-        mod = multiple % world_size
-        # The first @mod ranks get @div + 1 copies, the rest get @div copies
-        local_multiple = div + int(torch.distributed.get_rank(process_group) < mod)
-        super().__init__(
-            in_features, local_multiple * multiple_of, bias=bias, device=device, dtype=dtype
-        )
-        self.process_group = process_group
-        self.sequence_parallel = sequence_parallel
-
-    def forward(self, x):
-        # If self.sequence_parallel is True, we're doing Tensor Parallel with sequence parallelism:
-        # we do an all_gather of x before doing the matmul.
-        # If not, then the input is already gathered.
-        return fused_dense_func(
-            x,
-            self.weight,
-            self.bias,
-            process_group=self.process_group,
-            sequence_parallel=self.sequence_parallel,
-        )
-
-
-class RowParallelLinear(nn.Linear):
-    def __init__(
-        self,
-        in_features: int,
-        out_features: int,
-        process_group: ProcessGroup,
-        bias: bool = True,
-        sequence_parallel=True,
-        multiple_of=1,
-        device=None,
-        dtype=None,
-    ) -> None:
-        world_size = torch.distributed.get_world_size(process_group)
-        rank = torch.distributed.get_rank(process_group)
-        if in_features % multiple_of:
-            raise ValueError(f"in_features ({in_features}) must be a multiple of {multiple_of}")
-        multiple = in_features // multiple_of
-        # We want to split @multiple across world_size, but it could be an uneven split
-        div = multiple // world_size
-        mod = multiple % world_size
-        # The first @mod ranks get @div + 1 copies, the rest get @div copies
-        local_multiple = div + int(torch.distributed.get_rank(process_group) < mod)
-        # Only rank 0 will have bias
-        super().__init__(
-            local_multiple * multiple_of,
-            out_features,
-            bias=bias and rank == 0,
-            device=device,
-            dtype=dtype,
-        )
-        self.process_group = process_group
-        self.sequence_parallel = sequence_parallel
-
-    def forward(self, x):
-        """
-        We're doing Tensor Parallel with sequence parallelism: we do the matmul and then
-        a reduce_scatter of the result.
-        """
-        out = fused_dense_func(x, self.weight, self.bias)
-        reduce_fn = reduce_scatter if self.sequence_parallel else all_reduce
-        return reduce_fn(out, self.process_group)
-
-
-class FusedMLPFunc(torch.autograd.Function):
-    @staticmethod
-    @custom_fwd
-    def forward(
-        ctx,
-        x,
-        weight1,
-        bias1,
-        weight2,
-        bias2,
-        activation="gelu_approx",
-        save_pre_act=True,
-        return_residual=False,
-        checkpoint_lvl=0,
-        heuristic=0,
-        process_group=None,
-        sequence_parallel=True,
-    ):
-        """
-        If process_group is not None and sequence_parallel=True, we're doing Tensor Parallel
-        with sequence parallelism: we do an all_gather of x before doing the matmul.
-        If sequence_parallel=False, then the input is already gathered.
-
-        checkpoint_lvl:
-        0: no recomputation in the bwd
-        1: recompute gelu_out / relu_out in the bwd
-        2: recompute pre_act and gelu_out / relu_out in the bwd
-        """
-        assert -1 <= heuristic <= 4
-        assert activation in ["gelu_approx", "relu", "sqrelu"]
-        if activation == "sqrelu":
-            assert heuristic == -1
-        if not save_pre_act:
-            checkpoint_lvl = 2
-        assert checkpoint_lvl in [0, 1, 2]
-        ctx.return_residual = return_residual
-        ctx.process_group = process_group
-        ctx.sequence_parallel = sequence_parallel
-        ctx.checkpoint_lvl = checkpoint_lvl
-        ctx.activation = activation
-        ctx.heuristic = heuristic
-
-        if torch.is_autocast_enabled():
-            x = x.to(dtype=torch.get_autocast_gpu_dtype())
-        x = x.contiguous()
-        if process_group is not None and sequence_parallel:
-            # We want to kick off the all_gather early, before weight dtype conversion
-            total_x, handle_x = all_gather_raw(x, process_group, async_op=True)
-        else:
-            total_x = x
-
-        if torch.is_autocast_enabled():
-            dtype = torch.get_autocast_gpu_dtype()
-            weight1, weight2 = [a.to(dtype=dtype) for a in [weight1, weight2]]
-            bias1 = bias1.to(dtype=dtype) if bias1 is not None else None
-            bias2 = bias2.to(dtype=dtype) if bias2 is not None else None
-        weight1 = weight1.contiguous()
-        bias1 = bias1.contiguous() if bias1 is not None else None
-        weight2 = weight2.contiguous()
-        bias2 = bias2.contiguous() if bias2 is not None else None
-        if process_group is not None and sequence_parallel:
-            handle_x.wait()
-        batch_shape, n = total_x.shape[:-1], total_x.shape[-1]
-        batch_dim = batch_shape.numel()
-        # https://github.com/pytorch/pytorch/blob/5b51849b48a7dbccd297286cc0110def4706f9e7/aten/src/ATen/native/cuda/Blas.cpp#L174
-        if min(batch_dim, n, *weight1.shape, *weight2.shape) > 65535 * 32:
-            raise RuntimeError("fused_dense only supports matrix dims <= 2M")
-        if heuristic == -1:
-            pre_act = F.linear(total_x, weight1, bias1)
-            activation_fn = (
-                partial(F.gelu, approximate="tanh")
-                if activation == "gelu_approx"
-                else (sqrelu_fwd if activation == "sqrelu" else F.relu)
-            )
-            with torch.jit.fuser("fuser2"):
-                output1 = activation_fn(pre_act)
-            # This is before adding bias1
-            # pre_act = F.linear(total_x.reshape(batch_dim, n), weight1)
-            # with torch.jit.fuser('fuser2'):
-            #     output1 = bias_gelu(pre_act, bias1)
-        else:
-            is_gelu = activation == "gelu_approx"
-            output1, *rest = fused_dense_cuda.linear_act_forward(
-                total_x.reshape(batch_dim, n), weight1, bias1, is_gelu, save_pre_act, heuristic
-            )
-            if save_pre_act:
-                pre_act = rest[0]
-        output2 = F.linear(output1, weight2, bias2)
-        if checkpoint_lvl == 0 or (checkpoint_lvl == 1 and activation == "relu"):
-            # For RELU the pre_act is very small (just a bit-mask) so we just save it
-            ctx.save_for_backward(x, weight1, weight2, pre_act, output1)
-        elif checkpoint_lvl == 1:
-            ctx.save_for_backward(x, weight1, weight2, pre_act)
-        elif checkpoint_lvl == 2:
-            ctx.save_for_backward(x, weight1, weight2, bias1)
-        output2 = output2.reshape(*batch_shape, output2.shape[-1])
-        return output2 if not return_residual else (output2, x)
-
-    @staticmethod
-    @custom_bwd
-    def backward(ctx, grad_output, *args):
-        grad_output = grad_output.contiguous()
-        checkpoint_lvl = ctx.checkpoint_lvl
-        activation = ctx.activation
-        activation_fn = (
-            partial(F.gelu, approximate="tanh")
-            if activation == "gelu_approx"
-            else (sqrelu_fwd if activation == "sqrelu" else F.relu)
-        )
-        if ctx.return_residual:
-            (grad_input,) = args
-            grad_input = grad_input.contiguous()
-        process_group = ctx.process_group
-        sequence_parallel = ctx.sequence_parallel
-        x, weight1, weight2, *rest = ctx.saved_tensors
-        if process_group is None or not sequence_parallel:
-            total_x = x
-        batch_shape = grad_output.shape[:-1]
-        batch_dim = batch_shape.numel()
-        if checkpoint_lvl in [0, 1]:
-            if process_group is not None and sequence_parallel:
-                total_x, handle_x = all_gather_raw(x, process_group, async_op=True)
-            if checkpoint_lvl == 0 or (checkpoint_lvl == 1 and activation == "relu"):
-                pre_act, output1 = rest
-            elif checkpoint_lvl == 1:
-                (pre_act,) = rest
-                with torch.jit.fuser("fuser2"):
-                    output1 = activation_fn(pre_act)
-        elif checkpoint_lvl == 2:
-            (bias1,) = rest
-            if process_group is not None and sequence_parallel:
-                total_x, _ = all_gather_raw(x, process_group)
-            if ctx.heuristic == -1:
-                pre_act = F.linear(total_x, weight1, bias1)
-                with torch.jit.fuser("fuser2"):
-                    output1 = activation_fn(pre_act)
-            else:
-                output1, pre_act = fused_dense_cuda.linear_act_forward(
-                    total_x.reshape(batch_dim, total_x.shape[-1]),
-                    weight1,
-                    bias1,
-                    activation == "gelu_approx",
-                    True,
-                    ctx.heuristic,
-                )
-
-        grad_output = grad_output.reshape(batch_dim, grad_output.shape[-1])
-        output1 = output1.reshape(batch_dim, output1.shape[-1])
-        pre_act = pre_act.reshape(batch_dim, pre_act.shape[-1])
-        if ctx.needs_input_grad[3]:
-            grad_weight2, grad_bias2 = fused_dense_cuda.linear_bias_wgrad(
-                output1, grad_output, ctx.needs_input_grad[4]
-            )
-        else:
-            grad_weight2 = None
-            grad_bias2 = grad_output if ctx.needs_input_grad[4] else None
-        if ctx.heuristic == -1:
-            # grad_pre_act = matmul_dgelu(grad_output, weight2, pre_act)
-            grad_output1 = F.linear(grad_output, weight2.t())
-            activation_grad_fn = (
-                gelu_bwd
-                if activation == "gelu_approx"
-                else (sqrelu_bwd if activation == "sqrelu" else relu_bwd)
-            )
-            with torch.jit.fuser("fuser2"):
-                grad_pre_act = activation_grad_fn(grad_output1, pre_act)
-        else:
-            # The cublasLt epilogue has to compute both gelu/relu grad and bias grad, we can't
-            # just compute gelu/relu grad
-            grad_pre_act, grad_bias1 = fused_dense_cuda.bias_act_linear_dgrad_bgrad(
-                weight2, grad_output, pre_act, activation == "gelu_approx", ctx.heuristic
-            )
-            if not ctx.needs_input_grad[2]:
-                grad_bias1 = None
-        if ctx.needs_input_grad[0]:
-            if not ctx.return_residual:
-                grad_input = F.linear(grad_pre_act, weight1.t())
-            else:
-                grad_input = torch.addmm(
-                    grad_input.reshape(batch_dim, grad_input.shape[-1]), grad_pre_act, weight1
-                )
-            grad_input = grad_input.reshape(*batch_shape, grad_input.shape[-1])
-            if process_group is not None:
-                reduce_fn = reduce_scatter_raw if sequence_parallel else all_reduce_raw
-                grad_input, handle_grad_input = reduce_fn(grad_input, process_group, async_op=True)
-        else:
-            grad_input = None
-        if ctx.heuristic == -1:
-            if ctx.needs_input_grad[1]:
-                if process_group is not None and sequence_parallel and checkpoint_lvl != 2:
-                    handle_x.wait()
-                grad_weight1, grad_bias1 = fused_dense_cuda.linear_bias_wgrad(
-                    total_x.reshape(batch_dim, total_x.shape[-1]),
-                    grad_pre_act,
-                    ctx.needs_input_grad[2],
-                )
-            else:
-                grad_weight1 = None
-                grad_bias1 = grad_pre_act if ctx.needs_input_grad[2] else None
-        else:
-            if ctx.needs_input_grad[1]:
-                if process_group is not None and sequence_parallel and checkpoint_lvl != 2:
-                    handle_x.wait()
-                grad_weight1 = F.linear(
-                    grad_pre_act.t(), total_x.reshape(batch_dim, total_x.shape[-1]).t()
-                )
-            else:
-                grad_weight1 = None
-        if process_group is not None and ctx.needs_input_grad[0]:
-            handle_grad_input.wait()
-        return (
-            grad_input,
-            grad_weight1,
-            grad_bias1,
-            grad_weight2,
-            grad_bias2,
-            None,
-            None,
-            None,
-            None,
-            None,
-            None,
-            None,
-        )
-
-
-def fused_mlp_func(
-    x: Tensor,
-    weight1: Tensor,
-    weight2: Tensor,
-    bias1: Optional[Tensor] = None,
-    bias2: Optional[Tensor] = None,
-    activation: str = "gelu_approx",
-    save_pre_act: bool = True,
-    return_residual: bool = False,
-    checkpoint_lvl: int = 0,
-    heuristic: int = 0,
-    process_group: Optional[ProcessGroup] = None,
-    sequence_parallel: bool = True,
-):
-    assert activation in ["gelu_approx", "relu", "sqrelu"]
-    dtype_eligible = x.dtype in [torch.float16, torch.bfloat16] or (
-        x.dtype == torch.float32 and torch.is_autocast_enabled()
-    )
-    # If we save pre-activation, dimension must be divisible by 128 (relu) or 8 (gelu)
-    dim_eligible = not save_pre_act or (x.shape[-1] % (128 if activation == "relu" else 8) == 0)
-    if (
-        x.is_cuda
-        and weight1.is_cuda
-        and weight2.is_cuda
-        and (bias1 is None or bias1.is_cuda)
-        and (bias2 is None or bias2.is_cuda)
-        and dtype_eligible
-        and dim_eligible
-    ):
-        return FusedMLPFunc.apply(
-            x,
-            weight1,
-            bias1,
-            weight2,
-            bias2,
-            activation,
-            save_pre_act,
-            return_residual,
-            checkpoint_lvl,
-            heuristic,
-            process_group,
-            sequence_parallel,
-        )
-    else:
-        assert process_group is None
-        pre_act = F.linear(x, weight1, bias1)
-        activation_fn = (
-            partial(F.gelu, approximate="tanh")
-            if activation == "gelu_approx"
-            else partial(F.relu, inplace=True)
-        )
-        output1 = activation_fn(pre_act)
-        output2 = F.linear(output1, weight2, bias2)
-        return output2 if not return_residual else (output2, x)
-
-
-class FusedMLP(nn.Module):
-    def __init__(
-        self,
-        in_features,
-        hidden_features=None,
-        out_features=None,
-        bias1=True,
-        bias2=True,
-        activation="gelu_approx",
-        return_residual=False,
-        checkpoint_lvl=0,
-        heuristic="auto",
-        device=None,
-        dtype=None,
-    ):
-        """
-        If process_group is not None, we're doing Tensor Parallel with sequence parallelism:
-        we do an all_gather of x before doing the matmul, gelu, then matmul.
-        Finally we do a reduce_scatter of the output.
-
-        checkpoint_lvl (increasing lvl means slower but more memory saving):
-            0: no recomputation in the bwd
-            1: recompute gelu_out in the bwd
-            2: recompute pre_act and gelu_out in the bwd
-        heuristic:
-            -1: don't fuse gemm + gelu (separate kernel)
-            0..4: use this heuristic for the algo section in the fused gemm + gelu
-            'auto': heuristic will be picked automatically:
-                For CUDA >= 11.8, we set heuristic=0 for both fp16 and bf16 for best perf.
-                For CUDA <= 11.7, we set heuristic=1 for fp16 and heuristic=-1 for bf16.
-                For H100, we set heuristic=-1 for both fp16 and bf16 as the fused cuBlasLt implementation
-                is slower than the unfused version.
-        return_residual: whether to return the input x along with the output. This is for
-            performance reason: for post-norm architecture, returning the input allows us
-            to fuse the backward of nn.Linear with the residual connection.
-        """
-        assert checkpoint_lvl in [0, 1, 2]
-        assert activation in ["gelu_approx", "relu", "sqrelu"]
-        factory_kwargs = {"device": device, "dtype": dtype}
-        super().__init__()
-        out_features = out_features or in_features
-        hidden_features = hidden_features or in_features * 4
-        self.activation = activation
-        self.return_residual = return_residual
-        self.checkpoint_lvl = checkpoint_lvl
-        self.heuristic = heuristic if activation != "sqrelu" else -1
-        self.fc1 = nn.Linear(in_features, hidden_features, bias=bias1, **factory_kwargs)
-        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias2, **factory_kwargs)
-
-    def forward(self, x, process_group=None):
-        dtype = x.dtype if not torch.is_autocast_enabled() else torch.get_autocast_gpu_dtype()
-        if self.heuristic == "auto":
-            if self.activation == "gelu_approx":
-                if torch.cuda.get_device_capability("cuda") == (9, 0):
-                    heuristic = -1
-                else:
-                    cuda_ver = tuple(map(int, torch.version.cuda.split(".")))
-                    heuristic = 0 if cuda_ver >= (11, 8) else (1 if dtype == torch.float16 else -1)
-            else:
-                heuristic = 0
-        else:
-            heuristic = self.heuristic
-        out = fused_mlp_func(
-            x,
-            self.fc1.weight,
-            self.fc2.weight,
-            self.fc1.bias,
-            self.fc2.bias,
-            activation=self.activation,
-            save_pre_act=self.training,
-            return_residual=self.return_residual,
-            checkpoint_lvl=self.checkpoint_lvl,
-            heuristic=heuristic,
-            process_group=process_group,
-        )
-        if self.return_residual:
-            out, x = out
-        if process_group is not None:
-            out = reduce_scatter(out, process_group)
-        return out if not self.return_residual else (out, x)
-
-
-class ParallelFusedMLP(nn.Module):
-    def __init__(
-        self,
-        in_features,
-        hidden_features=None,
-        out_features=None,
-        activation="gelu_approx",
-        process_group: ProcessGroup = None,
-        bias1=True,
-        bias2=True,
-        sequence_parallel=True,
-        checkpoint_lvl=0,
-        heuristic="auto",
-        device=None,
-        dtype=None,
-    ):
-        """
-        process_group is required. We're doing Tensor Parallel with sequence parallelism:
-        we do an all_gather of x before doing the matmul, gelu, then matmul.
-        Finally we do a reduce_scatter of the output.
-
-        checkpoint_lvl (increasing lvl means slower but more memory saving):
-            0: no recomputation in the bwd
-            1: recompute gelu_out in the bwd
-            2: recompute pre_act and gelu_out in the bwd
-        heuristic:
-            -1: don't fuse gemm + gelu (separate kernel)
-            0..4: use this heuristic for the algo section in the fused gemm + gelu
-            'auto': heuristic will be picked automatically:
-                For CUDA >= 11.8, we set heuristic=0 for both fp16 and bf16 for best perf.
-                For CUDA <= 11.7, we set heuristic=1 for fp16 and heuristic=-1 for bf16.
-        """
-        assert checkpoint_lvl in [0, 1, 2]
-        assert activation in ["gelu_approx", "relu", "sqrelu"]
-        assert process_group is not None
-        factory_kwargs = {"device": device, "dtype": dtype}
-        super().__init__()
-        out_features = out_features or in_features
-        hidden_features = hidden_features or in_features * 4
-        self.activation = activation
-        self.process_group = process_group
-        self.sequence_parallel = sequence_parallel
-        self.checkpoint_lvl = checkpoint_lvl
-        self.heuristic = heuristic if activation != "sqrelu" else -1
-        self.fc1 = ColumnParallelLinear(
-            in_features, hidden_features, process_group, bias=bias1, **factory_kwargs
-        )
-        self.fc2 = RowParallelLinear(
-            hidden_features, out_features, process_group, bias=bias2, **factory_kwargs
-        )
-
-    def forward(self, x):
-        dtype = x.dtype if not torch.is_autocast_enabled() else torch.get_autocast_gpu_dtype()
-        if self.heuristic == "auto":
-            if self.activation == "gelu_approx":
-                cuda_ver = tuple(map(int, torch.version.cuda.split(".")))
-                heuristic = 0 if cuda_ver >= (11, 8) else (1 if dtype == torch.float16 else -1)
-            else:
-                heuristic = 0
-        else:
-            heuristic = self.heuristic
-        out = fused_mlp_func(
-            x,
-            self.fc1.weight,
-            self.fc2.weight,
-            self.fc1.bias,
-            self.fc2.bias,
-            activation=self.activation,
-            save_pre_act=self.training,
-            checkpoint_lvl=self.checkpoint_lvl,
-            heuristic=heuristic,
-            process_group=self.process_group,
-            sequence_parallel=self.sequence_parallel,
-        )
-        reduce_fn = reduce_scatter if self.sequence_parallel else all_reduce
-        return reduce_fn(out, self.process_group)
diff --git a/based/ops/layer_norm.py b/based/ops/layer_norm.py
deleted file mode 100644
index 4b6cd79..0000000
--- a/based/ops/layer_norm.py
+++ /dev/null
@@ -1,800 +0,0 @@
-# Copyright (c) 2022, Tri Dao.
-# Adapted from https://github.com/NVIDIA/apex/blob/master/apex/contrib/layer_norm/layer_norm.py
-
-import dropout_layer_norm
-import torch
-from torch.nn import init
-
-
-def maybe_align(x, alignment_in_bytes=16):
-    """Assume that x already has last dim divisible by alignment_in_bytes"""
-    # TD [2023-07-04] I'm not 100% sure that clone will align the memory
-    # https://discuss.pytorch.org/t/how-to-ensure-that-tensor-data-ptr-is-aligned-to-16-bytes/183440
-    return x if x.data_ptr() % alignment_in_bytes == 0 else x.clone()
-
-
-def _dropout_add_layer_norm_forward(
-    x0,
-    residual,
-    gamma,
-    beta,
-    rowscale,
-    colscale,
-    dropout_p,
-    epsilon,
-    residual_in_fp32=False,
-    is_rms_norm=False,
-):
-    """Assume that arguments are contiguous and aligned to 16 bytes"""
-    hidden_size = gamma.numel()
-    x0mat = x0.view((-1, hidden_size))
-    residualmat = residual.view((-1, hidden_size)) if residual is not None else None
-    rowscale = rowscale.view(-1) if rowscale is not None else None
-    zmat, xmat, dmask, mu, rsigma = dropout_layer_norm.dropout_add_ln_fwd(
-        x0mat,
-        residualmat,
-        gamma,
-        beta,
-        rowscale,
-        colscale,
-        None,
-        None,
-        dropout_p,
-        epsilon,
-        1.0,
-        0,
-        None,
-        residual_in_fp32,
-        is_rms_norm,
-    )
-    # dmask is None if dropout_p == 0.0
-    # xmat is None if dropout_p == 0.0 and residual is None and residual_dtype != input_dtype
-    return zmat, xmat if xmat is not None else x0mat, dmask, mu, rsigma
-
-
-def _dropout_add_layer_norm_backward(
-    dz,
-    dx,
-    x,
-    x0,
-    dmask,
-    mu,
-    rsigma,
-    gamma,
-    rowscale,
-    colscale,
-    dropout_p,
-    has_residual,
-    is_rms_norm=False,
-):
-    """Assume that arguments are contiguous and aligned to 16 bytes
-    dx == None means that it was a post-norm architecture
-    (x = drop(x0) + residual was not returned in the fwd).
-    x0 must not be None if we have colscale.
-    """
-    hidden_size = gamma.numel()
-    xmat = x.view((-1, hidden_size))
-    dzmat = dz.view(xmat.shape)
-    dxmat = dx.view(xmat.shape) if dx is not None else None
-    x0mat = x0.view((-1, hidden_size)) if x0 is not None else None
-    rowscale = rowscale.view(-1) if rowscale is not None else None
-    if colscale is not None:
-        assert x0 is not None, "x0 is required to compute the gradient of colscale"
-    dx0mat, dresidualmat, dgamma, dbeta, _, _, *rest = dropout_layer_norm.dropout_add_ln_bwd(
-        dzmat,
-        dxmat,
-        xmat,
-        x0mat,
-        dmask,
-        mu,
-        rsigma,
-        gamma,
-        rowscale,
-        colscale,
-        None,
-        None,
-        dropout_p,
-        1.0,
-        0,
-        has_residual,
-        is_rms_norm,
-    )
-    # dresidualmat is None if not has_residual
-    if colscale is None:
-        return dx0mat, dresidualmat, dgamma, dbeta
-    else:
-        dcolscale = rest[0]
-        return dx0mat, dresidualmat, dgamma, dbeta, dcolscale
-
-
-def _dropout_add_layer_norm_subset_forward(
-    x0,
-    residual,
-    gamma,
-    beta,
-    colscale,
-    x0_subset,
-    out_subset,
-    dropout_p,
-    epsilon,
-    rowscale_const,
-    out_numrows,
-    residual_in_fp32=False,
-    is_rms_norm=False,
-):
-    """Assume that arguments are contiguous and aligned to 16 bytes"""
-    hidden_size = gamma.numel()
-    x0mat = x0.view((-1, hidden_size))
-    residualmat = residual.view((-1, hidden_size)) if residual is not None else None
-    x0_subset = x0_subset.view(-1) if x0_subset is not None else None
-    out_subset = out_subset.view(-1) if out_subset is not None else None
-    zmat, xmat, dmask, mu, rsigma = dropout_layer_norm.dropout_add_ln_fwd(
-        x0mat,
-        residualmat,
-        gamma,
-        beta,
-        None,
-        colscale,
-        x0_subset,
-        out_subset,
-        dropout_p,
-        epsilon,
-        rowscale_const,
-        out_numrows,
-        None,
-        residual_in_fp32,
-        is_rms_norm,
-    )
-    # dmask is None if dropout_p == 0.0
-    # xmat is None if dropout_p == 0.0 and residual is None and residual_dtype != input_dtype
-    return zmat, xmat if xmat is not None else x0mat, dmask, mu, rsigma
-
-
-def _dropout_add_layer_norm_subset_backward(
-    dz,
-    dx,
-    x,
-    x0,
-    dmask,
-    mu,
-    rsigma,
-    gamma,
-    colscale,
-    x0_subset,
-    out_subset,
-    dropout_p,
-    rowscale_const,
-    x0_numrows,
-    has_residual,
-    is_rms_norm=False,
-):
-    """Assume that arguments are contiguous and aligned to 16 bytes
-    dx == None means that it was a post-norm architecture
-    (x = drop(x0) + residual was not returned in the fwd).
-    x0 must not be None if we have colscale.
-    """
-    hidden_size = gamma.numel()
-    xmat = x.view((-1, hidden_size))
-    dzmat = dz.view(-1, hidden_size)
-    dxmat = dx.view(xmat.shape) if dx is not None else None
-    x0mat = x0.view((-1, hidden_size)) if x0 is not None else None
-    x0_subset = x0_subset.view(-1) if x0_subset is not None else None
-    out_subset = out_subset.view(-1) if out_subset is not None else None
-    if colscale is not None:
-        assert x0 is not None, "x0 is required to compute the gradient of colscale"
-    dx0mat, dresidualmat, dgamma, dbeta, _, _, *rest = dropout_layer_norm.dropout_add_ln_bwd(
-        dzmat,
-        dxmat,
-        xmat,
-        x0mat,
-        dmask,
-        mu,
-        rsigma,
-        gamma,
-        None,
-        colscale,
-        x0_subset,
-        out_subset,
-        dropout_p,
-        rowscale_const,
-        x0_numrows,
-        has_residual,
-        is_rms_norm,
-    )
-    # dresidualmat is None if not has_residual
-    if colscale is None:
-        return dx0mat, dresidualmat, dgamma, dbeta
-    else:
-        dcolscale = rest[0]
-        return dx0mat, dresidualmat, dgamma, dbeta, dcolscale
-
-
-def _dropout_add_layer_norm_parallel_residual_forward(
-    x0,
-    x1,
-    residual,
-    gamma0,
-    beta0,
-    gamma1,
-    beta1,
-    dropout_p,
-    epsilon,
-    residual_in_fp32=False,
-    is_rms_norm=False,
-):
-    """Assume that arguments are contiguous and aligned to 16 bytes"""
-    hidden_size = gamma0.numel()
-    x0mat = x0.view((-1, hidden_size))
-    x1mat = x1.view((-1, hidden_size)) if x1 is not None else None
-    residualmat = residual.view((-1, hidden_size)) if residual is not None else None
-    (
-        z0mat,
-        z1mat,
-        xmat,
-        dmask0,
-        dmask1,
-        mu,
-        rsigma,
-    ) = dropout_layer_norm.dropout_add_ln_parallel_residual_fwd(
-        x0mat,
-        x1mat,
-        residualmat,
-        gamma0,
-        beta0,
-        gamma1,
-        beta1,
-        dropout_p,
-        epsilon,
-        None,
-        residual_in_fp32,
-        is_rms_norm,
-    )
-    # dmask0 and dmask1 are None if dropout_p == 0.0
-    # xmat is None if dropout_p == 0.0 and residual is None and residual_dtype != input_dtype
-    return z0mat, z1mat, xmat if xmat is not None else x0mat, dmask0, dmask1, mu, rsigma
-
-
-def _dropout_add_layer_norm_parallel_residual_backward(
-    dz0,
-    dz1,
-    dx,
-    x,
-    dmask0,
-    dmask1,
-    mu,
-    rsigma,
-    gamma0,
-    gamma1,
-    dropout_p,
-    has_x1,
-    has_residual,
-    is_rms_norm=False,
-):
-    """Assume that arguments are contiguous and aligned to 16 bytes
-    dx == None means that it was a post-norm architecture
-    (x = drop(x0) + residual was not returned in the fwd).
-    """
-    hidden_size = gamma0.numel()
-    xmat = x.view((-1, hidden_size))
-    dz0mat = dz0.view(xmat.shape)
-    dz1mat = dz1.view(xmat.shape) if dz1 is not None else None
-    dxmat = dx.view(xmat.shape) if dx is not None else None
-    (
-        dx0mat,
-        dx1mat,
-        dresidualmat,
-        dgamma0,
-        dbeta0,
-        dgamma1,
-        dbeta1,
-        *rest,
-    ) = dropout_layer_norm.dropout_add_ln_parallel_residual_bwd(
-        dz0mat,
-        dz1mat,
-        dxmat,
-        xmat,
-        dmask0,
-        dmask1,
-        mu,
-        rsigma,
-        gamma0,
-        gamma1,
-        dropout_p,
-        has_x1,
-        has_residual,
-        is_rms_norm,
-    )
-    # dresidualmat is None if not has_residual
-    return dx0mat, dx1mat, dresidualmat, dgamma0, dbeta0, dgamma1, dbeta1
-
-
-class DropoutAddLayerNormFn(torch.autograd.Function):
-    @staticmethod
-    def forward(
-        ctx,
-        x0,
-        residual,
-        gamma,
-        beta,
-        rowscale,
-        colscale,
-        dropout_p,
-        epsilon,
-        residual_in_fp32=False,
-        prenorm=False,
-        is_rms_norm=False,
-        return_dmask=False,
-    ):
-        x0 = maybe_align(x0.contiguous(), 16)
-        residual = maybe_align(residual.contiguous(), 16) if residual is not None else None
-        gamma = maybe_align(gamma.contiguous(), 16)
-        beta = maybe_align(beta.contiguous(), 16) if beta is not None else None
-        rowscale = maybe_align(rowscale.contiguous(), 16) if rowscale is not None else None
-        colscale = maybe_align(colscale.contiguous(), 16) if colscale is not None else None
-        zmat, xmat, dmask, mu, rsigma = _dropout_add_layer_norm_forward(
-            x0,
-            residual,
-            gamma,
-            beta,
-            rowscale,
-            colscale,
-            dropout_p,
-            epsilon,
-            residual_in_fp32,
-            is_rms_norm,
-        )
-        # Only need to save x0 if we need to compute gradient wrt colscale
-        x0_saved = x0 if colscale is not None else None
-        ctx.save_for_backward(
-            xmat.view(x0.shape), x0_saved, dmask, gamma, mu, rsigma, rowscale, colscale
-        )
-        ctx.prenorm = prenorm
-        ctx.dropout_p = dropout_p
-        ctx.has_residual = residual is not None
-        ctx.is_rms_norm = is_rms_norm
-        ctx.has_beta = beta is not None
-        if not return_dmask:
-            return (
-                zmat.view(x0.shape) if not prenorm else (zmat.view(x0.shape), xmat.view(x0.shape))
-            )
-        else:
-            dmask = (
-                dmask.view(x0.shape)
-                if dropout_p > 0.0
-                else torch.ones(x0.shape, dtype=torch.uint8, device=x0.device)
-            )
-            ctx.mark_non_differentiable(dmask)
-            return (
-                (zmat.view(x0.shape), dmask)
-                if not prenorm
-                else (zmat.view(x0.shape), xmat.view(x0.shape), dmask)
-            )
-
-    @staticmethod
-    def backward(ctx, dz, *args):
-        # assert dz.is_contiguous()
-        dz = maybe_align(dz.contiguous(), 16)  # this happens!
-        dx = maybe_align(args[0].contiguous(), 16) if ctx.prenorm else None
-        x, x0, dmask, gamma, mu, rsigma, rowscale, colscale = ctx.saved_tensors
-        # x0 is None if colscale is None
-        dropout_p = ctx.dropout_p
-        has_residual = ctx.has_residual
-        dx0mat, dresidualmat, dgamma, dbeta, *rest = _dropout_add_layer_norm_backward(
-            dz,
-            dx,
-            x,
-            x0,
-            dmask,
-            mu,
-            rsigma,
-            gamma,
-            rowscale,
-            colscale,
-            dropout_p,
-            has_residual,
-            ctx.is_rms_norm,
-        )
-        dx0 = dx0mat.view(x.shape)
-        dresidual = dresidualmat.view(x.shape) if dresidualmat is not None else None
-        dcolscale = rest[0] if colscale is not None else None
-        return (
-            dx0,
-            dresidual,
-            dgamma,
-            dbeta if ctx.has_beta else None,
-            None,
-            dcolscale,
-            None,
-            None,
-            None,
-            None,
-            None,
-            None,
-        )
-
-
-class DropoutAddLayerNormSubsetFn(torch.autograd.Function):
-    @staticmethod
-    def forward(
-        ctx,
-        x0,
-        residual,
-        gamma,
-        beta,
-        colscale,
-        x0_subset,
-        out_subset,
-        dropout_p,
-        epsilon,
-        rowscale_const,
-        out_numrows,
-        residual_in_fp32=False,
-        prenorm=False,
-        is_rms_norm=False,
-        return_dmask=False,
-    ):
-        x0 = maybe_align(x0.contiguous(), 16)
-        residual = maybe_align(residual.contiguous(), 16) if residual is not None else None
-        gamma = maybe_align(gamma.contiguous(), 16)
-        beta = maybe_align(beta.contiguous(), 16) if beta is not None else None
-        colscale = maybe_align(colscale.contiguous(), 16) if colscale is not None else None
-        zmat, xmat, dmask, mu, rsigma = _dropout_add_layer_norm_subset_forward(
-            x0,
-            residual,
-            gamma,
-            beta,
-            colscale,
-            x0_subset,
-            out_subset,
-            dropout_p,
-            epsilon,
-            rowscale_const,
-            out_numrows,
-            residual_in_fp32,
-            is_rms_norm,
-        )
-        # Only need to save x0 if we need to compute gradient wrt colscale
-        x0_saved = x0 if colscale is not None else None
-        x_shape = (-1, *x0.shape[1:])
-        ctx.save_for_backward(
-            xmat.view(x_shape), x0_saved, dmask, gamma, mu, rsigma, colscale, x0_subset, out_subset
-        )
-        ctx.prenorm = prenorm
-        ctx.dropout_p = dropout_p
-        ctx.rowscale_const = rowscale_const
-        ctx.x0_numrows = x0.shape[:-1].numel()
-        ctx.has_residual = residual is not None
-        ctx.is_rms_norm = is_rms_norm
-        ctx.has_beta = beta is not None
-        z_shape = (-1, *x0.shape[1:])
-        if not return_dmask:
-            return zmat.view(z_shape) if not prenorm else (zmat.view(z_shape), xmat.view(x0.shape))
-        else:
-            z = zmat.view(z_shape)
-            dmask = (
-                dmask.view(x0.shape)
-                if dropout_p > 0.0
-                else torch.ones(x0.shape, dtype=torch.uint8, device=x0.device)
-            )
-            ctx.mark_non_differentiable(dmask)
-            return (z, dmask) if not prenorm else (z, xmat.view(x_shape), dmask)
-
-    @staticmethod
-    def backward(ctx, dz, *args):
-        # assert dz.is_contiguous()
-        dz = maybe_align(dz.contiguous(), 16)  # this happens!
-        dx = maybe_align(args[0].contiguous(), 16) if ctx.prenorm else None
-        x, x0, dmask, gamma, mu, rsigma, colscale, x0_subset, out_subset = ctx.saved_tensors
-        # x0 is None if colscale is None
-        dropout_p = ctx.dropout_p
-        has_residual = ctx.has_residual
-        dx0mat, dresidualmat, dgamma, dbeta, *rest = _dropout_add_layer_norm_subset_backward(
-            dz,
-            dx,
-            x,
-            x0,
-            dmask,
-            mu,
-            rsigma,
-            gamma,
-            colscale,
-            x0_subset,
-            out_subset,
-            dropout_p,
-            ctx.rowscale_const,
-            ctx.x0_numrows,
-            has_residual,
-            ctx.is_rms_norm,
-        )
-        dx0 = dx0mat.view(-1, *x.shape[1:])
-        dresidual = dresidualmat.view(x.shape) if dresidualmat is not None else None
-        dcolscale = rest[0] if colscale is not None else None
-        return (
-            dx0,
-            dresidual,
-            dgamma,
-            dbeta if ctx.has_beta else None,
-            dcolscale,
-            None,
-            None,
-            None,
-            None,
-            None,
-            None,
-            None,
-            None,
-            None,
-            None,
-        )
-
-
-class DropoutAddLayerNormParallelResidualFn(torch.autograd.Function):
-    @staticmethod
-    def forward(
-        ctx,
-        x0,
-        x1,
-        residual,
-        gamma0,
-        beta0,
-        gamma1,
-        beta1,
-        dropout_p,
-        epsilon,
-        residual_in_fp32=False,
-        prenorm=False,
-        is_rms_norm=False,
-        return_dmask=False,
-    ):
-        x0 = maybe_align(x0.contiguous(), 16)
-        x1 = maybe_align(x1.contiguous(), 16) if x1 is not None else None
-        residual = maybe_align(residual.contiguous(), 16) if residual is not None else None
-        gamma0 = maybe_align(gamma0.contiguous(), 16)
-        beta0 = maybe_align(beta0.contiguous(), 16) if beta0 is not None else None
-        gamma1 = maybe_align(gamma1.contiguous(), 16) if gamma1 is not None else None
-        beta1 = maybe_align(beta1.contiguous(), 16) if beta1 is not None else None
-        (
-            z0mat,
-            z1mat,
-            xmat,
-            dmask0,
-            dmask1,
-            mu,
-            rsigma,
-        ) = _dropout_add_layer_norm_parallel_residual_forward(
-            x0,
-            x1,
-            residual,
-            gamma0,
-            beta0,
-            gamma1,
-            beta1,
-            dropout_p,
-            epsilon,
-            residual_in_fp32,
-            is_rms_norm,
-        )
-        ctx.save_for_backward(xmat.view(x0.shape), dmask0, dmask1, gamma0, gamma1, mu, rsigma)
-        ctx.prenorm = prenorm
-        ctx.dropout_p = dropout_p
-        ctx.has_x1 = x1 is not None
-        ctx.has_residual = residual is not None
-        ctx.is_rms_norm = is_rms_norm
-        ctx.has_beta = beta0 is not None
-        z = (z0mat.view(x0.shape), z1mat.view(x0.shape) if z1mat is not None else None)
-        if not return_dmask:
-            return z if not prenorm else (*z, xmat.view(x0.shape))
-        else:
-            dmask0 = (
-                dmask0.view(x0.shape)
-                if dropout_p > 0.0
-                else torch.ones(x0.shape, dtype=torch.uint8, device=x0.device)
-            )
-            dmask1 = (
-                dmask1.view(x0.shape)
-                if dropout_p > 0.0 and x1 is not None
-                else torch.ones(x0.shape, dtype=torch.uint8, device=x0.device)
-            )
-            ctx.mark_non_differentiable(dmask0)
-            ctx.mark_non_differentiable(dmask1)
-            return (
-                (*z, dmask0, dmask1) if not prenorm else (*z, xmat.view(x0.shape), dmask0, dmask1)
-            )
-
-    @staticmethod
-    def backward(ctx, dz0, dz1, *args):
-        dz0 = maybe_align(dz0.contiguous(), 16)  # this happens!
-        dz1 = maybe_align(dz1.contiguous(), 16) if dz1 is not None else None
-        dx = maybe_align(args[0].contiguous(), 16) if ctx.prenorm else None
-        x, dmask0, dmask1, gamma0, gamma1, mu, rsigma = ctx.saved_tensors
-        dropout_p = ctx.dropout_p
-        has_x1 = ctx.has_x1
-        has_residual = ctx.has_residual
-        (
-            dx0mat,
-            dx1mat,
-            dresidualmat,
-            dgamma0,
-            dbeta0,
-            dgamma1,
-            dbeta1,
-        ) = _dropout_add_layer_norm_parallel_residual_backward(
-            dz0,
-            dz1,
-            dx,
-            x,
-            dmask0,
-            dmask1,
-            mu,
-            rsigma,
-            gamma0,
-            gamma1,
-            dropout_p,
-            has_x1,
-            has_residual,
-            ctx.is_rms_norm,
-        )
-        dx0 = dx0mat.view(x.shape)
-        dx1 = dx1mat.view(x.shape) if dx1mat is not None else None
-        dresidual = dresidualmat.view(x.shape) if dresidualmat is not None else None
-        return (
-            dx0,
-            dx1,
-            dresidual,
-            dgamma0,
-            dbeta0 if ctx.has_beta else None,
-            dgamma1,
-            dbeta1 if ctx.has_beta else None,
-            None,
-            None,
-            None,
-            None,
-            None,
-            None,
-        )
-
-
-def layer_norm(x, weight, bias, epsilon):
-    return DropoutAddLayerNormFn.apply(x, None, weight, bias, None, None, 0.0, epsilon, False)
-
-
-def dropout_add_layer_norm(
-    x0,
-    residual,
-    weight,
-    bias,
-    dropout_p,
-    epsilon,
-    rowscale=None,
-    layerscale=None,
-    prenorm=False,
-    residual_in_fp32=False,
-    return_dropout_mask=False,
-):
-    """residual_in_fp32 only has an effect if residual is None.
-    Otherwise residual dtype is residual.dtype.
-    """
-    return DropoutAddLayerNormFn.apply(
-        x0,
-        residual,
-        weight,
-        bias,
-        rowscale,
-        layerscale,
-        dropout_p,
-        epsilon,
-        residual_in_fp32,
-        prenorm,
-        False,
-        return_dropout_mask,
-    )
-
-
-def dropout_add_layer_norm_subset(
-    x0,
-    residual,
-    weight,
-    bias,
-    dropout_p,
-    epsilon,
-    layerscale=None,
-    x0_subset=None,
-    out_subset=None,
-    rowscale_const=1.0,
-    out_numrows=0,
-    prenorm=False,
-    residual_in_fp32=False,
-    return_dropout_mask=False,
-):
-    """residual_in_fp32 only has an effect if residual is None.
-    Otherwise residual dtype is residual.dtype.
-    """
-    return DropoutAddLayerNormSubsetFn.apply(
-        x0,
-        residual,
-        weight,
-        bias,
-        layerscale,
-        x0_subset,
-        out_subset,
-        dropout_p,
-        epsilon,
-        rowscale_const,
-        out_numrows,
-        residual_in_fp32,
-        prenorm,
-        False,
-        return_dropout_mask,
-    )
-
-
-def dropout_add_layer_norm_parallel_residual(
-    x0,
-    x1,
-    residual,
-    weight0,
-    bias0,
-    weight1,
-    bias1,
-    dropout_p,
-    epsilon,
-    prenorm=False,
-    residual_in_fp32=False,
-    return_dropout_mask=False,
-):
-    """residual_in_fp32 only has an effect if residual is None.
-    Otherwise residual dtype is residual.dtype.
-    """
-    return DropoutAddLayerNormParallelResidualFn.apply(
-        x0,
-        x1,
-        residual,
-        weight0,
-        bias0,
-        weight1,
-        bias1,
-        dropout_p,
-        epsilon,
-        residual_in_fp32,
-        prenorm,
-        False,
-        return_dropout_mask,
-    )
-
-
-class DropoutAddLayerNorm(torch.nn.Module):
-    def __init__(
-        self,
-        hidden_size,
-        prenorm=False,
-        p=0.0,
-        eps=1e-5,
-        residual_in_fp32=False,
-        device=None,
-        dtype=None,
-    ):
-        factory_kwargs = {"device": device, "dtype": dtype}
-        super().__init__()
-        self.prenorm = prenorm
-        self.p = p
-        self.eps = eps
-        self.residual_in_fp32 = residual_in_fp32
-        self.weight = torch.nn.Parameter(torch.empty(hidden_size, **factory_kwargs))
-        self.bias = torch.nn.Parameter(torch.empty(hidden_size, **factory_kwargs))
-        self.reset_parameters()
-
-    def reset_parameters(self):
-        init.ones_(self.weight)
-        init.zeros_(self.bias)
-
-    def forward(self, x0, residual=None):
-        return dropout_add_layer_norm(
-            x0,
-            residual,
-            self.weight,
-            self.bias,
-            self.p if self.training else 0.0,
-            self.eps,
-            prenorm=self.prenorm,
-            residual_in_fp32=self.residual_in_fp32,
-        )
diff --git a/based/ops/rms_norm.py b/based/ops/rms_norm.py
deleted file mode 100644
index 068348d..0000000
--- a/based/ops/rms_norm.py
+++ /dev/null
@@ -1,174 +0,0 @@
-# Copyright (c) 2022, Tri Dao.
-# Adapted from https://github.com/NVIDIA/apex/blob/master/apex/contrib/layer_norm/layer_norm.py
-
-import torch
-from torch.nn import init
-
-from flash_attn.ops.layer_norm import (
-    DropoutAddLayerNormFn,
-    DropoutAddLayerNormParallelResidualFn,
-    DropoutAddLayerNormSubsetFn,
-)
-
-
-def rms_norm(x, weight, epsilon):
-    return DropoutAddLayerNormFn.apply(
-        x, None, weight, None, None, None, 0.0, epsilon, False, False, True
-    )
-
-
-def dropout_add_rms_norm(
-    x0,
-    residual,
-    weight,
-    bias,
-    dropout_p,
-    epsilon,
-    rowscale=None,
-    layerscale=None,
-    prenorm=False,
-    residual_in_fp32=False,
-    return_dropout_mask=False,
-):
-    """residual_in_fp32 only has an effect if residual is None.
-    Otherwise residual dtype is residual.dtype.
-    """
-    return DropoutAddLayerNormFn.apply(
-        x0,
-        residual,
-        weight,
-        bias,
-        rowscale,
-        layerscale,
-        dropout_p,
-        epsilon,
-        residual_in_fp32,
-        prenorm,
-        True,
-        return_dropout_mask,
-    )
-
-
-def dropout_add_rms_norm_subset(
-    x0,
-    residual,
-    weight,
-    bias,
-    dropout_p,
-    epsilon,
-    layerscale=None,
-    x0_subset=None,
-    out_subset=None,
-    rowscale_const=1.0,
-    out_numrows=0,
-    prenorm=False,
-    residual_in_fp32=False,
-    return_dropout_mask=False,
-):
-    """residual_in_fp32 only has an effect if residual is None.
-    Otherwise residual dtype is residual.dtype.
-    """
-    return DropoutAddLayerNormSubsetFn.apply(
-        x0,
-        residual,
-        weight,
-        bias,
-        layerscale,
-        x0_subset,
-        out_subset,
-        dropout_p,
-        epsilon,
-        rowscale_const,
-        out_numrows,
-        residual_in_fp32,
-        prenorm,
-        True,
-        return_dropout_mask,
-    )
-
-
-def dropout_add_rms_norm_parallel_residual(
-    x0,
-    x1,
-    residual,
-    weight0,
-    bias0,
-    weight1,
-    bias1,
-    dropout_p,
-    epsilon,
-    prenorm=False,
-    residual_in_fp32=False,
-    return_dropout_mask=False,
-):
-    """residual_in_fp32 only has an effect if residual is None.
-    Otherwise residual dtype is residual.dtype.
-    """
-    return DropoutAddLayerNormParallelResidualFn.apply(
-        x0,
-        x1,
-        residual,
-        weight0,
-        bias0,
-        weight1,
-        bias1,
-        dropout_p,
-        epsilon,
-        residual_in_fp32,
-        prenorm,
-        True,
-        return_dropout_mask,
-    )
-
-
-class RMSNorm(torch.nn.Module):
-    def __init__(self, hidden_size, eps=1e-5, device=None, dtype=None):
-        factory_kwargs = {"device": device, "dtype": dtype}
-        super().__init__()
-        self.eps = eps
-        self.weight = torch.nn.Parameter(torch.empty(hidden_size, **factory_kwargs))
-        self.register_parameter("bias", None)
-        self.reset_parameters()
-
-    def reset_parameters(self):
-        init.ones_(self.weight)
-
-    def forward(self, x):
-        return rms_norm(x, self.weight, self.eps)
-
-
-class DropoutAddRMSNorm(torch.nn.Module):
-    def __init__(
-        self,
-        hidden_size,
-        prenorm=False,
-        p=0.0,
-        eps=1e-5,
-        residual_in_fp32=False,
-        device=None,
-        dtype=None,
-    ):
-        factory_kwargs = {"device": device, "dtype": dtype}
-        super().__init__()
-        self.prenorm = prenorm
-        self.p = p
-        self.eps = eps
-        self.residual_in_fp32 = residual_in_fp32
-        self.weight = torch.nn.Parameter(torch.empty(hidden_size, **factory_kwargs))
-        self.register_parameter("bias", None)
-        self.reset_parameters()
-
-    def reset_parameters(self):
-        init.ones_(self.weight)
-
-    def forward(self, x0, residual=None):
-        return dropout_add_rms_norm(
-            x0,
-            residual,
-            self.weight,
-            None,
-            self.p if self.training else 0.0,
-            self.eps,
-            prenorm=self.prenorm,
-            residual_in_fp32=self.residual_in_fp32,
-        )
diff --git a/based/ops/triton/__init__.py b/based/ops/triton/__init__.py
deleted file mode 100644
index 8b13789..0000000
--- a/based/ops/triton/__init__.py
+++ /dev/null
@@ -1 +0,0 @@
-
diff --git a/based/ops/triton/cross_entropy.py b/based/ops/triton/cross_entropy.py
deleted file mode 100644
index c8111ca..0000000
--- a/based/ops/triton/cross_entropy.py
+++ /dev/null
@@ -1,320 +0,0 @@
-# Copyright (c) 2023, Tri Dao.
-
-from typing import Tuple, Optional, Union
-
-import torch
-
-from einops import rearrange
-
-import triton
-import triton.language as tl
-
-# `all_gather_into_tensor` and `reduce_scatter_tensor` are new placeholders for
-# `_all_gather_base` and `_reduce_scatter_base`. They require the most recent
-# version of PyTorch. The following 2 lines are for backward compatibility with
-# older PyTorch.
-if "all_gather_into_tensor" not in dir(torch.distributed):
-    torch.distributed.all_gather_into_tensor = torch.distributed._all_gather_base
-
-
-@triton.heuristics(
-    {
-        "HAS_SMOOTHING": lambda args: args["smoothing"] > 0.0,
-    }
-)
-@triton.jit
-def cross_entropy_fwd_kernel(
-    loss_ptr,  # data ptrs
-    lse_ptr,
-    z_loss_ptr,
-    logits_ptr,
-    labels_ptr,
-    smoothing,
-    logit_scale,
-    lse_square_scale,
-    ignored_index,
-    total_classes,
-    class_start_idx,  # Useful for tensor parallel when each rank only has a subset of classes
-    n_cols,  # shapes
-    n_rows,
-    logits_row_stride,  # strides
-    BLOCK_SIZE: tl.constexpr,
-    HAS_SMOOTHING: tl.constexpr,
-    # if SPLIT (e.g. tensor parallel), don't include the LSE in the loss since it's not the final LSE
-    SPLIT: tl.constexpr,
-):
-    row_idx = tl.program_id(0)
-    col_block_idx = tl.program_id(1)
-    logits_ptr = logits_ptr + row_idx * logits_row_stride.to(tl.int64)
-    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
-    label_idx = tl.load(labels_ptr + row_idx)
-    logits = tl.load(logits_ptr + col_offsets, mask=col_offsets < n_cols, other=-float("inf")).to(
-        tl.float32
-    ) * logit_scale
-    max_logits = tl.max(logits, 0)
-    if HAS_SMOOTHING:
-        sum_logits = tl.sum(tl.where(col_offsets < n_cols, logits, 0.0), 0)
-    lse = tl.log(tl.sum(tl.exp(logits - max_logits), 0)) + max_logits
-    tl.store(lse_ptr + col_block_idx * n_rows + row_idx, lse)
-    if label_idx == ignored_index:
-        loss = 0.0
-        z_loss = 0.0
-    else:
-        label_idx -= class_start_idx
-        if label_idx >= col_block_idx * BLOCK_SIZE and label_idx < min(
-            n_cols, (col_block_idx + 1) * BLOCK_SIZE
-        ):
-            logits_label = tl.load(logits_ptr + label_idx) * logit_scale
-            if HAS_SMOOTHING:
-                loss = (
-                    (lse if not SPLIT else 0.0)
-                    - smoothing * sum_logits / total_classes
-                    - (1 - smoothing) * logits_label
-                )
-            else:
-                loss = (lse if not SPLIT else 0.0) - logits_label
-        else:
-            # If label is out of bounds, we set the CE loss to 0.0. But we still want the smoothing loss
-            if HAS_SMOOTHING:
-                loss = smoothing * ((lse if not SPLIT else 0.0) - sum_logits / total_classes)
-            else:
-                loss = 0.0
-        if not SPLIT:
-            z_loss = lse_square_scale * lse * lse
-            loss += z_loss
-        else:
-            z_loss = 0.0
-    tl.store(loss_ptr + col_block_idx * n_rows + row_idx, loss)
-    if not SPLIT:
-        tl.store(z_loss_ptr + col_block_idx * n_rows + row_idx, z_loss)
-
-
-@triton.heuristics(
-    {
-        "HAS_SMOOTHING": lambda args: args["smoothing"] > 0.0,
-    }
-)
-@triton.jit
-def cross_entropy_bwd_kernel(
-    dlogits_ptr,  # data ptrs
-    dloss_ptr,
-    logits_ptr,
-    lse_ptr,
-    labels_ptr,
-    smoothing,
-    logit_scale,
-    lse_square_scale,
-    ignored_index,
-    total_classes,
-    class_start_idx,  # Useful for tensor parallel when each rank only has a subset of classes
-    n_cols,  # shapes
-    logits_row_stride,  # strides
-    dlogits_row_stride,
-    dloss_row_stride,
-    BLOCK_SIZE: tl.constexpr,
-    HAS_SMOOTHING: tl.constexpr,
-):
-    row_idx = tl.program_id(0)
-    col_block_idx = tl.program_id(1)
-    logits_ptr = logits_ptr + row_idx * logits_row_stride.to(tl.int64)
-    dlogits_ptr = dlogits_ptr + row_idx * dlogits_row_stride.to(tl.int64)
-    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
-    label_idx = tl.load(labels_ptr + row_idx)
-    if label_idx != ignored_index:
-        dloss = tl.load(dloss_ptr + row_idx * dloss_row_stride)
-    else:
-        dloss = 0.0
-    logits = tl.load(logits_ptr + col_offsets, mask=col_offsets < n_cols, other=-float("inf")).to(
-        tl.float32
-    ) * logit_scale
-    lse = tl.load(lse_ptr + row_idx)
-    probs = tl.exp(logits - lse)
-    probs += 2.0 * lse_square_scale * lse * probs
-    label_idx -= class_start_idx
-    if HAS_SMOOTHING:
-        smooth_positive = 1.0 - smoothing
-        smooth_negative = smoothing / total_classes
-        probs = tl.where(col_offsets == label_idx, probs - (1 - smoothing), probs) - smooth_negative
-    else:
-        probs = tl.where(col_offsets == label_idx, probs - 1.0, probs)
-    tl.store(dlogits_ptr + col_offsets, (dloss * logit_scale) * probs, mask=col_offsets < n_cols)
-
-
-class CrossEntropyLoss(torch.autograd.Function):
-
-    @staticmethod
-    def forward(
-        ctx,
-        logits,
-        labels,
-        smoothing=0.0,
-        logit_scale=1.0,
-        lse_square_scale=0.0,
-        ignored_index=-100,
-        inplace_backward=False,
-        process_group=None,
-    ):
-        n_rows, n_cols = logits.shape
-        assert labels.shape == (n_rows,)
-        world_size = 1 if process_group is None else torch.distributed.get_world_size(process_group)
-        total_classes = world_size * n_cols
-        rank = 0 if process_group is None else torch.distributed.get_rank(process_group)
-        class_start_idx = rank * n_cols
-
-        if logits.stride(-1) != 1:
-            logits = logits.contiguous()
-        # Set these similar to https://github.com/openai/triton/blob/main/python/tutorials/02-fused-softmax.py
-        MAX_BLOCK_SIZE = 64 * 1024
-        BLOCK_SIZE = min(triton.next_power_of_2(n_cols), MAX_BLOCK_SIZE)
-        num_warps = (
-            4
-            if BLOCK_SIZE < 2048
-            else (8 if BLOCK_SIZE < 8192 else (16 if BLOCK_SIZE < 128 * 1024 else 32))
-        )
-        # We may split the lse computation across multiple blocks, then do a reduction
-        # lse(local_lse) to get the final LSE. This is faster for large n_cols (e.g., > 64k)
-        # where having just one thread block processing more than 64k elements is slow.
-        split = world_size > 1 or n_cols > MAX_BLOCK_SIZE
-        n_splits = (n_cols + BLOCK_SIZE - 1) // BLOCK_SIZE
-        loss_shape = (n_splits, n_rows) if n_splits > 1 else (n_rows,)
-        losses = torch.empty(*loss_shape, dtype=torch.float, device=logits.device)
-        lse = torch.empty(*loss_shape, dtype=torch.float, device=logits.device)
-        z_losses = torch.empty(*loss_shape, dtype=torch.float, device=logits.device)
-        # Need this, otherwise Triton tries to launch from cuda:0 and we get
-        # ValueError: Pointer argument (at 0) cannot be accessed from Triton (cpu tensor?)
-        with torch.cuda.device(logits.device.index):
-            cross_entropy_fwd_kernel[(n_rows, n_splits)](
-                losses,  # data ptrs
-                lse,
-                z_losses,
-                logits,
-                labels,
-                smoothing,
-                logit_scale,
-                lse_square_scale,
-                ignored_index,
-                total_classes,
-                class_start_idx,
-                n_cols,  # shapes
-                n_rows,
-                logits.stride(0),  # strides
-                BLOCK_SIZE=BLOCK_SIZE,  # constants
-                num_warps=num_warps,
-                SPLIT=split,
-            )
-
-        if split:
-            # If there's no smoothing, if labels are in the vocab of this partition, losses contains
-            # - predicted logit, and 0 otherwise.
-            # If there's smoothing=0.1, for labels in the vocab of this partition, losses contains
-            # -0.9 * predicted logit - 0.1 * sum logit / total_classes.
-            # For labels not in the vocab of this partition, losses contains
-            # -0.1 * sum logit / total_classes.
-            if n_splits > 1:
-                lse = torch.logsumexp(lse, dim=0)
-                losses = losses.sum(dim=0)
-            if world_size > 1:
-                lse_allgather = torch.empty(world_size, n_rows, dtype=lse.dtype, device=lse.device)
-                torch.distributed.all_gather_into_tensor(lse_allgather, lse, group=process_group)
-                handle_losses = torch.distributed.all_reduce(
-                    losses, op=torch.distributed.ReduceOp.SUM, group=process_group, async_op=True
-                )
-                lse = torch.logsumexp(lse_allgather, dim=0)
-                handle_losses.wait()
-            # After the allreduce, if there's no smoothing, the total losses are - predicted_logit,
-            # we just have to add the (global) lse.
-            # If there's smoothing=0.1, the total losses are
-            # -0.9 * predicted_logit - 0.1 * sum logit / total_classes.
-            # Again, we just have to add the (global) lse.
-            losses += lse
-            if lse_square_scale != 0.0:
-                z_losses = lse_square_scale * lse.square()
-                z_losses.masked_fill_(labels == ignored_index, 0.0)
-                losses += z_losses
-            else:
-                z_losses = torch.zeros_like(losses)
-            losses.masked_fill_(labels == ignored_index, 0.0)
-
-        ctx.save_for_backward(logits, lse, labels)
-        ctx.mark_non_differentiable(z_losses)
-        ctx.smoothing = smoothing
-        ctx.logit_scale = logit_scale
-        ctx.lse_square_scale = lse_square_scale
-        ctx.ignored_index = ignored_index
-        ctx.total_classes = total_classes
-        ctx.class_start_idx = class_start_idx
-        ctx.inplace_backward = inplace_backward
-
-        return losses, z_losses
-
-    @staticmethod
-    def backward(ctx, grad_losses, grad_z_losses):
-        del grad_z_losses  # z_losses are only for logging.
-
-        logits, lse, labels = ctx.saved_tensors
-        dlogits = logits if ctx.inplace_backward else torch.empty_like(logits)
-        n_rows, n_cols = logits.shape
-        BLOCK_SIZE = min(triton.next_power_of_2(n_cols), 4 * 1024)
-        num_warps = 4 if BLOCK_SIZE < 2048 else (8 if BLOCK_SIZE < 8192 else 16)
-        grid = lambda META: (n_rows, triton.cdiv(n_cols, META["BLOCK_SIZE"]))  # noqa
-        # Need this, otherwise Triton tries to launch from cuda:0 and we get
-        # ValueError: Pointer argument (at 0) cannot be accessed from Triton (cpu tensor?)
-        with torch.cuda.device(logits.device.index):
-            cross_entropy_bwd_kernel[grid](
-                dlogits,  # data ptrs
-                grad_losses,
-                logits,
-                lse,
-                labels,
-                ctx.smoothing,
-                ctx.logit_scale,
-                ctx.lse_square_scale,
-                ctx.ignored_index,
-                ctx.total_classes,
-                ctx.class_start_idx,
-                n_cols,  # shapes
-                logits.stride(0),  # strides
-                dlogits.stride(0),
-                grad_losses.stride(0),
-                BLOCK_SIZE=BLOCK_SIZE,  # constants
-                num_warps=num_warps,
-            )
-        return dlogits, None, None, None, None, None, None, None, None
-
-def cross_entropy_loss(
-    logits: torch.Tensor,
-    labels: torch.Tensor,
-    label_smoothing: float = 0.0,
-    logit_scale: float = 1.0,
-    lse_square_scale: float = 0.0,
-    ignored_index=-100,
-    inplace_backward: bool = False,
-    process_group=None,
-) -> Tuple[torch.Tensor, torch.Tensor]:
-    """
-    Arguments:
-        logits: (batch, vocab_size)
-        labels: (batch,)
-        label_smoothing: float
-        logit_scale: float. Multiply logits by this scale before calculating the loss.
-        lse_square_scale: float. If > 0, we add lse_square_scale * lse(logits) ^ 2 to the loss.
-            This is also referred to as "z-loss".
-        ignored_index: int. If labels == ignored_index, the loss is set to 0.0.
-        inplace_backward: bool. If True, we do the backward pass in-place by modifying the logits.
-            This saves memory.
-        process_group: if not None, we're doing Tensor Parallel: each process is responsible for
-            one part of the vocab. The loss will be aggregated across processes.
-    Returns:
-        losses: (batch,), float
-        z_losses: (batch,), float
-    """
-    return CrossEntropyLoss.apply(
-        logits,
-        labels,
-        label_smoothing,
-        logit_scale,
-        lse_square_scale,
-        ignored_index,
-        inplace_backward,
-        process_group,
-    )
diff --git a/based/ops/triton/k_activations.py b/based/ops/triton/k_activations.py
deleted file mode 100644
index efb83c3..0000000
--- a/based/ops/triton/k_activations.py
+++ /dev/null
@@ -1,162 +0,0 @@
-# Adapted from https://github.com/facebookresearch/xformers/blob/main/xformers/triton/k_activations.py
-# Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.
-#
-# This source code is licensed under the BSD license found in the
-# LICENSE file in the root directory of this source tree.
-
-import math
-from enum import Enum
-from typing import Optional
-
-import triton
-import triton.language as tl
-
-_sqrt2pi = math.sqrt(2.0 / math.pi)
-_sqrt1_2 = math.sqrt(1.0 / 2)
-_gaussian_pdf_normalization = 1.0 / math.sqrt(2 * math.pi)
-
-
-class Activation(str, Enum):
-    SquaredReLU = "squared_relu"
-    GeLU = "gelu"
-    GeLUApprox = "gelu_approx"
-    LeakyReLU = "leaky_relu"
-    ReLU = "relu"
-
-
-def get_triton_activation_kernel(activation: Optional[Activation]):
-    return (
-        {
-            Activation.ReLU: relu,
-            Activation.LeakyReLU: leaky_relu,
-            Activation.GeLU: gelu,
-            Activation.GeLUApprox: gelu_approx,
-            Activation.SquaredReLU: squared_relu,
-        }[activation]
-        if activation
-        else None
-    )
-
-
-def get_triton_activation_bwd_kernel(activation: Optional[Activation]):
-    return (
-        {
-            Activation.ReLU: relu_grad,
-            Activation.LeakyReLU: leaky_relu_grad,
-            Activation.GeLU: gelu_grad,
-            Activation.GeLUApprox: gelu_approx_grad,
-            Activation.SquaredReLU: squared_relu_grad,
-        }[activation]
-        if activation
-        else None
-    )
-
-
-@triton.jit
-def tanh(x):
-    # Tanh is just a scaled sigmoid
-    return 2 * tl.sigmoid(2 * x) - 1
-
-
-@triton.jit
-def cosh(x):
-    exp_x = tl.exp(x)
-    return (exp_x + 1.0 / exp_x) * 0.5
-
-
-# a Triton implementation of the most used activations
-# See for instance http://arxiv.org/abs/1606.08415 for an overview
-
-# ReLU
-@triton.jit
-def relu(x):
-    """
-    ReLU_ activation function
-
-    .. _ReLU: https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html
-    """
-    zero = 0.0
-    return tl.where(x >= 0, x, zero.to(x.dtype))
-
-
-@triton.jit
-def relu_grad(x):
-    # ReLU is different from other activations
-    # in that it does not require the input to retrospectively compute its gradient
-    # here the input is the downstream gradient, and we return the upstream gradient directly
-    zero = 0.0
-    one = 1.0
-    return tl.where(x >= 0, one.to(x.dtype), zero.to(x.dtype))
-
-
-@triton.jit
-def squared_relu(x):
-    """
-    Squared ReLU activation, as proposed in the Primer_ paper.
-
-    .. _Primer: https://arxiv.org/abs/2109.08668
-    """
-    x_ = relu(x)
-    return (x_ * x_).to(x.dtype)
-
-
-@triton.jit
-def squared_relu_grad(x):
-    return tl.where(x >= 0, 2.0 * x, 0.0)
-
-
-# Leaky ReLU
-@triton.jit
-def leaky_relu(x):
-    """
-    LeakyReLU_ activation
-
-    .. _LeakyReLU: https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html
-    """
-    scale = 0.01 + 0.0
-    scale = scale.to(x.dtype)
-    return tl.where(x >= 0, x, scale * x)
-
-
-@triton.jit
-def leaky_relu_grad(x):
-    min_grad = 0.01
-    max_grad = 1
-
-    min_grad = min_grad.to(x.dtype)
-    max_grad = max_grad.to(x.dtype)
-
-    return tl.where(x >= 0, max_grad, min_grad)
-
-
-@triton.jit
-def gelu(x):
-    """Gaussian Error Linear Unit (GELU)"""
-    return x * 0.5 * (1.0 + tl.libdevice.erf(x * _sqrt1_2))
-
-
-@triton.jit
-def gelu_grad(x):
-    cdf = 0.5 * (1.0 + tl.libdevice.erf(x * _sqrt1_2))
-    pdf = tl.exp(-0.5 * x * x) * _gaussian_pdf_normalization
-    return cdf + x * pdf
-
-
-@triton.jit
-def gelu_approx(x):
-    """
-    GeLU_ activation - Gaussian error linear unit, with tanh approximation
-
-    .. _GeLU: https://arxiv.org/pdf/1606.08415.pdf
-    """
-    return 0.5 * x * (1.0 + tanh(_sqrt2pi * x * (1.0 + 0.044715 * x * x)))
-
-
-@triton.jit
-def gelu_approx_grad(x):
-    # CREDITS: Fast implementation proposed in
-    # https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/fused_bias_gelu.py#L30
-    tanh_out = tanh(0.79788456 * x * (1 + 0.044715 * x * x))
-    return 0.5 * x * ((1 - tanh_out * tanh_out) * (0.79788456 + 0.1070322243 * x * x)) + 0.5 * (
-        1 + tanh_out
-    )
diff --git a/based/ops/triton/layer_norm.py b/based/ops/triton/layer_norm.py
deleted file mode 100644
index c922906..0000000
--- a/based/ops/triton/layer_norm.py
+++ /dev/null
@@ -1,1086 +0,0 @@
-# Copyright (c) 2024, Tri Dao.
-# Implement dropout + residual + layer_norm / rms_norm.
-
-# Based on the Triton LayerNorm tutorial: https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html
-# For the backward pass, we keep weight_grad and bias_grad in registers and accumulate.
-# This is faster for dimensions up to 8k, but after that it's much slower due to register spilling.
-# The models we train have hidden dim up to 8k anyway (e.g. Llama 70B), so this is fine.
-
-import math
-
-import torch
-import torch.nn.functional as F
-from torch.cuda.amp import custom_fwd, custom_bwd
-
-import triton
-import triton.language as tl
-
-
-def layer_norm_ref(
-    x,
-    weight,
-    bias,
-    residual=None,
-    x1=None,
-    weight1=None,
-    bias1=None,
-    eps=1e-6,
-    dropout_p=0.0,
-    rowscale=None,
-    prenorm=False,
-    dropout_mask=None,
-    dropout_mask1=None,
-    upcast=False,
-):
-    dtype = x.dtype
-    if upcast:
-        x = x.float()
-        weight = weight.float()
-        bias = bias.float() if bias is not None else None
-        residual = residual.float() if residual is not None else residual
-        x1 = x1.float() if x1 is not None else None
-        weight1 = weight1.float() if weight1 is not None else None
-        bias1 = bias1.float() if bias1 is not None else None
-    if x1 is not None:
-        assert rowscale is None, "rowscale is not supported with parallel LayerNorm"
-    if rowscale is not None:
-        x = x * rowscale[..., None]
-    if dropout_p > 0.0:
-        if dropout_mask is not None:
-            x = x.masked_fill(~dropout_mask, 0.0) / (1.0 - dropout_p)
-        else:
-            x = F.dropout(x, p=dropout_p)
-        if x1 is not None:
-            if dropout_mask1 is not None:
-                x1 = x1.masked_fill(~dropout_mask1, 0.0) / (1.0 - dropout_p)
-            else:
-                x1 = F.dropout(x1, p=dropout_p)
-    if x1 is not None:
-        x = x + x1
-    if residual is not None:
-        x = (x + residual).to(x.dtype)
-    out = F.layer_norm(x.to(weight.dtype), x.shape[-1:], weight=weight, bias=bias, eps=eps).to(
-        dtype
-    )
-    if weight1 is None:
-        return out if not prenorm else (out, x)
-    else:
-        out1 = F.layer_norm(
-            x.to(weight1.dtype), x.shape[-1:], weight=weight1, bias=bias1, eps=eps
-        ).to(dtype)
-        return (out, out1) if not prenorm else (out, out1, x)
-
-
-def rms_norm_ref(
-    x,
-    weight,
-    bias,
-    residual=None,
-    x1=None,
-    weight1=None,
-    bias1=None,
-    eps=1e-6,
-    dropout_p=0.0,
-    rowscale=None,
-    prenorm=False,
-    dropout_mask=None,
-    dropout_mask1=None,
-    upcast=False,
-):
-    dtype = x.dtype
-    if upcast:
-        x = x.float()
-        weight = weight.float()
-        bias = bias.float() if bias is not None else None
-        residual = residual.float() if residual is not None else residual
-        x1 = x1.float() if x1 is not None else None
-        weight1 = weight1.float() if weight1 is not None else None
-        bias1 = bias1.float() if bias1 is not None else None
-    if x1 is not None:
-        assert rowscale is None, "rowscale is not supported with parallel LayerNorm"
-    if rowscale is not None:
-        x = x * rowscale[..., None]
-    if dropout_p > 0.0:
-        if dropout_mask is not None:
-            x = x.masked_fill(~dropout_mask, 0.0) / (1.0 - dropout_p)
-        else:
-            x = F.dropout(x, p=dropout_p)
-        if x1 is not None:
-            if dropout_mask1 is not None:
-                x1 = x1.masked_fill(~dropout_mask1, 0.0) / (1.0 - dropout_p)
-            else:
-                x1 = F.dropout(x1, p=dropout_p)
-    if x1 is not None:
-        x = x + x1
-    if residual is not None:
-        x = (x + residual).to(x.dtype)
-    rstd = 1 / torch.sqrt((x.square()).mean(dim=-1, keepdim=True) + eps)
-    out = ((x * rstd * weight) + bias if bias is not None else (x * rstd * weight)).to(dtype)
-    if weight1 is None:
-        return out if not prenorm else (out, x)
-    else:
-        out1 = ((x * rstd * weight1) + bias1 if bias1 is not None else (x * rstd * weight1)).to(
-            dtype
-        )
-        return (out, out1) if not prenorm else (out, out1, x)
-
-
-@triton.autotune(
-    configs=[
-        triton.Config({}, num_warps=1),
-        triton.Config({}, num_warps=2),
-        triton.Config({}, num_warps=4),
-        triton.Config({}, num_warps=8),
-        triton.Config({}, num_warps=16),
-        triton.Config({}, num_warps=32),
-    ],
-    key=["N", "HAS_RESIDUAL", "STORE_RESIDUAL_OUT", "IS_RMS_NORM", "HAS_BIAS"],
-)
-# @triton.heuristics({"HAS_BIAS": lambda args: args["B"] is not None})
-# @triton.heuristics({"HAS_RESIDUAL": lambda args: args["RESIDUAL"] is not None})
-@triton.heuristics({"HAS_X1": lambda args: args["X1"] is not None})
-@triton.heuristics({"HAS_W1": lambda args: args["W1"] is not None})
-@triton.heuristics({"HAS_B1": lambda args: args["B1"] is not None})
-@triton.jit
-def _layer_norm_fwd_1pass_kernel(
-    X,  # pointer to the input
-    Y,  # pointer to the output
-    W,  # pointer to the weights
-    B,  # pointer to the biases
-    RESIDUAL,  # pointer to the residual
-    X1,
-    W1,
-    B1,
-    Y1,
-    RESIDUAL_OUT,  # pointer to the residual
-    ROWSCALE,
-    SEEDS,  # Dropout seeds for each row
-    DROPOUT_MASK,
-    Mean,  # pointer to the mean
-    Rstd,  # pointer to the 1/std
-    stride_x_row,  # how much to increase the pointer when moving by 1 row
-    stride_y_row,
-    stride_res_row,
-    stride_res_out_row,
-    stride_x1_row,
-    stride_y1_row,
-    M,  # number of rows in X
-    N,  # number of columns in X
-    eps,  # epsilon to avoid division by zero
-    dropout_p,  # Dropout probability
-    IS_RMS_NORM: tl.constexpr,
-    BLOCK_N: tl.constexpr,
-    HAS_RESIDUAL: tl.constexpr,
-    STORE_RESIDUAL_OUT: tl.constexpr,
-    HAS_BIAS: tl.constexpr,
-    HAS_DROPOUT: tl.constexpr,
-    STORE_DROPOUT_MASK: tl.constexpr,
-    HAS_ROWSCALE: tl.constexpr,
-    HAS_X1: tl.constexpr,
-    HAS_W1: tl.constexpr,
-    HAS_B1: tl.constexpr,
-):
-    # Map the program id to the row of X and Y it should compute.
-    row = tl.program_id(0)
-    X += row * stride_x_row
-    Y += row * stride_y_row
-    if HAS_RESIDUAL:
-        RESIDUAL += row * stride_res_row
-    if STORE_RESIDUAL_OUT:
-        RESIDUAL_OUT += row * stride_res_out_row
-    if HAS_X1:
-        X1 += row * stride_x1_row
-    if HAS_W1:
-        Y1 += row * stride_y1_row
-    # Compute mean and variance
-    cols = tl.arange(0, BLOCK_N)
-    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)
-    if HAS_ROWSCALE:
-        rowscale = tl.load(ROWSCALE + row).to(tl.float32)
-        x *= rowscale
-    if HAS_DROPOUT:
-        # Compute dropout mask
-        # 7 rounds is good enough, and reduces register pressure
-        keep_mask = tl.rand(tl.load(SEEDS + row).to(tl.uint32), cols, n_rounds=7) > dropout_p
-        x = tl.where(keep_mask, x / (1.0 - dropout_p), 0.0)
-        if STORE_DROPOUT_MASK:
-            tl.store(DROPOUT_MASK + row * N + cols, keep_mask, mask=cols < N)
-    if HAS_X1:
-        x1 = tl.load(X1 + cols, mask=cols < N, other=0.0).to(tl.float32)
-        if HAS_ROWSCALE:
-            rowscale = tl.load(ROWSCALE + M + row).to(tl.float32)
-            x1 *= rowscale
-        if HAS_DROPOUT:
-            # Compute dropout mask
-            # 7 rounds is good enough, and reduces register pressure
-            keep_mask = (
-                tl.rand(tl.load(SEEDS + M + row).to(tl.uint32), cols, n_rounds=7) > dropout_p
-            )
-            x1 = tl.where(keep_mask, x1 / (1.0 - dropout_p), 0.0)
-            if STORE_DROPOUT_MASK:
-                tl.store(DROPOUT_MASK + (M + row) * N + cols, keep_mask, mask=cols < N)
-        x += x1
-    if HAS_RESIDUAL:
-        residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0).to(tl.float32)
-        x += residual
-    if STORE_RESIDUAL_OUT:
-        tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)
-    if not IS_RMS_NORM:
-        mean = tl.sum(x, axis=0) / N
-        tl.store(Mean + row, mean)
-        xbar = tl.where(cols < N, x - mean, 0.0)
-        var = tl.sum(xbar * xbar, axis=0) / N
-    else:
-        xbar = tl.where(cols < N, x, 0.0)
-        var = tl.sum(xbar * xbar, axis=0) / N
-    rstd = 1 / tl.sqrt(var + eps)
-    tl.store(Rstd + row, rstd)
-    # Normalize and apply linear transformation
-    mask = cols < N
-    w = tl.load(W + cols, mask=mask).to(tl.float32)
-    if HAS_BIAS:
-        b = tl.load(B + cols, mask=mask).to(tl.float32)
-    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd
-    y = x_hat * w + b if HAS_BIAS else x_hat * w
-    # Write output
-    tl.store(Y + cols, y, mask=mask)
-    if HAS_W1:
-        w1 = tl.load(W1 + cols, mask=mask).to(tl.float32)
-        if HAS_B1:
-            b1 = tl.load(B1 + cols, mask=mask).to(tl.float32)
-        y1 = x_hat * w1 + b1 if HAS_B1 else x_hat * w1
-        tl.store(Y1 + cols, y1, mask=mask)
-
-
-def _layer_norm_fwd(
-    x,
-    weight,
-    bias,
-    eps,
-    residual=None,
-    x1=None,
-    weight1=None,
-    bias1=None,
-    dropout_p=0.0,
-    rowscale=None,
-    out_dtype=None,
-    residual_dtype=None,
-    is_rms_norm=False,
-    return_dropout_mask=False,
-):
-    if residual is not None:
-        residual_dtype = residual.dtype
-    M, N = x.shape
-    assert x.stride(-1) == 1
-    if residual is not None:
-        assert residual.stride(-1) == 1
-        assert residual.shape == (M, N)
-    assert weight.shape == (N,)
-    assert weight.stride(-1) == 1
-    if bias is not None:
-        assert bias.stride(-1) == 1
-        assert bias.shape == (N,)
-    if x1 is not None:
-        assert x1.shape == x.shape
-        assert rowscale is None
-        assert x1.stride(-1) == 1
-    if weight1 is not None:
-        assert weight1.shape == (N,)
-        assert weight1.stride(-1) == 1
-    if bias1 is not None:
-        assert bias1.shape == (N,)
-        assert bias1.stride(-1) == 1
-    if rowscale is not None:
-        assert rowscale.is_contiguous()
-        assert rowscale.shape == (M,)
-    # allocate output
-    y = torch.empty_like(x, dtype=x.dtype if out_dtype is None else out_dtype)
-    assert y.stride(-1) == 1
-    if weight1 is not None:
-        y1 = torch.empty_like(y)
-        assert y1.stride(-1) == 1
-    else:
-        y1 = None
-    if (
-        residual is not None
-        or (residual_dtype is not None and residual_dtype != x.dtype)
-        or dropout_p > 0.0
-        or rowscale is not None
-        or x1 is not None
-    ):
-        residual_out = torch.empty(
-            M, N, device=x.device, dtype=residual_dtype if residual_dtype is not None else x.dtype
-        )
-        assert residual_out.stride(-1) == 1
-    else:
-        residual_out = None
-    mean = torch.empty((M,), dtype=torch.float32, device=x.device) if not is_rms_norm else None
-    rstd = torch.empty((M,), dtype=torch.float32, device=x.device)
-    if dropout_p > 0.0:
-        seeds = torch.randint(
-            2**32, (M if x1 is None else 2 * M,), device=x.device, dtype=torch.int64
-        )
-    else:
-        seeds = None
-    if return_dropout_mask and dropout_p > 0.0:
-        dropout_mask = torch.empty(M if x1 is None else 2 * M, N, device=x.device, dtype=torch.bool)
-    else:
-        dropout_mask = None
-    # Less than 64KB per feature: enqueue fused kernel
-    MAX_FUSED_SIZE = 65536 // x.element_size()
-    BLOCK_N = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))
-    if N > BLOCK_N:
-        raise RuntimeError("This layer norm doesn't support feature dim >= 64KB.")
-    with torch.cuda.device(x.device.index):
-        _layer_norm_fwd_1pass_kernel[(M,)](
-            x,
-            y,
-            weight,
-            bias,
-            residual,
-            x1,
-            weight1,
-            bias1,
-            y1,
-            residual_out,
-            rowscale,
-            seeds,
-            dropout_mask,
-            mean,
-            rstd,
-            x.stride(0),
-            y.stride(0),
-            residual.stride(0) if residual is not None else 0,
-            residual_out.stride(0) if residual_out is not None else 0,
-            x1.stride(0) if x1 is not None else 0,
-            y1.stride(0) if y1 is not None else 0,
-            M,
-            N,
-            eps,
-            dropout_p,
-            is_rms_norm,
-            BLOCK_N,
-            residual is not None,
-            residual_out is not None,
-            bias is not None,
-            dropout_p > 0.0,
-            dropout_mask is not None,
-            rowscale is not None,
-        )
-    # residual_out is None if residual is None and residual_dtype == input_dtype and dropout_p == 0.0
-    if dropout_mask is not None and x1 is not None:
-        dropout_mask, dropout_mask1 = dropout_mask.tensor_split(2, dim=0)
-    else:
-        dropout_mask1 = None
-    return (
-        y,
-        y1,
-        mean,
-        rstd,
-        residual_out if residual_out is not None else x,
-        seeds,
-        dropout_mask,
-        dropout_mask1,
-    )
-
-
-@triton.autotune(
-    configs=[
-        triton.Config({}, num_warps=1),
-        triton.Config({}, num_warps=2),
-        triton.Config({}, num_warps=4),
-        triton.Config({}, num_warps=8),
-        triton.Config({}, num_warps=16),
-        triton.Config({}, num_warps=32),
-    ],
-    key=["N", "HAS_DRESIDUAL", "STORE_DRESIDUAL", "IS_RMS_NORM", "HAS_BIAS", "HAS_DROPOUT"],
-)
-# @triton.heuristics({"HAS_BIAS": lambda args: args["B"] is not None})
-# @triton.heuristics({"HAS_DRESIDUAL": lambda args: args["DRESIDUAL"] is not None})
-# @triton.heuristics({"STORE_DRESIDUAL": lambda args: args["DRESIDUAL_IN"] is not None})
-@triton.heuristics({"HAS_ROWSCALE": lambda args: args["ROWSCALE"] is not None})
-@triton.heuristics({"HAS_DY1": lambda args: args["DY1"] is not None})
-@triton.heuristics({"HAS_DX1": lambda args: args["DX1"] is not None})
-@triton.heuristics({"HAS_B1": lambda args: args["DB1"] is not None})
-@triton.heuristics({"RECOMPUTE_OUTPUT": lambda args: args["Y"] is not None})
-@triton.jit
-def _layer_norm_bwd_kernel(
-    X,  # pointer to the input
-    W,  # pointer to the weights
-    B,  # pointer to the biases
-    Y,  # pointer to the output to be recomputed
-    DY,  # pointer to the output gradient
-    DX,  # pointer to the input gradient
-    DW,  # pointer to the partial sum of weights gradient
-    DB,  # pointer to the partial sum of biases gradient
-    DRESIDUAL,
-    W1,
-    DY1,
-    DX1,
-    DW1,
-    DB1,
-    DRESIDUAL_IN,
-    ROWSCALE,
-    SEEDS,
-    Mean,  # pointer to the mean
-    Rstd,  # pointer to the 1/std
-    stride_x_row,  # how much to increase the pointer when moving by 1 row
-    stride_y_row,
-    stride_dy_row,
-    stride_dx_row,
-    stride_dres_row,
-    stride_dy1_row,
-    stride_dx1_row,
-    stride_dres_in_row,
-    M,  # number of rows in X
-    N,  # number of columns in X
-    eps,  # epsilon to avoid division by zero
-    dropout_p,
-    rows_per_program,
-    IS_RMS_NORM: tl.constexpr,
-    BLOCK_N: tl.constexpr,
-    HAS_DRESIDUAL: tl.constexpr,
-    STORE_DRESIDUAL: tl.constexpr,
-    HAS_BIAS: tl.constexpr,
-    HAS_DROPOUT: tl.constexpr,
-    HAS_ROWSCALE: tl.constexpr,
-    HAS_DY1: tl.constexpr,
-    HAS_DX1: tl.constexpr,
-    HAS_B1: tl.constexpr,
-    RECOMPUTE_OUTPUT: tl.constexpr,
-):
-    # Map the program id to the elements of X, DX, and DY it should compute.
-    row_block_id = tl.program_id(0)
-    row_start = row_block_id * rows_per_program
-    # Do not early exit if row_start >= M, because we need to write DW and DB
-    cols = tl.arange(0, BLOCK_N)
-    mask = cols < N
-    X += row_start * stride_x_row
-    if HAS_DRESIDUAL:
-        DRESIDUAL += row_start * stride_dres_row
-    if STORE_DRESIDUAL:
-        DRESIDUAL_IN += row_start * stride_dres_in_row
-    DY += row_start * stride_dy_row
-    DX += row_start * stride_dx_row
-    if HAS_DY1:
-        DY1 += row_start * stride_dy1_row
-    if HAS_DX1:
-        DX1 += row_start * stride_dx1_row
-    if RECOMPUTE_OUTPUT:
-        Y += row_start * stride_y_row
-    w = tl.load(W + cols, mask=mask).to(tl.float32)
-    if RECOMPUTE_OUTPUT and HAS_BIAS:
-        b = tl.load(B + cols, mask=mask, other=0.0).to(tl.float32)
-    if HAS_DY1:
-        w1 = tl.load(W1 + cols, mask=mask).to(tl.float32)
-    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)
-    if HAS_BIAS:
-        db = tl.zeros((BLOCK_N,), dtype=tl.float32)
-    if HAS_DY1:
-        dw1 = tl.zeros((BLOCK_N,), dtype=tl.float32)
-        if HAS_B1:
-            db1 = tl.zeros((BLOCK_N,), dtype=tl.float32)
-    row_end = min((row_block_id + 1) * rows_per_program, M)
-    for row in range(row_start, row_end):
-        # Load data to SRAM
-        x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)
-        dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)
-        if HAS_DY1:
-            dy1 = tl.load(DY1 + cols, mask=mask, other=0).to(tl.float32)
-        if not IS_RMS_NORM:
-            mean = tl.load(Mean + row)
-        rstd = tl.load(Rstd + row)
-        # Compute dx
-        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd
-        xhat = tl.where(mask, xhat, 0.0)
-        if RECOMPUTE_OUTPUT:
-            y = xhat * w + b if HAS_BIAS else xhat * w
-            tl.store(Y + cols, y, mask=mask)
-        wdy = w * dy
-        dw += dy * xhat
-        if HAS_BIAS:
-            db += dy
-        if HAS_DY1:
-            wdy += w1 * dy1
-            dw1 += dy1 * xhat
-            if HAS_B1:
-                db1 += dy1
-        if not IS_RMS_NORM:
-            c1 = tl.sum(xhat * wdy, axis=0) / N
-            c2 = tl.sum(wdy, axis=0) / N
-            dx = (wdy - (xhat * c1 + c2)) * rstd
-        else:
-            c1 = tl.sum(xhat * wdy, axis=0) / N
-            dx = (wdy - xhat * c1) * rstd
-        if HAS_DRESIDUAL:
-            dres = tl.load(DRESIDUAL + cols, mask=mask, other=0).to(tl.float32)
-            dx += dres
-        # Write dx
-        if STORE_DRESIDUAL:
-            tl.store(DRESIDUAL_IN + cols, dx, mask=mask)
-        if HAS_DX1:
-            if HAS_DROPOUT:
-                keep_mask = (
-                    tl.rand(tl.load(SEEDS + M + row).to(tl.uint32), cols, n_rounds=7) > dropout_p
-                )
-                dx1 = tl.where(keep_mask, dx / (1.0 - dropout_p), 0.0)
-            else:
-                dx1 = dx
-            tl.store(DX1 + cols, dx1, mask=mask)
-        if HAS_DROPOUT:
-            keep_mask = tl.rand(tl.load(SEEDS + row).to(tl.uint32), cols, n_rounds=7) > dropout_p
-            dx = tl.where(keep_mask, dx / (1.0 - dropout_p), 0.0)
-        if HAS_ROWSCALE:
-            rowscale = tl.load(ROWSCALE + row).to(tl.float32)
-            dx *= rowscale
-        tl.store(DX + cols, dx, mask=mask)
-
-        X += stride_x_row
-        if HAS_DRESIDUAL:
-            DRESIDUAL += stride_dres_row
-        if STORE_DRESIDUAL:
-            DRESIDUAL_IN += stride_dres_in_row
-        if RECOMPUTE_OUTPUT:
-            Y += stride_y_row
-        DY += stride_dy_row
-        DX += stride_dx_row
-        if HAS_DY1:
-            DY1 += stride_dy1_row
-        if HAS_DX1:
-            DX1 += stride_dx1_row
-    tl.store(DW + row_block_id * N + cols, dw, mask=mask)
-    if HAS_BIAS:
-        tl.store(DB + row_block_id * N + cols, db, mask=mask)
-    if HAS_DY1:
-        tl.store(DW1 + row_block_id * N + cols, dw1, mask=mask)
-        if HAS_B1:
-            tl.store(DB1 + row_block_id * N + cols, db1, mask=mask)
-
-
-def _layer_norm_bwd(
-    dy,
-    x,
-    weight,
-    bias,
-    eps,
-    mean,
-    rstd,
-    dresidual=None,
-    dy1=None,
-    weight1=None,
-    bias1=None,
-    seeds=None,
-    dropout_p=0.0,
-    rowscale=None,
-    has_residual=False,
-    has_x1=False,
-    is_rms_norm=False,
-    x_dtype=None,
-    recompute_output=False,
-):
-    M, N = x.shape
-    assert x.stride(-1) == 1
-    assert dy.stride(-1) == 1
-    assert dy.shape == (M, N)
-    if dresidual is not None:
-        assert dresidual.stride(-1) == 1
-        assert dresidual.shape == (M, N)
-    assert weight.shape == (N,)
-    assert weight.stride(-1) == 1
-    if bias is not None:
-        assert bias.stride(-1) == 1
-        assert bias.shape == (N,)
-    if dy1 is not None:
-        assert weight1 is not None
-        assert dy1.shape == dy.shape
-        assert dy1.stride(-1) == 1
-    if weight1 is not None:
-        assert weight1.shape == (N,)
-        assert weight1.stride(-1) == 1
-    if bias1 is not None:
-        assert bias1.shape == (N,)
-        assert bias1.stride(-1) == 1
-    if seeds is not None:
-        assert seeds.is_contiguous()
-        assert seeds.shape == (M if not has_x1 else M * 2,)
-    if rowscale is not None:
-        assert rowscale.is_contiguous()
-        assert rowscale.shape == (M,)
-    # allocate output
-    dx = (
-        torch.empty_like(x)
-        if x_dtype is None
-        else torch.empty(M, N, dtype=x_dtype, device=x.device)
-    )
-    dresidual_in = (
-        torch.empty_like(x)
-        if has_residual
-        and (dx.dtype != x.dtype or dropout_p > 0.0 or rowscale is not None or has_x1)
-        else None
-    )
-    dx1 = torch.empty_like(dx) if (has_x1 and dropout_p > 0.0) else None
-    y = torch.empty(M, N, dtype=dy.dtype, device=dy.device) if recompute_output else None
-    if recompute_output:
-        assert weight1 is None, "recompute_output is not supported with parallel LayerNorm"
-
-    # Less than 64KB per feature: enqueue fused kernel
-    MAX_FUSED_SIZE = 65536 // x.element_size()
-    BLOCK_N = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))
-    if N > BLOCK_N:
-        raise RuntimeError("This layer norm doesn't support feature dim >= 64KB.")
-    sm_count = torch.cuda.get_device_properties(x.device).multi_processor_count
-    _dw = torch.empty((sm_count, N), dtype=torch.float32, device=weight.device)
-    _db = (
-        torch.empty((sm_count, N), dtype=torch.float32, device=bias.device)
-        if bias is not None
-        else None
-    )
-    _dw1 = torch.empty_like(_dw) if weight1 is not None else None
-    _db1 = torch.empty_like(_db) if bias1 is not None else None
-    rows_per_program = math.ceil(M / sm_count)
-    grid = (sm_count,)
-    with torch.cuda.device(x.device.index):
-        _layer_norm_bwd_kernel[grid](
-            x,
-            weight,
-            bias,
-            y,
-            dy,
-            dx,
-            _dw,
-            _db,
-            dresidual,
-            weight1,
-            dy1,
-            dx1,
-            _dw1,
-            _db1,
-            dresidual_in,
-            rowscale,
-            seeds,
-            mean,
-            rstd,
-            x.stride(0),
-            0 if not recompute_output else y.stride(0),
-            dy.stride(0),
-            dx.stride(0),
-            dresidual.stride(0) if dresidual is not None else 0,
-            dy1.stride(0) if dy1 is not None else 0,
-            dx1.stride(0) if dx1 is not None else 0,
-            dresidual_in.stride(0) if dresidual_in is not None else 0,
-            M,
-            N,
-            eps,
-            dropout_p,
-            rows_per_program,
-            is_rms_norm,
-            BLOCK_N,
-            dresidual is not None,
-            dresidual_in is not None,
-            bias is not None,
-            dropout_p > 0.0,
-        )
-    dw = _dw.sum(0).to(weight.dtype)
-    db = _db.sum(0).to(bias.dtype) if bias is not None else None
-    dw1 = _dw1.sum(0).to(weight1.dtype) if weight1 is not None else None
-    db1 = _db1.sum(0).to(bias1.dtype) if bias1 is not None else None
-    # Don't need to compute dresidual_in separately in this case
-    if has_residual and dx.dtype == x.dtype and dropout_p == 0.0 and rowscale is None:
-        dresidual_in = dx
-    if has_x1 and dropout_p == 0.0:
-        dx1 = dx
-    return (
-        (dx, dw, db, dresidual_in, dx1, dw1, db1)
-        if not recompute_output
-        else (dx, dw, db, dresidual_in, dx1, dw1, db1, y)
-    )
-
-
-class LayerNormFn(torch.autograd.Function):
-    @staticmethod
-    def forward(
-        ctx,
-        x,
-        weight,
-        bias,
-        residual=None,
-        x1=None,
-        weight1=None,
-        bias1=None,
-        eps=1e-6,
-        dropout_p=0.0,
-        rowscale=None,
-        prenorm=False,
-        residual_in_fp32=False,
-        is_rms_norm=False,
-        return_dropout_mask=False,
-    ):
-        x_shape_og = x.shape
-        # reshape input data into 2D tensor
-        x = x.reshape(-1, x.shape[-1])
-        if x.stride(-1) != 1:
-            x = x.contiguous()
-        if residual is not None:
-            assert residual.shape == x_shape_og
-            residual = residual.reshape(-1, residual.shape[-1])
-            if residual.stride(-1) != 1:
-                residual = residual.contiguous()
-        if x1 is not None:
-            assert x1.shape == x_shape_og
-            assert rowscale is None, "rowscale is not supported with parallel LayerNorm"
-            x1 = x1.reshape(-1, x1.shape[-1])
-            if x1.stride(-1) != 1:
-                x1 = x1.contiguous()
-        weight = weight.contiguous()
-        if bias is not None:
-            bias = bias.contiguous()
-        if weight1 is not None:
-            weight1 = weight1.contiguous()
-        if bias1 is not None:
-            bias1 = bias1.contiguous()
-        if rowscale is not None:
-            rowscale = rowscale.reshape(-1).contiguous()
-        residual_dtype = (
-            residual.dtype
-            if residual is not None
-            else (torch.float32 if residual_in_fp32 else None)
-        )
-        y, y1, mean, rstd, residual_out, seeds, dropout_mask, dropout_mask1 = _layer_norm_fwd(
-            x,
-            weight,
-            bias,
-            eps,
-            residual,
-            x1,
-            weight1,
-            bias1,
-            dropout_p=dropout_p,
-            rowscale=rowscale,
-            residual_dtype=residual_dtype,
-            is_rms_norm=is_rms_norm,
-            return_dropout_mask=return_dropout_mask,
-        )
-        ctx.save_for_backward(
-            residual_out, weight, bias, weight1, bias1, rowscale, seeds, mean, rstd
-        )
-        ctx.x_shape_og = x_shape_og
-        ctx.eps = eps
-        ctx.dropout_p = dropout_p
-        ctx.is_rms_norm = is_rms_norm
-        ctx.has_residual = residual is not None
-        ctx.has_x1 = x1 is not None
-        ctx.prenorm = prenorm
-        ctx.x_dtype = x.dtype
-        y = y.reshape(x_shape_og)
-        y1 = y1.reshape(x_shape_og) if y1 is not None else None
-        residual_out = residual_out.reshape(x_shape_og) if residual_out is not None else None
-        dropout_mask = dropout_mask.reshape(x_shape_og) if dropout_mask is not None else None
-        dropout_mask1 = dropout_mask1.reshape(x_shape_og) if dropout_mask1 is not None else None
-        if not return_dropout_mask:
-            if weight1 is None:
-                return y if not prenorm else (y, residual_out)
-            else:
-                return (y, y1) if not prenorm else (y, y1, residual_out)
-        else:
-            if weight1 is None:
-                return (
-                    (y, dropout_mask, dropout_mask1)
-                    if not prenorm
-                    else (y, residual_out, dropout_mask, dropout_mask1)
-                )
-            else:
-                return (
-                    (y, y1, dropout_mask, dropout_mask1)
-                    if not prenorm
-                    else (y, y1, residual_out, dropout_mask, dropout_mask1)
-                )
-
-    @staticmethod
-    def backward(ctx, dy, *args):
-        x, weight, bias, weight1, bias1, rowscale, seeds, mean, rstd = ctx.saved_tensors
-        dy = dy.reshape(-1, dy.shape[-1])
-        if dy.stride(-1) != 1:
-            dy = dy.contiguous()
-        assert dy.shape == x.shape
-        if weight1 is not None:
-            dy1, args = args[0], args[1:]
-            dy1 = dy1.reshape(-1, dy1.shape[-1])
-            if dy1.stride(-1) != 1:
-                dy1 = dy1.contiguous()
-            assert dy1.shape == x.shape
-        else:
-            dy1 = None
-        if ctx.prenorm:
-            dresidual = args[0]
-            dresidual = dresidual.reshape(-1, dresidual.shape[-1])
-            if dresidual.stride(-1) != 1:
-                dresidual = dresidual.contiguous()
-            assert dresidual.shape == x.shape
-        else:
-            dresidual = None
-        dx, dw, db, dresidual_in, dx1, dw1, db1 = _layer_norm_bwd(
-            dy,
-            x,
-            weight,
-            bias,
-            ctx.eps,
-            mean,
-            rstd,
-            dresidual,
-            dy1,
-            weight1,
-            bias1,
-            seeds,
-            ctx.dropout_p,
-            rowscale,
-            ctx.has_residual,
-            ctx.has_x1,
-            ctx.is_rms_norm,
-            x_dtype=ctx.x_dtype,
-        )
-        return (
-            dx.reshape(ctx.x_shape_og),
-            dw,
-            db,
-            dresidual_in.reshape(ctx.x_shape_og) if ctx.has_residual else None,
-            dx1.reshape(ctx.x_shape_og) if dx1 is not None else None,
-            dw1,
-            db1,
-            None,
-            None,
-            None,
-            None,
-            None,
-            None,
-            None,
-        )
-
-
-def layer_norm_fn(
-    x,
-    weight,
-    bias,
-    residual=None,
-    x1=None,
-    weight1=None,
-    bias1=None,
-    eps=1e-6,
-    dropout_p=0.0,
-    rowscale=None,
-    prenorm=False,
-    residual_in_fp32=False,
-    is_rms_norm=False,
-    return_dropout_mask=False,
-):
-    return LayerNormFn.apply(
-        x,
-        weight,
-        bias,
-        residual,
-        x1,
-        weight1,
-        bias1,
-        eps,
-        dropout_p,
-        rowscale,
-        prenorm,
-        residual_in_fp32,
-        is_rms_norm,
-        return_dropout_mask,
-    )
-
-
-def rms_norm_fn(
-    x,
-    weight,
-    bias,
-    residual=None,
-    x1=None,
-    weight1=None,
-    bias1=None,
-    eps=1e-6,
-    dropout_p=0.0,
-    rowscale=None,
-    prenorm=False,
-    residual_in_fp32=False,
-    return_dropout_mask=False,
-):
-    return LayerNormFn.apply(
-        x,
-        weight,
-        bias,
-        residual,
-        x1,
-        weight1,
-        bias1,
-        eps,
-        dropout_p,
-        rowscale,
-        prenorm,
-        residual_in_fp32,
-        True,
-        return_dropout_mask,
-    )
-
-
-class RMSNorm(torch.nn.Module):
-
-    def __init__(self, hidden_size, eps=1e-5, dropout_p=0.0, device=None, dtype=None):
-        factory_kwargs = {"device": device, "dtype": dtype}
-        super().__init__()
-        self.eps = eps
-        if dropout_p > 0.0:
-            self.drop = torch.nn.Dropout(dropout_p)
-        else:
-            self.drop = None
-        self.weight = torch.nn.Parameter(torch.empty(hidden_size, **factory_kwargs))
-        self.register_parameter("bias", None)
-        self.reset_parameters()
-
-    def reset_parameters(self):
-        torch.nn.init.ones_(self.weight)
-
-    def forward(self, x, residual=None, prenorm=False, residual_in_fp32=False):
-        return rms_norm_fn(
-            x,
-            self.weight,
-            self.bias,
-            residual=residual,
-            eps=self.eps,
-            dropout_p=self.drop.p if self.drop is not None and self.training else 0.0,
-            prenorm=prenorm,
-            residual_in_fp32=residual_in_fp32,
-        )
-
-
-class LayerNormLinearFn(torch.autograd.Function):
-    @staticmethod
-    @custom_fwd
-    def forward(
-        ctx,
-        x,
-        norm_weight,
-        norm_bias,
-        linear_weight,
-        linear_bias,
-        residual=None,
-        eps=1e-6,
-        prenorm=False,
-        residual_in_fp32=False,
-        is_rms_norm=False,
-    ):
-        x_shape_og = x.shape
-        # reshape input data into 2D tensor
-        x = x.reshape(-1, x.shape[-1])
-        if x.stride(-1) != 1:
-            x = x.contiguous()
-        if residual is not None:
-            assert residual.shape == x_shape_og
-            residual = residual.reshape(-1, residual.shape[-1])
-            if residual.stride(-1) != 1:
-                residual = residual.contiguous()
-        norm_weight = norm_weight.contiguous()
-        if norm_bias is not None:
-            norm_bias = norm_bias.contiguous()
-        residual_dtype = (
-            residual.dtype
-            if residual is not None
-            else (torch.float32 if residual_in_fp32 else None)
-        )
-        y, mean, rstd, residual_out = _layer_norm_fwd(
-            x,
-            norm_weight,
-            norm_bias,
-            eps,
-            residual,
-            out_dtype=None if not torch.is_autocast_enabled() else torch.get_autocast_gpu_dtype(),
-            residual_dtype=residual_dtype,
-            is_rms_norm=is_rms_norm,
-        )
-        y = y.reshape(x_shape_og)
-        dtype = torch.get_autocast_gpu_dtype() if torch.is_autocast_enabled() else y.dtype
-        linear_weight = linear_weight.to(dtype)
-        linear_bias = linear_bias.to(dtype) if linear_bias is not None else None
-        out = F.linear(y.to(linear_weight.dtype), linear_weight, linear_bias)
-        # We don't store y, will be recomputed in the backward pass to save memory
-        ctx.save_for_backward(residual_out, norm_weight, norm_bias, linear_weight, mean, rstd)
-        ctx.x_shape_og = x_shape_og
-        ctx.eps = eps
-        ctx.is_rms_norm = is_rms_norm
-        ctx.has_residual = residual is not None
-        ctx.prenorm = prenorm
-        ctx.x_dtype = x.dtype
-        ctx.linear_bias_is_none = linear_bias is None
-        return out if not prenorm else (out, residual_out.reshape(x_shape_og))
-
-    @staticmethod
-    @custom_bwd
-    def backward(ctx, dout, *args):
-        x, norm_weight, norm_bias, linear_weight, mean, rstd = ctx.saved_tensors
-        dout = dout.reshape(-1, dout.shape[-1])
-        dy = F.linear(dout, linear_weight.t())
-        dlinear_bias = None if ctx.linear_bias_is_none else dout.sum(0)
-        if dy.stride(-1) != 1:
-            dy = dy.contiguous()
-        assert dy.shape == x.shape
-        if ctx.prenorm:
-            dresidual = args[0]
-            dresidual = dresidual.reshape(-1, dresidual.shape[-1])
-            if dresidual.stride(-1) != 1:
-                dresidual = dresidual.contiguous()
-            assert dresidual.shape == x.shape
-        else:
-            dresidual = None
-        dx, dnorm_weight, dnorm_bias, dresidual_in, y = _layer_norm_bwd(
-            dy,
-            x,
-            norm_weight,
-            norm_bias,
-            ctx.eps,
-            mean,
-            rstd,
-            dresidual,
-            ctx.has_residual,
-            ctx.is_rms_norm,
-            x_dtype=ctx.x_dtype,
-            recompute_output=True,
-        )
-        dlinear_weight = torch.einsum("bo,bi->oi", dout, y)
-        return (
-            dx.reshape(ctx.x_shape_og),
-            dnorm_weight,
-            dnorm_bias,
-            dlinear_weight,
-            dlinear_bias,
-            dresidual_in.reshape(ctx.x_shape_og) if ctx.has_residual else None,
-            None,
-            None,
-            None,
-            None,
-        )
-
-
-def layer_norm_linear_fn(
-    x,
-    norm_weight,
-    norm_bias,
-    linear_weight,
-    linear_bias,
-    residual=None,
-    eps=1e-6,
-    prenorm=False,
-    residual_in_fp32=False,
-    is_rms_norm=False,
-):
-    return LayerNormLinearFn.apply(
-        x,
-        norm_weight,
-        norm_bias,
-        linear_weight,
-        linear_bias,
-        residual,
-        eps,
-        prenorm,
-        residual_in_fp32,
-        is_rms_norm,
-    )
diff --git a/based/ops/triton/linear.py b/based/ops/triton/linear.py
deleted file mode 100644
index a8966db..0000000
--- a/based/ops/triton/linear.py
+++ /dev/null
@@ -1,594 +0,0 @@
-# Adapted from https://github.com/ELS-RD/kernl/blob/main/src/kernl/implementations/linear_layer.py
-# and https://github.com/openai/triton/blob/master/python/triton/ops/matmul.py
-from typing import Optional
-
-import torch
-import triton
-import triton.language as tl
-from triton.ops.matmul_perf_model import early_config_prune, estimate_matmul_time
-
-from flash_attn.ops.triton.k_activations import (
-    gelu,
-    gelu_approx,
-    gelu_approx_grad,
-    gelu_grad,
-    squared_relu,
-    squared_relu_grad,
-)
-
-# CREDITS: Initially inspired by the Triton tutorial on matrix multiplications
-
-
-def init_to_zero(name):
-    return lambda nargs: nargs[name].zero_()
-
-
-def get_configs_io_bound():
-    configs = []
-    for num_stages in [2, 3, 4, 5, 6]:
-        for block_m in [16, 32]:
-            for block_k in [32, 64]:
-                for block_n in [32, 64, 128, 256]:
-                    num_warps = 2 if block_n <= 64 else 4
-                    configs.append(
-                        triton.Config(
-                            {
-                                "BLOCK_M": block_m,
-                                "BLOCK_N": block_n,
-                                "BLOCK_K": block_k,
-                                "SPLIT_K": 1,
-                            },
-                            num_stages=num_stages,
-                            num_warps=num_warps,
-                        )
-                    )
-                    # split_k not used
-                    # for split_k in [2, 4, 8, 16]:
-                    #     configs.append(triton.Config(
-                    #         {'BLOCK_M': block_m, 'BLOCK_N': block_n, 'BLOCK_K': block_k, 'SPLIT_K': split_k},
-                    #         num_stages=num_stages, num_warps=num_warps, pre_hook=init_to_zero('C')))
-    return configs
-
-
-@triton.autotune(
-    configs=[
-        triton.Config(
-            {"BLOCK_M": 128, "BLOCK_N": 256, "BLOCK_K": 32, "SPLIT_K": 1}, num_stages=3, num_warps=8
-        ),
-        triton.Config(
-            {"BLOCK_M": 256, "BLOCK_N": 128, "BLOCK_K": 32, "SPLIT_K": 1}, num_stages=3, num_warps=8
-        ),
-        triton.Config(
-            {"BLOCK_M": 256, "BLOCK_N": 64, "BLOCK_K": 32, "SPLIT_K": 1}, num_stages=4, num_warps=4
-        ),
-        triton.Config(
-            {"BLOCK_M": 64, "BLOCK_N": 256, "BLOCK_K": 32, "SPLIT_K": 1}, num_stages=4, num_warps=4
-        ),
-        triton.Config(
-            {"BLOCK_M": 128, "BLOCK_N": 128, "BLOCK_K": 32, "SPLIT_K": 1}, num_stages=4, num_warps=4
-        ),
-        triton.Config(
-            {"BLOCK_M": 128, "BLOCK_N": 64, "BLOCK_K": 32, "SPLIT_K": 1}, num_stages=4, num_warps=4
-        ),
-        triton.Config(
-            {"BLOCK_M": 64, "BLOCK_N": 128, "BLOCK_K": 32, "SPLIT_K": 1}, num_stages=4, num_warps=4
-        ),
-        triton.Config(
-            {"BLOCK_M": 128, "BLOCK_N": 32, "BLOCK_K": 32, "SPLIT_K": 1}, num_stages=4, num_warps=4
-        ),
-        triton.Config(
-            {"BLOCK_M": 64, "BLOCK_N": 32, "BLOCK_K": 32, "SPLIT_K": 1}, num_stages=5, num_warps=2
-        ),
-        # good for int8
-        triton.Config(
-            {"BLOCK_M": 128, "BLOCK_N": 256, "BLOCK_K": 128, "SPLIT_K": 1},
-            num_stages=3,
-            num_warps=8,
-        ),
-        triton.Config(
-            {"BLOCK_M": 256, "BLOCK_N": 128, "BLOCK_K": 128, "SPLIT_K": 1},
-            num_stages=3,
-            num_warps=8,
-        ),
-        triton.Config(
-            {"BLOCK_M": 256, "BLOCK_N": 64, "BLOCK_K": 128, "SPLIT_K": 1}, num_stages=4, num_warps=4
-        ),
-        triton.Config(
-            {"BLOCK_M": 64, "BLOCK_N": 256, "BLOCK_K": 128, "SPLIT_K": 1}, num_stages=4, num_warps=4
-        ),
-        triton.Config(
-            {"BLOCK_M": 128, "BLOCK_N": 128, "BLOCK_K": 128, "SPLIT_K": 1},
-            num_stages=4,
-            num_warps=4,
-        ),
-        triton.Config(
-            {"BLOCK_M": 128, "BLOCK_N": 64, "BLOCK_K": 64, "SPLIT_K": 1}, num_stages=4, num_warps=4
-        ),
-        triton.Config(
-            {"BLOCK_M": 64, "BLOCK_N": 128, "BLOCK_K": 64, "SPLIT_K": 1}, num_stages=4, num_warps=4
-        ),
-        triton.Config(
-            {"BLOCK_M": 128, "BLOCK_N": 32, "BLOCK_K": 64, "SPLIT_K": 1}, num_stages=4, num_warps=4
-        ),
-        triton.Config(
-            {"BLOCK_M": 64, "BLOCK_N": 32, "BLOCK_K": 64, "SPLIT_K": 1}, num_stages=5, num_warps=2
-        ),
-    ]
-    + get_configs_io_bound(),
-    key=["CACHE_KEY_M", "CACHE_KEY_N", "CACHE_KEY_K"],
-    prune_configs_by={
-        "early_config_prune": early_config_prune,
-        "perf_model": estimate_matmul_time,
-        "top_k": 10,
-    },
-)
-@triton.heuristics(
-    {
-        "EVEN_K": lambda args: args["K"] % (args["BLOCK_K"] * args["SPLIT_K"]) == 0,
-    }
-)
-@triton.jit
-def kernel_fwd(
-    C,  # Pointers to matrices
-    ACT_INPUT,
-    A,
-    B,
-    bias,
-    # Matrix dimensions
-    M,
-    N,
-    K,
-    CACHE_KEY_M,
-    CACHE_KEY_N,
-    CACHE_KEY_K,
-    # The stride variables represent how much to increase the ptr by when moving by 1
-    # element in a particular dimension. E.g. stride_am is how much to increase a_ptr
-    # by to get the element one row down (A has M rows)
-    stride_cm,
-    # stride_cn,  # Assume that stride_cn == 1
-    stride_am,
-    stride_ak,
-    stride_bn,
-    stride_bk,
-    # Meta-parameters
-    BLOCK_M: tl.constexpr,
-    GROUP_M: tl.constexpr,
-    BLOCK_N: tl.constexpr,
-    BLOCK_K: tl.constexpr,
-    # split k not used, not performant with activation, kept because early_config_prune is expecting it
-    SPLIT_K: tl.constexpr,
-    EVEN_K: tl.constexpr,
-    A_ROWMAJOR: tl.constexpr,
-    B_COLMAJOR: tl.constexpr,
-    BIAS: tl.constexpr,
-    SAVE_ACT_INPUT: tl.constexpr,
-    ACTIVATION: tl.constexpr,
-):
-
-    """
-    Kernel for computing Out = activation(A x W + C)
-    - Input has shape (M, K)
-    - Weight has shape (K, N)
-    - Bias has shape (N,)
-    - Output has shape (M, N)
-    - ActInputs (optional) has shape (M, N)
-    'ActInputs' optionally saves the A x W + C intermediate for backward computations
-    This kernel will consolidate over K
-    """
-
-    pid = tl.program_id(axis=0)
-
-    grid_m = (M + BLOCK_M - 1) // BLOCK_M
-    grid_n = (N + BLOCK_N - 1) // BLOCK_N
-    # re-order program ID for better L2 performance
-    width = GROUP_M * grid_n
-    group_id = pid // width
-    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
-    pid_m = group_id * GROUP_M + (pid % group_size)
-    pid_n = (pid % width) // (group_size)
-
-    # now compute the block that each program will go through
-    # rm (resp. rn) denotes a range of indices
-    # for rows (resp. col) of C
-    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
-    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
-    # trick to avoid masking on M and N axis
-    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
-    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
-    rk = tl.arange(0, BLOCK_K)
-
-    if A_ROWMAJOR:
-        A = A + (ram[:, None] * stride_am + rk[None, :])
-    else:
-        A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
-    if B_COLMAJOR:
-        B = B + (rk[:, None] + rbn[None, :] * stride_bn)
-    else:
-        B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)
-
-    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)
-
-    for k in range(K, 0, -BLOCK_K):
-        if EVEN_K:
-            a = tl.load(A)
-            b = tl.load(B)
-        else:
-            a = tl.load(A, mask=rk[None, :] < k, other=0.0)
-            b = tl.load(B, mask=rk[:, None] < k, other=0.0)
-        acc += tl.dot(a, b)
-
-        if A_ROWMAJOR:
-            A += BLOCK_K
-        else:
-            A += BLOCK_K * stride_ak
-        if B_COLMAJOR:
-            B += BLOCK_K
-        else:
-            B += BLOCK_K * stride_bk
-
-    # Putting bias after the matmul (instead of before) is faster, idk why
-    if BIAS:
-        bias = tl.load(bias + rn, mask=rn < N, other=0.0).to(tl.float32)
-        acc += bias[None, :]
-
-    # optional: save the activation inputs
-    if SAVE_ACT_INPUT:
-        # act_in_ptrs = ACT_INPUT + ram[:, None] * stride_cm + rbn[None, :] * stride_cn
-        act_in_ptrs = ACT_INPUT + ram[:, None] * stride_cm + rbn[None, :]
-        tl.store(act_in_ptrs, acc)
-
-    # optional: fused activation (while the data is in shared memory)
-    if ACTIVATION == "gelu":
-        acc = gelu(acc)
-    elif ACTIVATION == "gelu_approx":
-        acc = gelu_approx(acc)
-    elif ACTIVATION == "squared_relu":
-        acc = squared_relu(acc)
-    # rematerialize rm and rn to save registers
-    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
-    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
-
-    # write back result
-    # C = C + rm[:, None] * stride_cm + rn[None, :] * stride_cn
-    C = C + rm[:, None] * stride_cm + rn[None, :]
-    mask = (rm < M)[:, None] & (rn < N)[None, :]
-    tl.store(C, acc)
-
-
-def triton_linear_act(
-    x: torch.Tensor,
-    weight: torch.Tensor,
-    bias: Optional[torch.Tensor] = None,
-    activation: str = "id",
-    save_act_input: bool = False,
-) -> torch.Tensor:
-    """
-    Compute e = activation(x @ weight.T + bias).
-    This wrapper kicks the `kernel_fwd` Triton kernel
-    :param x: input tensor
-    :param weight: weight matrix
-    :param bias: an optional bias tensor
-    :param activation: Activation name. Needs to be a Triton kernel.
-    :param act_input: an optional tensor to save the activation inputs (for backward)
-    :return: result tensor
-    """
-    # if torch.is_autocast_enabled():
-    #     dtype = torch.get_autocast_gpu_dtype()
-    #     x, weight, bias = [a.to(dtype=dtype) for a in [x, weight, bias]]
-
-    assert activation in ["id", "gelu", "gelu_approx", "squared_relu"]
-
-    batch_shape, n = x.shape[:-1], x.shape[-1]
-    batch_dim = batch_shape.numel()
-    x_reshaped = x.reshape(batch_dim, n)
-
-    if x_reshaped.stride(0) > 1 and x_reshaped.stride(1) > 1:
-        x_reshaped = x_reshaped.contiguous()
-    if weight.stride(0) > 1 and weight.stride(1) > 1:
-        weight = weight.contiguous()
-    bias = bias.contiguous() if bias is not None else None
-
-    assert (
-        x.dtype == weight.dtype
-    ), f"Input and weight must have the same dtype, got {x.dtype} and {weight.dtype}"
-    if bias is not None:
-        assert (
-            x.dtype == bias.dtype
-        ), f"Input and bias must have the same dtype, got {x.dtype} and {bias.dtype}"
-    assert (
-        x_reshaped.shape[1] == weight.shape[1]
-    ), f"Incompatible dimensions: {x_reshaped.shape} - {weight.shape}"
-
-    assert (
-        bias is None or bias.shape[0] == weight.shape[0]
-    ), "Incompatible dimensions in between weight and bias"
-
-    M, K = x_reshaped.shape
-    N, K = weight.shape
-
-    output = torch.empty((M, N), device=x.device, dtype=x.dtype)
-    act_input = torch.empty_like(output) if save_act_input else None
-
-    # 1D launch kernel where each block gets its own program.
-    grid = lambda META: (triton.cdiv(M, META["BLOCK_M"]) * triton.cdiv(N, META["BLOCK_N"]),)  # noqa
-
-    kernel_fwd[grid](
-        output,
-        act_input,
-        x_reshaped,
-        weight,  # data ptrs
-        bias if bias is not None else x,  # auto skip bias if not present
-        M,  # shapes
-        N,
-        K,
-        M // 32,  # key for triton cache (limit number of compilations)
-        N // 32,
-        K // 32,
-        stride_cm=output.stride(0),  # strides
-        # stride_cn=output.stride(1),
-        stride_am=x_reshaped.stride(0),
-        stride_ak=x_reshaped.stride(1),
-        stride_bk=weight.stride(1),
-        stride_bn=weight.stride(0),
-        BIAS=bias is not None,  # optional fused bias
-        SAVE_ACT_INPUT=save_act_input,  # optional save activation inputs
-        ACTIVATION=activation,  # optional fused activation
-        A_ROWMAJOR=x_reshaped.stride(1) == 1,
-        B_COLMAJOR=weight.stride(1) == 1,
-        GROUP_M=8,  # speed optimization: group the programs
-    )
-
-    if not save_act_input:
-        return output.reshape(*batch_shape, output.shape[-1])
-    else:
-        return (
-            output.reshape(*batch_shape, output.shape[-1]),
-            act_input.reshape(*batch_shape, act_input.shape[-1]),
-        )
-
-
-@triton.autotune(
-    configs=[
-        triton.Config(
-            {"BLOCK_M": 128, "BLOCK_N": 256, "BLOCK_K": 32, "SPLIT_K": 1}, num_stages=3, num_warps=8
-        ),
-        triton.Config(
-            {"BLOCK_M": 256, "BLOCK_N": 128, "BLOCK_K": 32, "SPLIT_K": 1}, num_stages=3, num_warps=8
-        ),
-        triton.Config(
-            {"BLOCK_M": 256, "BLOCK_N": 64, "BLOCK_K": 32, "SPLIT_K": 1}, num_stages=4, num_warps=4
-        ),
-        triton.Config(
-            {"BLOCK_M": 64, "BLOCK_N": 256, "BLOCK_K": 32, "SPLIT_K": 1}, num_stages=4, num_warps=4
-        ),
-        triton.Config(
-            {"BLOCK_M": 128, "BLOCK_N": 128, "BLOCK_K": 32, "SPLIT_K": 1}, num_stages=4, num_warps=4
-        ),
-        triton.Config(
-            {"BLOCK_M": 128, "BLOCK_N": 64, "BLOCK_K": 32, "SPLIT_K": 1}, num_stages=4, num_warps=4
-        ),
-        triton.Config(
-            {"BLOCK_M": 64, "BLOCK_N": 128, "BLOCK_K": 32, "SPLIT_K": 1}, num_stages=4, num_warps=4
-        ),
-        triton.Config(
-            {"BLOCK_M": 128, "BLOCK_N": 32, "BLOCK_K": 32, "SPLIT_K": 1}, num_stages=4, num_warps=4
-        ),
-        triton.Config(
-            {"BLOCK_M": 64, "BLOCK_N": 32, "BLOCK_K": 32, "SPLIT_K": 1}, num_stages=5, num_warps=2
-        ),
-        # good for int8
-        triton.Config(
-            {"BLOCK_M": 128, "BLOCK_N": 256, "BLOCK_K": 128, "SPLIT_K": 1},
-            num_stages=3,
-            num_warps=8,
-        ),
-        triton.Config(
-            {"BLOCK_M": 256, "BLOCK_N": 128, "BLOCK_K": 128, "SPLIT_K": 1},
-            num_stages=3,
-            num_warps=8,
-        ),
-        triton.Config(
-            {"BLOCK_M": 256, "BLOCK_N": 64, "BLOCK_K": 128, "SPLIT_K": 1}, num_stages=4, num_warps=4
-        ),
-        triton.Config(
-            {"BLOCK_M": 64, "BLOCK_N": 256, "BLOCK_K": 128, "SPLIT_K": 1}, num_stages=4, num_warps=4
-        ),
-        triton.Config(
-            {"BLOCK_M": 128, "BLOCK_N": 128, "BLOCK_K": 128, "SPLIT_K": 1},
-            num_stages=4,
-            num_warps=4,
-        ),
-        triton.Config(
-            {"BLOCK_M": 128, "BLOCK_N": 64, "BLOCK_K": 64, "SPLIT_K": 1}, num_stages=4, num_warps=4
-        ),
-        triton.Config(
-            {"BLOCK_M": 64, "BLOCK_N": 128, "BLOCK_K": 64, "SPLIT_K": 1}, num_stages=4, num_warps=4
-        ),
-        triton.Config(
-            {"BLOCK_M": 128, "BLOCK_N": 32, "BLOCK_K": 64, "SPLIT_K": 1}, num_stages=4, num_warps=4
-        ),
-        triton.Config(
-            {"BLOCK_M": 64, "BLOCK_N": 32, "BLOCK_K": 64, "SPLIT_K": 1}, num_stages=5, num_warps=2
-        ),
-    ]
-    + get_configs_io_bound(),
-    key=["CACHE_KEY_M", "CACHE_KEY_N", "CACHE_KEY_K"],
-    prune_configs_by={
-        "early_config_prune": early_config_prune,
-        "perf_model": estimate_matmul_time,
-        "top_k": 10,
-    },
-)
-@triton.heuristics(
-    {
-        "EVEN_K": lambda args: args["K"] % (args["BLOCK_K"] * args["SPLIT_K"]) == 0,
-    }
-)
-@triton.jit
-def kernel_bwd(
-    C,  # Pointers to matrices
-    ACT_INPUT,
-    A,
-    B,
-    # Matrix dimensions
-    M,
-    N,
-    K,
-    CACHE_KEY_M,
-    CACHE_KEY_N,
-    CACHE_KEY_K,
-    # The stride variables represent how much to increase the ptr by when moving by 1
-    # element in a particular dimension. E.g. stride_am is how much to increase a_ptr
-    # by to get the element one row down (A has M rows)
-    stride_cm,
-    # stride_cn,  # Assume that stride_cn == 1
-    stride_am,
-    stride_ak,
-    stride_bk,
-    stride_bn,
-    # Meta-parameters
-    BLOCK_M: tl.constexpr,
-    GROUP_M: tl.constexpr,
-    BLOCK_N: tl.constexpr,
-    BLOCK_K: tl.constexpr,
-    # split k not used, not performant with activation, kept because early_config_prune is expecting it
-    SPLIT_K: tl.constexpr,
-    EVEN_K: tl.constexpr,
-    ACTIVATION: tl.constexpr,
-):
-
-    """
-    Kernel for computing Out = activation(A x W + C)
-    - Input has shape (M, K)
-    - Weight has shape (K, N)
-    - Output has shape (M, N)
-    - ActInputs (optional) has shape (M, N)
-    'ActInputs' optionally saves the A x W + C intermediate for backward computations
-    This kernel will consolidate over K
-    """
-
-    pid = tl.program_id(axis=0)
-
-    grid_m = (M + BLOCK_M - 1) // BLOCK_M
-    grid_n = (N + BLOCK_N - 1) // BLOCK_N
-    # re-order program ID for better L2 performance
-    width = GROUP_M * grid_n
-    group_id = pid // width
-    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
-    pid_m = group_id * GROUP_M + (pid % group_size)
-    pid_n = (pid % width) // (group_size)
-
-    # now compute the block that each program will go through
-    # rm (resp. rn) denotes a range of indices
-    # for rows (resp. col) of C
-    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
-    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
-    # trick to avoid masking on M and N axis
-    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
-    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
-    rk = tl.arange(0, BLOCK_K)
-
-    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
-    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)
-
-    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)
-
-    for k in range(K, 0, -BLOCK_K):
-        if EVEN_K:
-            a = tl.load(A)
-            b = tl.load(B)
-        else:
-            a = tl.load(A, mask=rk[None, :] < k, other=0.0)
-            b = tl.load(B, mask=rk[:, None] < k, other=0.0)
-        acc += tl.dot(a, b)
-
-        A += BLOCK_K * stride_ak
-        B += BLOCK_K * stride_bk
-
-    # optional: fused activation (while the data is in shared memory)
-    if ACTIVATION != "id":
-        act_in_ptrs = ACT_INPUT + ram[:, None] * stride_cm + rbn[None, :]
-        act_input = tl.load(act_in_ptrs).to(acc.dtype)
-    if ACTIVATION == "gelu":
-        acc *= gelu_grad(act_input)
-    elif ACTIVATION == "gelu_approx":
-        acc *= gelu_approx_grad(act_input)
-    elif ACTIVATION == "squared_relu":
-        acc *= squared_relu_grad(act_input)
-
-    # rematerialize rm and rn to save registers
-    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
-    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
-
-    # write back result
-    C = C + rm[:, None] * stride_cm + rn[None, :]
-    mask = (rm < M)[:, None] & (rn < N)[None, :]
-    tl.store(C, acc, mask=mask)
-
-
-def triton_dgrad_act(
-    grad_output: torch.Tensor,
-    weight: torch.Tensor,
-    activation: str = "id",
-    act_input: Optional[torch.Tensor] = None,
-) -> torch.Tensor:
-    """
-    Compute e = activation(grad_output @ weight + bias).
-    This wrapper kicks the `kernel_fwd` Triton kernel
-    :param grad_output: input tensor
-    :param weight: weight matrix
-    :param activation: Activation name. Needs to be a Triton kernel.
-    :param act_input: an optional tensor to save the activation inputs (for backward)
-    :return: result tensor
-    """
-    assert activation in ["id", "gelu", "gelu_approx", "squared_relu"]
-
-    batch_shape, n = grad_output.shape[:-1], grad_output.shape[-1]
-    batch_dim = batch_shape.numel()
-    grad_output_reshaped = grad_output.reshape(batch_dim, n)
-
-    if grad_output_reshaped.stride(0) > 1 and grad_output_reshaped.stride(1) > 1:
-        grad_output_reshaped = grad_output_reshaped.contiguous()
-    if weight.stride(0) > 1 and weight.stride(1) > 1:
-        weight = weight.contiguous()
-
-    assert (
-        grad_output.dtype == weight.dtype
-    ), f"grad_output and weight must have the same dtype, got {grad_output.dtype} and {weight.dtype}"
-    assert (
-        grad_output_reshaped.shape[1] == weight.shape[0]
-    ), f"Incompatible dimensions: {grad_output_reshaped.shape} - {weight.shape}"
-    if activation != "id":
-        assert act_input is not None, f"act_input is required for activation {activation}"
-
-    # M, N, K in bwd are different from M, N, K in fwd
-    M, K = grad_output_reshaped.shape
-    K, N = weight.shape
-
-    grad_input = torch.empty((M, N), device=grad_output.device, dtype=grad_output.dtype)
-
-    # 1D launch kernel where each block gets its own program.
-    grid = lambda META: (triton.cdiv(M, META["BLOCK_M"]) * triton.cdiv(N, META["BLOCK_N"]),)  # noqa
-
-    kernel_bwd[grid](
-        grad_input,
-        act_input,
-        grad_output_reshaped,
-        weight,  # data ptrs
-        M,  # shapes
-        N,
-        K,
-        M // 32,  # key for triton cache (limit number of compilations)
-        N // 32,
-        K // 32,
-        stride_cm=grad_input.stride(0),  # strides
-        # stride_cn=grad_input.stride(1),
-        stride_am=grad_output_reshaped.stride(0),
-        stride_ak=grad_output_reshaped.stride(1),
-        stride_bk=weight.stride(0),
-        stride_bn=weight.stride(1),
-        ACTIVATION=activation,  # optional fused activation
-        GROUP_M=8,  # speed optimization: group the programs
-    )
-
-    return grad_input.reshape(*batch_shape, grad_input.shape[-1])
diff --git a/based/ops/triton/mlp.py b/based/ops/triton/mlp.py
deleted file mode 100644
index b795310..0000000
--- a/based/ops/triton/mlp.py
+++ /dev/null
@@ -1,149 +0,0 @@
-# The triton fused matmul + sqrelu is faster for fp16 but slower for bf16, compared
-# to naive implementation.
-import fused_dense_lib as fused_dense_cuda
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-from torch.cuda.amp import custom_bwd, custom_fwd
-
-from flash_attn.ops.activations import sqrelu_bwd, sqrelu_fwd
-from flash_attn.ops.triton.linear import triton_dgrad_act, triton_linear_act
-
-
-class FusedDenseSqreluDenseFunc(torch.autograd.Function):
-    @staticmethod
-    @custom_fwd
-    def forward(ctx, x, weight1, bias1, weight2, bias2, checkpoint_lvl=0):
-        """checkpoint_lvl:
-        0: no recomputation in the bwd
-        1: recompute gelu_out in the bwd
-        2: recompute act_input and gelu_out in the bwd
-        """
-        if torch.is_autocast_enabled():
-            dtype = torch.get_autocast_gpu_dtype()
-            x, weight1, bias1, weight2, bias2 = [
-                a.to(dtype=dtype) for a in [x, weight1, bias1, weight2, bias2]
-            ]
-        is_bf16 = x.dtype == torch.bfloat16
-        assert checkpoint_lvl in [0, 1, 2]
-        x = x.contiguous()
-        weight1 = weight1.contiguous()
-        bias1 = bias1.contiguous()
-        weight2 = weight2.contiguous()
-        bias2 = bias2.contiguous()
-        batch_shape, n = x.shape[:-1], x.shape[-1]
-        batch_dim = batch_shape.numel()
-        if is_bf16:
-            act_input = fused_dense_cuda.linear_bias_forward(
-                x.reshape(batch_dim, n), weight1, bias1
-            )
-            output1 = sqrelu_fwd(act_input)
-        else:
-            save_act_input = checkpoint_lvl != 2
-            result = triton_linear_act(
-                x.reshape(batch_dim, n),
-                weight1,
-                bias1,
-                activation="squared_relu",
-                save_act_input=save_act_input,
-            )
-            if save_act_input:
-                output1, act_input = result
-            else:
-                output1 = result
-        output2 = fused_dense_cuda.linear_bias_forward(output1, weight2, bias2)
-        ctx.checkpoint_lvl = checkpoint_lvl
-        if checkpoint_lvl == 0:
-            ctx.save_for_backward(x, weight1, bias1, weight2, act_input, output1)
-        elif checkpoint_lvl == 1:
-            ctx.save_for_backward(x, weight1, bias1, weight2, act_input)
-        elif checkpoint_lvl == 2:
-            ctx.save_for_backward(x, weight1, bias1, weight2)
-        return output2.reshape(*batch_shape, output2.shape[-1])
-
-    @staticmethod
-    @custom_bwd
-    def backward(ctx, grad_output):
-        grad_output = grad_output.contiguous()
-        checkpoint_lvl = ctx.checkpoint_lvl
-        x, weight1, bias1, weight2, *rest = ctx.saved_tensors
-        batch_shape, n = x.shape[:-1], x.shape[-1]
-        batch_dim = batch_shape.numel()
-        is_bf16 = x.dtype == torch.bfloat16
-        if checkpoint_lvl == 0:
-            act_input, output1 = rest
-        elif checkpoint_lvl == 1:
-            (act_input,) = rest
-            output1 = sqrelu_fwd(act_input)
-        elif checkpoint_lvl == 2:
-            if is_bf16:
-                act_input = fused_dense_cuda.linear_bias_forward(
-                    x.reshape(batch_dim, n), weight1, bias1
-                )
-                output1 = sqrelu_fwd(act_input)
-            else:
-                output1, act_input = triton_linear_act(
-                    x.reshape(batch_dim, n),
-                    weight1,
-                    bias1,
-                    activation="squared_relu",
-                    save_act_input=True,
-                )
-
-        if is_bf16:
-            grad_output = grad_output.reshape(batch_dim, grad_output.shape[-1])
-            grad_weight2, grad_bias2 = fused_dense_cuda.linear_bias_wgrad(output1, grad_output)
-            grad_output1 = grad_output @ weight2
-            grad_act_input = sqrelu_bwd(grad_output1, act_input)
-            grad_input, grad_weight1, grad_bias1 = fused_dense_cuda.linear_bias_backward(
-                x.reshape(batch_dim, n), weight1, grad_act_input
-            )
-        else:
-            grad_output = grad_output.reshape(batch_dim, grad_output.shape[-1])
-            grad_weight2, grad_bias2 = fused_dense_cuda.linear_bias_wgrad(output1, grad_output)
-            grad_act_input = triton_dgrad_act(
-                grad_output, weight2, activation="squared_relu", act_input=act_input
-            )
-            grad_input, grad_weight1, grad_bias1 = fused_dense_cuda.linear_bias_backward(
-                x.reshape(batch_dim, n), weight1, grad_act_input
-            )
-        return grad_input.reshape_as(x), grad_weight1, grad_bias1, grad_weight2, grad_bias2, None
-
-
-fused_dense_sqrelu_dense_function = FusedDenseSqreluDenseFunc.apply
-
-
-class FusedDenseSqreluDense(nn.Module):
-    def __init__(
-        self,
-        in_features,
-        hidden_features=None,
-        out_features=None,
-        bias1=True,
-        bias2=True,
-        checkpoint_lvl=0,
-        device=None,
-        dtype=None,
-    ):
-        """
-        checkpoint_lvl (increasing lvl means slower but more memory saving):
-            0: no recomputation in the bwd
-            1: recompute gelu_out in the bwd
-            2: recompute gelu_in and gelu_out in the bwd
-        """
-        assert checkpoint_lvl in [0, 1, 2]
-        factory_kwargs = {"device": device, "dtype": dtype}
-        super().__init__()
-        out_features = out_features or in_features
-        hidden_features = hidden_features or in_features * 4
-        assert bias1 == True, "DenseSqreluDense module without bias is currently not supported"
-        assert bias2 == True, "DenseSqreluDense module without bias is currently not supported"
-        self.checkpoint_lvl = checkpoint_lvl
-        self.fc1 = nn.Linear(in_features, hidden_features, bias=bias1, **factory_kwargs)
-        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias2, **factory_kwargs)
-
-    def forward(self, x):
-        assert x.is_cuda
-        return fused_dense_sqrelu_dense_function(
-            x, self.fc1.weight, self.fc1.bias, self.fc2.weight, self.fc2.bias, self.checkpoint_lvl
-        )
diff --git a/based/ops/triton/rotary.py b/based/ops/triton/rotary.py
deleted file mode 100644
index 8d2e09b..0000000
--- a/based/ops/triton/rotary.py
+++ /dev/null
@@ -1,240 +0,0 @@
-# Copyright (c) 2023, Tri Dao.
-
-from typing import Optional, Union
-
-import torch
-
-import triton
-import triton.language as tl
-
-
-# @triton.autotune(
-#     configs=[
-#         triton.Config({"BLOCK_M": 2}),
-#         triton.Config({"BLOCK_M": 4}),
-#         triton.Config({"BLOCK_M": 8}),
-#         triton.Config({"BLOCK_M": 16}),
-#     ],
-#     key=["CACHE_KEY_SEQLEN", "BLOCK_K", "INTERLEAVED"],
-# )
-@triton.jit
-def rotary_kernel(
-    OUT,  # Pointers to matrices
-    X,
-    COS,
-    SIN,
-    CU_SEQLENS,
-    SEQLEN_OFFSETS,  # this could be int or a pointer
-    # Matrix dimensions
-    seqlen,
-    nheads,
-    rotary_dim,
-    seqlen_ro,
-    CACHE_KEY_SEQLEN,
-    # strides
-    stride_out_batch,
-    stride_out_seqlen,
-    stride_out_nheads,
-    stride_out_headdim,
-    stride_x_batch,
-    stride_x_seqlen,
-    stride_x_nheads,
-    stride_x_headdim,
-    # Meta-parameters
-    BLOCK_K: tl.constexpr,
-    IS_SEQLEN_OFFSETS_TENSOR: tl.constexpr,
-    IS_VARLEN: tl.constexpr,
-    INTERLEAVED: tl.constexpr,
-    CONJUGATE: tl.constexpr,
-    BLOCK_M: tl.constexpr,
-):
-    pid_m = tl.program_id(axis=0)
-    pid_batch = tl.program_id(axis=1)
-    pid_head = tl.program_id(axis=2)
-    rotary_dim_half = rotary_dim // 2
-
-    if not IS_VARLEN:
-        X = X + pid_batch * stride_x_batch + pid_head * stride_x_nheads
-        OUT = OUT + pid_batch * stride_out_batch + pid_head * stride_out_nheads
-    else:
-        start_idx = tl.load(CU_SEQLENS + pid_batch)
-        seqlen = tl.load(CU_SEQLENS + pid_batch + 1) - start_idx
-        X = X + start_idx * stride_x_seqlen + pid_head * stride_x_nheads
-        OUT = OUT + start_idx * stride_out_seqlen + pid_head * stride_out_nheads
-
-    if pid_m * BLOCK_M >= seqlen:
-        return
-    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
-    if not IS_SEQLEN_OFFSETS_TENSOR:
-        rm_cs = rm + SEQLEN_OFFSETS
-    else:
-        rm_cs = rm + tl.load(SEQLEN_OFFSETS + pid_batch)
-    rk = tl.arange(0, BLOCK_K)
-    rk_half = tl.arange(0, BLOCK_K // 2)
-
-    if not INTERLEAVED:
-        # Load the 1st and 2nd halves of X, do calculation, then store to 1st and 2nd halves of OUT
-        X = X + (rm[:, None] * stride_x_seqlen + rk_half[None, :] * stride_x_headdim)
-        COS = COS + (rm_cs[:, None] * rotary_dim_half + rk_half[None, :])
-        SIN = SIN + (rm_cs[:, None] * rotary_dim_half + rk_half[None, :])
-        cos = tl.load(
-            COS, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[None, :] < rotary_dim_half), other=1.0
-        ).to(tl.float32)
-        sin = tl.load(
-            SIN, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[None, :] < rotary_dim_half), other=0.0
-        ).to(tl.float32)
-        x0 = tl.load(
-            X, mask=(rm[:, None] < seqlen) & (rk_half[None, :] < rotary_dim_half), other=0.0
-        ).to(tl.float32)
-        x1 = tl.load(
-            X + rotary_dim_half * stride_x_headdim,
-            mask=(rm[:, None] < seqlen) & (rk_half[None, :] < rotary_dim_half),
-            other=0.0,
-        ).to(tl.float32)
-        if CONJUGATE:
-            sin = -sin
-        o0 = x0 * cos - x1 * sin
-        o1 = x0 * sin + x1 * cos
-        # write back result
-        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk_half[None, :] * stride_out_headdim)
-        tl.store(OUT, o0, mask=(rm[:, None] < seqlen) & (rk_half[None, :] < rotary_dim_half))
-        tl.store(
-            OUT + rotary_dim_half * stride_out_headdim,
-            o1,
-            mask=(rm[:, None] < seqlen) & (rk_half[None, :] < rotary_dim_half),
-        )
-    else:
-        # We don't want to load X[0, 2, 4, ...] and X[1, 3, 5, ...] separately since both are slow.
-        # Instead, we load x0 = X[0, 1, 2, 3, ...] and x1 = X[1, 0, 3, 2, ...].
-        # Loading x0 will be fast but x1 will be slow.
-        # Then we load cos = COS[0, 0, 1, 1, ...] and sin = SIN[0, 0, 1, 1, ...].
-        # Then we do the calculation and use tl.where to pick put the right outputs for the even
-        # and for the odd indices.
-        rk_swap = rk + ((rk + 1) % 2) * 2 - 1  # 1, 0, 3, 2, 5, 4, ...
-        rk_repeat = tl.arange(0, BLOCK_K) // 2
-        X0 = X + (rm[:, None] * stride_x_seqlen + rk[None, :] * stride_x_headdim)
-        X1 = X + (rm[:, None] * stride_x_seqlen + rk_swap[None, :] * stride_x_headdim)
-        COS = COS + (rm_cs[:, None] * rotary_dim_half + rk_repeat[None, :])
-        SIN = SIN + (rm_cs[:, None] * rotary_dim_half + rk_repeat[None, :])
-        cos = tl.load(
-            COS,
-            mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[None, :] < rotary_dim_half),
-            other=1.0,
-        ).to(tl.float32)
-        sin = tl.load(
-            SIN,
-            mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[None, :] < rotary_dim_half),
-            other=0.0,
-        ).to(tl.float32)
-        x0 = tl.load(X0, mask=(rm[:, None] < seqlen) & (rk[None, :] < rotary_dim), other=0.0).to(
-            tl.float32
-        )
-        x1 = tl.load(
-            X1, mask=(rm[:, None] < seqlen) & (rk_swap[None, :] < rotary_dim), other=0.0
-        ).to(tl.float32)
-        if CONJUGATE:
-            sin = -sin
-        x0_cos = x0 * cos
-        x1_sin = x1 * sin
-        out = tl.where(rk[None, :] % 2 == 0, x0_cos - x1_sin, x0_cos + x1_sin)
-        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk[None, :] * stride_out_headdim)
-        tl.store(OUT, out, mask=(rm[:, None] < seqlen) & (rk[None, :] < rotary_dim))
-
-
-def apply_rotary(
-    x: torch.Tensor,
-    cos: torch.Tensor,
-    sin: torch.Tensor,
-    seqlen_offsets: Union[int, torch.Tensor] = 0,
-    cu_seqlens: Optional[torch.Tensor] = None,
-    max_seqlen: Optional[int] = None,
-    interleaved=False,
-    inplace=False,
-    conjugate=False,
-) -> torch.Tensor:
-    """
-    Arguments:
-        x: (batch, seqlen, nheads, headdim) if cu_seqlens is None
-            else (total_seqlen, nheads, headdim).
-        cos: (seqlen_ro, rotary_dim / 2)
-        sin: (seqlen_ro, rotary_dim / 2)
-        seqlen_offsets: integer or integer tensor of size (batch,)
-        cu_seqlens: (batch + 1,) or None
-        max_seqlen: int
-    Returns:
-        y: (batch, seqlen, nheads, headdim)
-    """
-    is_varlen = cu_seqlens is not None
-    if not is_varlen:
-        batch, seqlen, nheads, headdim = x.shape
-    else:
-        assert max_seqlen is not None, "If cu_seqlens is passed in, then max_seqlen must be passed"
-        total_seqlen, nheads, headdim = x.shape
-        batch_p_1 = cu_seqlens.shape[0]
-        batch = batch_p_1 - 1
-        seqlen = max_seqlen
-    seqlen_ro, rotary_dim = cos.shape
-    assert sin.shape == cos.shape
-    rotary_dim *= 2
-    assert rotary_dim <= headdim, "rotary_dim must be <= headdim"
-    assert headdim <= 256, "Only support headdim <= 256"
-    assert seqlen_ro >= seqlen, "seqlen_ro must be >= seqlen"
-
-    assert (
-        cos.dtype == sin.dtype
-    ), f"cos and sin must have the same dtype, got {cos.dtype} and {sin.dtype}"
-    assert (
-        x.dtype == cos.dtype
-    ), f"Input and cos/sin must have the same dtype, got {x.dtype} and {cos.dtype}"
-
-    cos, sin = cos.contiguous(), sin.contiguous()
-    if isinstance(seqlen_offsets, torch.Tensor):
-        assert seqlen_offsets.shape == (batch,)
-        assert seqlen_offsets.dtype in [torch.int32, torch.int64]
-        seqlen_offsets = seqlen_offsets.contiguous()
-    else:
-        assert seqlen_offsets + seqlen <= seqlen_ro
-
-    output = torch.empty_like(x) if not inplace else x
-    if rotary_dim < headdim and not inplace:
-        output[..., rotary_dim:].copy_(x[..., rotary_dim:])
-
-    BLOCK_K = (
-        32
-        if rotary_dim <= 32
-        else (64 if rotary_dim <= 64 else (128 if rotary_dim <= 128 else 256))
-    )
-    grid = lambda META: (triton.cdiv(seqlen, META["BLOCK_M"]), batch, nheads)  # noqa
-    BLOCK_M = 4 if interleaved else (8 if rotary_dim <= 64 else 4)
-
-    # Need this, otherwise Triton tries to launch from cuda:0 and we get
-    # ValueError: Pointer argument (at 0) cannot be accessed from Triton (cpu tensor?)
-    with torch.cuda.device(x.device.index):
-        rotary_kernel[grid](
-            output,  # data ptrs
-            x,
-            cos,
-            sin,
-            cu_seqlens,
-            seqlen_offsets,
-            seqlen,  # shapes
-            nheads,
-            rotary_dim,
-            seqlen_ro,
-            seqlen // 128,  # key for triton cache (limit number of compilations)
-            output.stride(0) if not is_varlen else 0,  # batch_strides if not varlen else 0
-            output.stride(-3),  # seqlen_stride or total_seqlen_stride
-            output.stride(-2),  # nheads_stride
-            output.stride(-1),  # headdim_stride
-            x.stride(0) if not is_varlen else 0,  # batch_strides if not varlen else 0
-            x.stride(-3),  # seqlen stride or total_seqlen_stride
-            x.stride(-2),  # nheads stride
-            x.stride(-1),  # headdim stride
-            BLOCK_K,
-            isinstance(seqlen_offsets, torch.Tensor),
-            is_varlen,
-            interleaved,
-            conjugate,
-            BLOCK_M,
-        )
-    return output
diff --git a/based/tests/__init__.py b/based/tests/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/based/tests/test_generation.py b/based/tests/test_generation.py
deleted file mode 100644
index f009174..0000000
--- a/based/tests/test_generation.py
+++ /dev/null
@@ -1,173 +0,0 @@
-import pytest
-import torch
-
-from based.models.gpt import GPTLMHeadModel, GPT2MixerConfig
-
-
-BASE_CONFIG = {
-    "n_embd": 256,
-    "special_initializer": True,
-    "n_head": 16,
-    "n_layer": 2,
-    "rms_norm": True,
-    "fused_mlp": False,
-    "attn_pdrop": 0,
-    "embd_pdrop": 0,
-    "n_positions": 2048,
-    "resid_pdrop": 0,
-    "mlp_fc1_bias": False,
-    "mlp_fc2_bias": False,
-    "fused_bias_fc": True,
-    "out_proj_bias": False,
-    "qkv_proj_bias": False,
-    "use_flash_attn": False,
-    "residual_in_fp32": True,
-    "activation_function": "geglu",    # flagging,
-    # rotary_emb_fraction: 1        # flagging -- 0.5 for the other model.
-    "fused_dropout_add_ln": True,
-    "max_position_embeddings": 2048,   # flagging -- not RoPE,
-    "pad_vocab_size_multiple": 8,
-    "reorder_and_upcast_attn": False,
-    "scale_attn_by_inverse_layer_idx": False,
-    "n_inner": 1408 * 2,
-    "mlp_type": "alt"                 
-}
-
-CONFIGS = {
-    "conv": GPT2MixerConfig(
-        **{
-            "mixer": {
-                "_target_": "based.models.mixers.convolution.ShortConvolution",
-                "kernel_size": 3,
-                "use_cuda": False
-            }, **BASE_CONFIG
-        }
-    ),
-    "base_conv": GPT2MixerConfig(
-        **{
-            "mixer": {
-                "_target_": "based.models.mixers.convolution.BaseConv",
-                "kernel_size": 3,
-                "l_max": 2048
-            }, **BASE_CONFIG
-        }
-    ),
-    "linear_attn": GPT2MixerConfig(
-        **{
-            "feature_dim": 16,
-            "mixer": {
-                "_target_": "based.models.mixers.linear_attention.LinearAttention",
-                "feature_map": {
-                    "_target_": "based.models.mixers.linear_attention.TaylorExp",
-                    "input_dim": 16,
-                },
-                "num_heads": 16
-            }, 
-            **BASE_CONFIG
-        }
-    ),
-    "sliding": GPT2MixerConfig(
-        **{
-            "mixer": {
-                "_target_": "based.models.mixers.slide_attention.SlidingAttention",  
-                "window_size": 128,
-                # "embed_dim": BASE_CONFIG["n_embd"],
-                "num_heads": 16,
-                "causal": True,
-                # "do_update": True
-            }, **BASE_CONFIG
-        }
-    ),
-    "mha": GPT2MixerConfig(
-        **{
-            "mixer": {
-                "_target_": "based.models.mixers.mha.MHA",  
-                "window_size": (64, -1),
-                # "embed_dim": BASE_CONFIG["n_embd"],
-                "num_heads": 16,
-                "causal": True,
-                "use_flash_attn": True
-                # "do_update": True
-            }, **BASE_CONFIG
-        }
-    ),
-}
-
-CONFIGS_TO_TEST = [
-    # "conv",
-    # "base_conv",
-    # "linear_attn",
-    # "sliding",
-    "mha"
-]
-
-# SE (02/26): borrowing these tolerances from Mamba's test_selective_state_update
-# https://github.com/state-spaces/mamba/blob/ce59daea3a090d011d6476c6e5b97f6d58ddad8b/tests/ops/triton/test_selective_state_update.py#L24C1-L26C32
-DTYPE_TO_ATOL = {
-    torch.float32: 1e-3,
-    torch.float16: 1e-2,
-    torch.bfloat16: 5e-2,
-}
-DTYPE_TO_RTOL = {
-    torch.float32: 3e-4,
-    torch.float16: 5e-4,
-    torch.bfloat16: 1e-2,
-}
-
-@pytest.mark.parametrize("config", CONFIGS_TO_TEST)
-@pytest.mark.parametrize("prefill_size", [1, 128])
-@pytest.mark.parametrize("cache_graph", [False])
-@pytest.mark.parametrize("naive_generation", [False])
-@pytest.mark.parametrize("dtype", [
-    torch.float32, 
-    torch.float16, torch.bfloat16])
-def test_generation(
-    config: GPT2MixerConfig, 
-    prefill_size: int, 
-    cache_graph: bool, 
-    naive_generation: bool,
-    dtype: torch.dtype,
-):
-    if config in ["mha", "sliding"] and dtype == torch.torch.float32:
-        # SE: MHA is not implemented for float32
-        return 
-
-    config = CONFIGS[config]
-    batch_size = 4
-    n_generated_tokens = 64
-    device = "cuda"
-
-
-    model = GPTLMHeadModel(config).to(device=device, dtype=dtype)
-    model.eval()
-    torch.manual_seed(0)
-    input_ids = torch.randint(1, 1000, (batch_size, prefill_size), dtype=torch.long, device=device)
-
-    fn = model.generate_naive if naive_generation else model.generate
-    out = fn(
-        input_ids=input_ids, 
-        max_length=prefill_size + n_generated_tokens, 
-        return_dict_in_generate=True, 
-        output_scores=True, 
-        eos_token_id=None,  # ensure this is None so that we test full output length
-        top_k=1, # enforces that we take the top token
-        cg=cache_graph 
-    )
-    print("done with generation")
-
-    # SE: need to clone because of "RuntimeError: Inference tensors cannot be saved for 
-    # backward. To work around you can make a clone to get a normal tensor and use it 
-    # in autograd.
-    scores = torch.stack(out.scores, dim=1)
-    out = out.sequences.clone()
-
-    # pick a tolerance based on dtype -- for bfloat16 and float16 we have to be more lenient
-    atol, rtol = DTYPE_TO_ATOL[dtype], DTYPE_TO_RTOL[dtype]
-
-    # get reference output by repeatedly using the parallel view of the model
-    # (e.g. with a transformer this is like generating without a kv cache)
-    for i in range(n_generated_tokens):
-        scores_ref = model(input_ids=out[:, :prefill_size + i]).logits
-        out_ref = scores_ref.argmax(dim=-1)
-        assert torch.allclose(scores_ref[:, -1], scores[:, i], atol=atol, rtol=rtol)
-        assert torch.allclose(out[:, prefill_size + i], out_ref[:, -1])
diff --git a/based/utils/__init__.py b/based/utils/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/based/utils/checkpoint.py b/based/utils/checkpoint.py
deleted file mode 100755
index 64e3db6..0000000
--- a/based/utils/checkpoint.py
+++ /dev/null
@@ -1,76 +0,0 @@
-import re
-from pathlib import Path
-
-import torch
-import math
-from einops import rearrange
-
-def load_checkpoint(path, device='cpu'):
-    path = Path(path).expanduser()
-    is_deepspeed = False
-    if path.is_dir():  # DeepSpeed checkpoint
-        is_deepspeed = True
-        latest_path = path / 'latest'
-        if latest_path.is_file():
-            with open(latest_path, 'r') as fd:
-                tag = fd.read().strip()
-        else:
-            raise ValueError(f"Unable to find 'latest' file at {latest_path}")
-        path /= f'{tag}/mp_rank_00_model_states.pt'
-    state_dict = torch.load(path, map_location=device)
-    if is_deepspeed:
-        state_dict = state_dict['module']
-
-        # Replace the names of some of the submodules
-        def key_mapping(key):
-            return re.sub(r'^module.model.', '', key)
-
-        state_dict = {key_mapping(k): v for k, v in state_dict.items()}
-    return state_dict
-
-
-def blockdiag_to_dense_mlp_bert(state_dict):
-    from src.ops.blockdiag_multiply import blockdiag_weight_to_dense_weight
-    names = {name for name in state_dict
-             if re.match('bert.encoder.layer.(\d+).(mlp.fc(1|2)|(intermediate|output).dense).weight',
-                         name)}
-    for name in names:
-        state_dict[name] = blockdiag_weight_to_dense_weight(state_dict[name])
-    return state_dict
-
-def interpolate_pos_embedding(state_dict, out_seqlen, pos_embedding_name='model.pos_encoder.pe', interleave=False):
-    orig_emb = state_dict['state_dict'][pos_embedding_name]
-    assert (out_seqlen % orig_emb.shape[1]) == 0, 'out_seqlen must be a multiple of the original sequence length'
-    reps = [1 for i in orig_emb.shape]
-    reps[1] = out_seqlen // orig_emb.shape[1]
-    
-    if interleave:
-        assert math.isqrt(orig_emb.shape[1]) ** 2 == orig_emb.shape[1], 'interleave only works for square lengths'
-        assert math.isqrt(out_seqlen) ** 2 == out_seqlen, 'interleave only works for square lengths'
-        assert math.isqrt(reps[1]) ** 2 == reps[1], 'out_seqlen / seqlen must be a perfect square'
-
-        emb_square = rearrange(orig_emb, 'b (h w) d -> b h w d', h = math.isqrt(orig_emb.shape[1]))
-        emb_square_expanded = emb_square.repeat_interleave(math.isqrt(reps[1]), axis=1).repeat_interleave(math.isqrt(reps[1]), axis=2)
-        new_emb = rearrange(emb_square_expanded, 'b h w d -> b (h w) d')
-        state_dict['state_dict'][pos_embedding_name] = new_emb
-    else:
-        state_dict['state_dict'][pos_embedding_name] = orig_emb.repeat(*reps)
-
-    ret = remove_model_prefix(state_dict)
-    # # HACK: this is a hack for block-sparse flash attention
-    ret = {
-        k: v
-        for k, v in ret.items()
-        if not k.endswith('inner_attn.layout')
-    }
-    return ret
-
-def remove_model_prefix(state_dict):
-    # HACK: this is a hack to get the model to load properly, get rid of 'model.' prefix
-    for key in list(state_dict['state_dict'].keys()):
-        if key.startswith('model.'):
-            new_key = key[len('model.'):]
-            state_dict['state_dict'][new_key] = state_dict['state_dict'].pop(key)
-
-    # HACK: something is wrong with the state dict being loaded...
-    return state_dict['state_dict']
diff --git a/based/utils/ddp_zero1.py b/based/utils/ddp_zero1.py
deleted file mode 100755
index 479725e..0000000
--- a/based/utils/ddp_zero1.py
+++ /dev/null
@@ -1,106 +0,0 @@
-# Meant to work with Pytorch's ZeroRedundancyOptimizer
-
-from typing import Any, Callable, Dict, List, Optional, Union
-from pathlib import Path
-
-import torch
-from torch.optim.optimizer import Optimizer
-from torch.distributed.optim import ZeroRedundancyOptimizer
-
-from pytorch_lightning.strategies.ddp import DDPStrategy
-from pytorch_lightning.core.optimizer import LightningOptimizer
-try:  # pytorch_lightning <= 1.7
-    from pytorch_lightning.utilities.types import _PATH
-except ImportError:  # pytorch_lightning >= 1.8
-    try:
-        from lightning_lite.utilities.types import _PATH
-    except ImportError:  # pytorch_lightning >= 1.9
-        from lightning_fabric.utilities.types import _PATH
-
-
-# Copied from Pytorch's ZeroRedundancyOptimizer's state_dict method, but we only get
-# the local state dict to avoid synchronization across GPUs.
-# https://github.com/pytorch/pytorch/blob/0c7ca2d97ba5980a2af7dcd6b8106dc915e591cd/torch/distributed/optim/zero_redundancy_optimizer.py#L1131
-def get_zero_optimizer_state_dict_local(optimizer, global_rank):
-    optimizer._check_overlap_initialized()
-
-    # Sync the exposed `param_groups` attributes to the local optimizer in
-    # case they have been updated
-    optimizer._sync_param_groups(optimizer.param_groups, optimizer.optim.param_groups)
-
-    local_state_dict = optimizer.optim.state_dict()
-    state_dict = super(ZeroRedundancyOptimizer, optimizer).state_dict()
-
-    # Update the global optimizer state with local state information,
-    # factoring in the translation from local to global indexing
-    rank = global_rank
-    # TODO: recursive copy to device
-    local_param_groups = local_state_dict["param_groups"]
-    global_param_groups = optimizer._partition_parameters()[rank]
-    assert len(local_param_groups) == len(global_param_groups), \
-        "Mismatch between number of local and global parameter groups"
-
-    for local_param_group, global_param_group in zip(local_param_groups, global_param_groups):
-        # `local_param_group` stores local indices, while
-        # `global_param_group` stores the tensors directly
-        local_param_indices = local_param_group["params"]
-        global_params = global_param_group["params"]
-
-        assert len(local_param_indices) == len(global_params), \
-            "Mismatch between number of local and global parameters in parameter group"
-        for local_param_index, global_param in zip(local_param_indices, global_params):
-            # Update the global parameter state, if any
-            if local_param_index in local_state_dict["state"]:
-                global_param_index = optimizer._param_to_index[global_param]
-                state_dict["state"][global_param_index] = local_state_dict["state"][local_param_index]
-
-    # Sort the parameters in the state
-    state_dict["state"] = dict(sorted(state_dict["state"].items()))
-    return state_dict
-
-
-class DDPStrategyZero1(DDPStrategy):
-    """To use ZeroRedundancyOptimizer, we need to shard the optimizer states when
-    saving/loading checkpoints.
-    """
-
-    strategy_name = "ddp_zero1"
-
-    def optimizer_state(self, optimizer: Optimizer) -> Optional[dict]:
-        if isinstance(optimizer, LightningOptimizer):
-            optimizer = optimizer._optimizer
-        if isinstance(optimizer, ZeroRedundancyOptimizer):
-            return get_zero_optimizer_state_dict_local(optimizer, self.global_rank)
-        else:
-            return optimizer.state_dict()
-
-    def save_checkpoint(
-        self, checkpoint: Dict[str, Any], filepath: _PATH, storage_options: Optional[Any] = None
-    ) -> None:
-        """Save model/training states as a checkpoint file through state-dump and file-write.
-        Args:
-            checkpoint: dict containing model and trainer state
-            filepath: write-target file's path
-            storage_options: parameter for how to save to storage, passed to ``CheckpointIO`` plugin
-        """
-        filepath = Path(filepath)
-        filepath.mkdir(parents=True, exist_ok=True)
-        local_optimizer_states = checkpoint.pop('optimizer_states')
-        if self.is_global_zero:
-            self.checkpoint_io.save_checkpoint(checkpoint, filepath / 'model_states.pt',
-                                               storage_options=storage_options)
-        self.checkpoint_io.save_checkpoint(local_optimizer_states,
-                                           filepath / f'{self.global_rank:03d}_optim_states.pt',
-                                           storage_options=storage_options)
-
-    def load_checkpoint(self, checkpoint_path: _PATH) -> Dict[str, Any]:
-        torch.cuda.empty_cache()
-        checkpoint_path = Path(checkpoint_path)
-        if checkpoint_path.is_file():
-            return super().load_checkpoint(self, str(checkpoint_path))
-        else:
-            assert checkpoint_path.is_dir()
-            global_states = self.checkpoint_io.load_checkpoint(checkpoint_path / 'model_states.pt')
-            local_optimizer_states = self.checkpoint_io.load_checkpoint(checkpoint_path / f'{self.global_rank:03d}_optim_states.pt')
-            global_states['optimizer_states'] = local_optimizer_states
-            return global_states
diff --git a/based/utils/ddp_zero2.py b/based/utils/ddp_zero2.py
deleted file mode 100755
index f600338..0000000
--- a/based/utils/ddp_zero2.py
+++ /dev/null
@@ -1,146 +0,0 @@
-# Meant to work with Apex's DistributeFusedAdam
-
-from typing import Any, Callable, Dict, List, Optional, Union
-from pathlib import Path
-import types
-
-import torch
-from torch.optim.optimizer import Optimizer
-from torch.optim import LBFGS
-
-from apex.contrib.optimizers.distributed_fused_adam import DistributedFusedAdam
-
-from pytorch_lightning.strategies.ddp import DDPStrategy
-from pytorch_lightning.plugins.precision import PrecisionPlugin, NativeMixedPrecisionPlugin
-from pytorch_lightning.core.optimizer import LightningOptimizer
-from pytorch_lightning.utilities.exceptions import MisconfigurationException
-try:  # pytorch_lightning <= 1.7
-    from pytorch_lightning.utilities.types import _PATH
-except ImportError:  # pytorch_lightning >= 1.8
-    try:
-        from lightning_lite.utilities.types import _PATH
-    except ImportError:  # pytorch_lightning >= 1.9
-        from lightning_fabric.utilities.types import _PATH
-
-
-class DistAdamNativeMixedPrecisionPlugin(NativeMixedPrecisionPlugin):
-
-    def optimizer_step(  # type: ignore[override]
-        self,
-        model: "pl.LightningModule",
-        optimizer,
-        optimizer_idx: int,
-        closure: Callable[[], Any],
-        **kwargs: Any,
-    ) -> Any:
-        if self.scaler is None:
-            # skip scaler logic, as bfloat16 does not require scaler
-            return NativeMixedPrecisionPlugin.optimizer_step(
-                self, optimizer, model=model, optimizer_idx=optimizer_idx, closure=closure, **kwargs
-            )
-        if isinstance(optimizer, LBFGS):
-            raise MisconfigurationException(
-                f"Native AMP and the LBFGS optimizer are not compatible (optimizer {optimizer_idx})."
-            )
-        closure_result = closure()
-        # HACK: we don't call self.scaler.unscale_ here. This is because DistributedFusedAdam
-        # optimizer internally takes the scale into account.
-        # If we call unscale_ here, it would be equivalent to unscaling the gradients twice.
-        # Not unscaling has the side-effect that the NormMonitor callback will report the
-        # gradient norm to be much larger than reality.
-        # # `unscale` after the closure is executed but before the `on_before_optimizer_step` hook.
-        # self.scaler.unscale_(optimizer)
-        # This will call gradient clipping
-        self._after_closure(model, optimizer, optimizer_idx)
-        skipped_backward = closure_result is None
-        # in manual optimization, the closure does not return a value
-        if not model.automatic_optimization or not skipped_backward:
-            # note: the scaler will skip the `optimizer.step` if nonfinite gradients are found
-            step_output = self.scaler.step(optimizer, **kwargs)
-            self.scaler.update()
-            return step_output
-        return closure_result
-
-    def clip_grad_by_norm(self, optimizer: DistributedFusedAdam, clip_val: Union[int, float]) -> None:
-        """Clip gradients by norm."""
-        # DistributedFusedAdam wants list, not generator
-        # Gradients have not be scaled, so we need to scale up the clip_val
-        if self.scaler is not None:
-            clip_val *= self.scaler.get_scale()
-        return optimizer.clip_grad_norm(clip_val)
-
-
-class DDPStrategyZero2(DDPStrategy):
-    """To use Apex's DistributedFusedAdam, we need to shard the optimizer states when
-    saving/loading checkpoints.
-    """
-
-    strategy_name = "ddp_zero2"
-
-    def __init__(
-        self,
-        *args,
-        precision_plugin: Optional[PrecisionPlugin] = DistAdamNativeMixedPrecisionPlugin,
-        # precision_plugin: Optional[PrecisionPlugin] = None,
-        **kwargs: Union[Any, Dict[str, Any]],
-    ) -> None:
-        super().__init__(
-            *args, precision_plugin=precision_plugin, **kwargs
-        )
-
-    @property
-    def precision_plugin(self) -> PrecisionPlugin:
-        return self._precision_plugin if self._precision_plugin is not None else PrecisionPlugin()
-
-    @precision_plugin.setter
-    def precision_plugin(self, precision_plugin: Optional[PrecisionPlugin]) -> None:
-        self._precision_plugin = precision_plugin
-        # https://stackoverflow.com/questions/972/adding-a-method-to-an-existing-object-instance
-        self._precision_plugin.optimizer_step = types.MethodType(
-            DistAdamNativeMixedPrecisionPlugin.optimizer_step, self._precision_plugin
-        )
-        self._precision_plugin.clip_grad_by_norm = types.MethodType(
-            DistAdamNativeMixedPrecisionPlugin.clip_grad_by_norm, self._precision_plugin
-        )
-
-    def optimizer_state(self, optimizer: Optimizer) -> Optional[dict]:
-        if isinstance(optimizer, LightningOptimizer):
-            optimizer = optimizer._optimizer
-        if isinstance(optimizer, DistributedFusedAdam):
-            return optimizer.state_dict(gather_on_root=False)
-        else:
-            return optimizer.state_dict()
-
-    def save_checkpoint(
-        self, checkpoint: Dict[str, Any], filepath: _PATH, storage_options: Optional[Any] = None
-    ) -> None:
-        """Save model/training states as a checkpoint file through state-dump and file-write.
-        Args:
-            checkpoint: dict containing model and trainer state
-            filepath: write-target file's path
-            storage_options: parameter for how to save to storage, passed to ``CheckpointIO`` plugin
-        """
-        filepath = Path(filepath)
-        filepath.mkdir(parents=True, exist_ok=True)
-        local_optimizer_states = checkpoint.pop('optimizer_states')
-        if self.is_global_zero:
-            self.checkpoint_io.save_checkpoint(checkpoint, filepath / 'model_states.pt',
-                                               storage_options=storage_options)
-        self.checkpoint_io.save_checkpoint(local_optimizer_states,
-                                           filepath / f'{self.global_rank:03d}_optim_states.pt',
-                                           storage_options=storage_options)
-
-    def load_checkpoint(self, checkpoint_path: _PATH) -> Dict[str, Any]:
-        torch.cuda.empty_cache()
-        checkpoint_path = Path(checkpoint_path)
-        if checkpoint_path.is_file():
-            return super().load_checkpoint(self, str(checkpoint_path))
-        else:
-            assert checkpoint_path.is_dir()
-            global_states = self.checkpoint_io.load_checkpoint(checkpoint_path / 'model_states.pt')
-            local_optimizer_states = self.checkpoint_io.load_checkpoint(
-                checkpoint_path / f'{self.global_rank:03d}_optim_states.pt',
-                map_location='cuda'
-            )
-            global_states['optimizer_states'] = local_optimizer_states
-            return global_states
diff --git a/based/utils/distributed.py b/based/utils/distributed.py
deleted file mode 100755
index 073b613..0000000
--- a/based/utils/distributed.py
+++ /dev/null
@@ -1,111 +0,0 @@
-# Copied from https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/LanguageModeling/Transformer-XL/pytorch/utils/distributed.py
-
-# Copyright (c) 2019-2020, NVIDIA CORPORATION. All rights reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#       http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-import os
-from contextlib import contextmanager
-
-import torch
-
-
-def init_distributed(cuda):
-    """
-    Initializes distributed backend.
-    :param cuda: (bool) if True initializes nccl backend, if False initializes
-        gloo backend
-    """
-    world_size = int(os.environ.get('WORLD_SIZE', 1))
-    distributed = (world_size > 1)
-    if distributed:
-        backend = 'nccl' if cuda else 'gloo'
-        torch.distributed.init_process_group(backend=backend,
-                                             init_method='env://')
-        assert torch.distributed.is_initialized()
-    return distributed
-
-
-def barrier():
-    """
-    Call torch.distributed.barrier() if distritubed is in use
-    """
-    if torch.distributed.is_available() and torch.distributed.is_initialized():
-        torch.distributed.barrier()
-
-
-def get_rank():
-    """
-    Gets distributed rank or returns zero if distributed is not initialized.
-    """
-    if torch.distributed.is_available() and torch.distributed.is_initialized():
-        rank = torch.distributed.get_rank()
-    else:
-        rank = 0
-    return rank
-
-
-def get_world_size():
-    """
-    Gets total number of distributed workers or returns one if distributed is
-    not initialized.
-    """
-    if torch.distributed.is_available() and torch.distributed.is_initialized():
-        world_size = torch.distributed.get_world_size()
-    else:
-        world_size = 1
-    return world_size
-
-
-def all_reduce_item(value, op='sum'):
-    """
-    All-reduces single scalar value if distributed is in use
-    """
-    if torch.distributed.is_available() and torch.distributed.is_initialized():
-        if op == 'sum' or op == 'mean':
-            dop = torch.distributed.ReduceOp.SUM
-        elif op == 'min':
-            dop = torch.distributed.ReduceOp.MIN
-        elif op == 'max':
-            dop = torch.distributed.ReduceOp.MAX
-        elif op == 'product':
-            dop = torch.distributed.ReduceOp.PRODUCT
-        else:
-            raise RuntimeError('Unsupported reduce op')
-
-        backend = torch.distributed.get_backend()
-        if backend == torch.distributed.Backend.NCCL:
-            device = torch.device('cuda')
-        elif backend == torch.distributed.Backend.GLOO:
-            device = torch.device('cpu')
-        else:
-            raise RuntimeError('Unsupported distributed backend')
-
-        tensor = torch.tensor(value, device=device)
-        torch.distributed.all_reduce(tensor, dop)
-        if op == 'mean':
-            tensor /= get_world_size()
-        ret = tensor.item()
-    else:
-        ret = value
-    return ret
-
-
-@contextmanager
-def sync_workers():
-    """
-    Yields distributed rank and synchronizes all workers on exit.
-    """
-    rank = get_rank()
-    yield rank
-    barrier()
diff --git a/based/utils/ema.py b/based/utils/ema.py
deleted file mode 100755
index 9fb3beb..0000000
--- a/based/utils/ema.py
+++ /dev/null
@@ -1,280 +0,0 @@
-# Copied from https://github.com/fadel/pytorch_ema/blob/master/torch_ema/ema.py
-from __future__ import division
-from __future__ import unicode_literals
-
-from typing import Iterable, Optional
-import weakref
-import copy
-import contextlib
-
-import torch
-
-
-def to_float_maybe(x):
-    return x.float() if x.dtype in [torch.float16, torch.bfloat16] else x
-
-
-# Partially based on:
-# https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/python/training/moving_averages.py
-class ExponentialMovingAverage:
-    """
-    Maintains (exponential) moving average of a set of parameters.
-    Args:
-        parameters: Iterable of `torch.nn.Parameter` (typically from
-            `model.parameters()`).
-        decay: The exponential decay.
-        use_num_updates: Whether to use number of updates when computing
-            averages.
-    """
-    def __init__(
-        self,
-        parameters: Iterable[torch.nn.Parameter],
-        decay: float,
-        use_num_updates: bool = True
-    ):
-        if decay < 0.0 or decay > 1.0:
-            raise ValueError('Decay must be between 0 and 1')
-        self.decay = decay
-        self.num_updates = 0 if use_num_updates else None
-        parameters = list(parameters)
-        self.shadow_params = [to_float_maybe(p.clone().detach())
-                              for p in parameters if p.requires_grad]
-        self.collected_params = None
-        # By maintaining only a weakref to each parameter,
-        # we maintain the old GC behaviour of ExponentialMovingAverage:
-        # if the model goes out of scope but the ExponentialMovingAverage
-        # is kept, no references to the model or its parameters will be
-        # maintained, and the model will be cleaned up.
-        self._params_refs = [weakref.ref(p) for p in parameters]
-
-    def _get_parameters(
-        self,
-        parameters: Optional[Iterable[torch.nn.Parameter]]
-    ) -> Iterable[torch.nn.Parameter]:
-        if parameters is None:
-            parameters = [p() for p in self._params_refs]
-            if any(p is None for p in parameters):
-                raise ValueError(
-                    "(One of) the parameters with which this "
-                    "ExponentialMovingAverage "
-                    "was initialized no longer exists (was garbage collected);"
-                    " please either provide `parameters` explicitly or keep "
-                    "the model to which they belong from being garbage "
-                    "collected."
-                )
-            return parameters
-        else:
-            parameters = list(parameters)
-            if len(parameters) != len(self.shadow_params):
-                raise ValueError(
-                    "Number of parameters passed as argument is different "
-                    "from number of shadow parameters maintained by this "
-                    "ExponentialMovingAverage"
-                )
-            return parameters
-
-    def update(
-        self,
-        parameters: Optional[Iterable[torch.nn.Parameter]] = None
-    ) -> None:
-        """
-        Update currently maintained parameters.
-        Call this every time the parameters are updated, such as the result of
-        the `optimizer.step()` call.
-        Args:
-            parameters: Iterable of `torch.nn.Parameter`; usually the same set of
-                parameters used to initialize this object. If `None`, the
-                parameters with which this `ExponentialMovingAverage` was
-                initialized will be used.
-        """
-        parameters = self._get_parameters(parameters)
-        decay = self.decay
-        if self.num_updates is not None:
-            self.num_updates += 1
-            decay = min(
-                decay,
-                (1 + self.num_updates) / (10 + self.num_updates)
-            )
-        one_minus_decay = 1.0 - decay
-        if parameters[0].device != self.shadow_params[0].device:
-            self.to(device=parameters[0].device)
-        with torch.no_grad():
-            parameters = [p for p in parameters if p.requires_grad]
-            for s_param, param in zip(self.shadow_params, parameters):
-                torch.lerp(s_param, param.to(dtype=s_param.dtype), one_minus_decay, out=s_param)
-
-    def copy_to(
-        self,
-        parameters: Optional[Iterable[torch.nn.Parameter]] = None
-    ) -> None:
-        """
-        Copy current averaged parameters into given collection of parameters.
-        Args:
-            parameters: Iterable of `torch.nn.Parameter`; the parameters to be
-                updated with the stored moving averages. If `None`, the
-                parameters with which this `ExponentialMovingAverage` was
-                initialized will be used.
-        """
-        parameters = self._get_parameters(parameters)
-        for s_param, param in zip(self.shadow_params, parameters):
-            if param.requires_grad:
-                param.data.copy_(s_param.data)
-
-    def store(
-        self,
-        parameters: Optional[Iterable[torch.nn.Parameter]] = None
-    ) -> None:
-        """
-        Save the current parameters for restoring later.
-        Args:
-            parameters: Iterable of `torch.nn.Parameter`; the parameters to be
-                temporarily stored. If `None`, the parameters of with which this
-                `ExponentialMovingAverage` was initialized will be used.
-        """
-        parameters = self._get_parameters(parameters)
-        self.collected_params = [
-            param.clone()
-            for param in parameters
-            if param.requires_grad
-        ]
-
-    def restore(
-        self,
-        parameters: Optional[Iterable[torch.nn.Parameter]] = None
-    ) -> None:
-        """
-        Restore the parameters stored with the `store` method.
-        Useful to validate the model with EMA parameters without affecting the
-        original optimization process. Store the parameters before the
-        `copy_to` method. After validation (or model saving), use this to
-        restore the former parameters.
-        Args:
-            parameters: Iterable of `torch.nn.Parameter`; the parameters to be
-                updated with the stored parameters. If `None`, the
-                parameters with which this `ExponentialMovingAverage` was
-                initialized will be used.
-        """
-        if self.collected_params is None:
-            raise RuntimeError(
-                "This ExponentialMovingAverage has no `store()`ed weights "
-                "to `restore()`"
-            )
-        parameters = self._get_parameters(parameters)
-        for c_param, param in zip(self.collected_params, parameters):
-            if param.requires_grad:
-                param.data.copy_(c_param.data)
-
-    @contextlib.contextmanager
-    def average_parameters(
-        self,
-        parameters: Optional[Iterable[torch.nn.Parameter]] = None
-    ):
-        r"""
-        Context manager for validation/inference with averaged parameters.
-        Equivalent to:
-            ema.store()
-            ema.copy_to()
-            try:
-                ...
-            finally:
-                ema.restore()
-        Args:
-            parameters: Iterable of `torch.nn.Parameter`; the parameters to be
-                updated with the stored parameters. If `None`, the
-                parameters with which this `ExponentialMovingAverage` was
-                initialized will be used.
-        """
-        parameters = self._get_parameters(parameters)
-        self.store(parameters)
-        self.copy_to(parameters)
-        try:
-            yield
-        finally:
-            self.restore(parameters)
-
-    def to(self, device=None, dtype=None) -> None:
-        r"""Move internal buffers of the ExponentialMovingAverage to `device`.
-        Args:
-            device: like `device` argument to `torch.Tensor.to`
-        """
-        # .to() on the tensors handles None correctly
-        self.shadow_params = [
-            p.to(device=device, dtype=dtype)
-            if p.is_floating_point()
-            else p.to(device=device)
-            for p in self.shadow_params
-        ]
-        if self.collected_params is not None:
-            self.collected_params = [
-                p.to(device=device, dtype=dtype)
-                if p.is_floating_point()
-                else p.to(device=device)
-                for p in self.collected_params
-            ]
-        return
-
-    def state_dict(self) -> dict:
-        r"""Returns the state of the ExponentialMovingAverage as a dict."""
-        # Following PyTorch conventions, references to tensors are returned:
-        # "returns a reference to the state and not its copy!" -
-        # https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict
-        return {
-            "decay": self.decay,
-            "num_updates": self.num_updates,
-            "shadow_params": self.shadow_params,
-            "collected_params": self.collected_params
-        }
-
-    def load_state_dict(self, state_dict: dict) -> None:
-        r"""Loads the ExponentialMovingAverage state.
-        Args:
-            state_dict (dict): EMA state. Should be an object returned
-                from a call to :meth:`state_dict`.
-        """
-        # deepcopy, to be consistent with module API
-        state_dict = copy.deepcopy(state_dict)
-        self.decay = state_dict["decay"]
-        if self.decay < 0.0 or self.decay > 1.0:
-            raise ValueError('Decay must be between 0 and 1')
-        self.num_updates = state_dict["num_updates"]
-        assert self.num_updates is None or isinstance(self.num_updates, int), \
-            "Invalid num_updates"
-
-        self.shadow_params = state_dict["shadow_params"]
-        assert isinstance(self.shadow_params, list), \
-            "shadow_params must be a list"
-        assert all(
-            isinstance(p, torch.Tensor) for p in self.shadow_params
-        ), "shadow_params must all be Tensors"
-
-        self.collected_params = state_dict["collected_params"]
-        if self.collected_params is not None:
-            assert isinstance(self.collected_params, list), \
-                "collected_params must be a list"
-            assert all(
-                isinstance(p, torch.Tensor) for p in self.collected_params
-            ), "collected_params must all be Tensors"
-            assert len(self.collected_params) == len(self.shadow_params), \
-                "collected_params and shadow_params had different lengths"
-
-        if len(self.shadow_params) == len(self._params_refs):
-            # Consistent with torch.optim.Optimizer, cast things to consistent
-            # device and dtype with the parameters
-            params = [p() for p in self._params_refs]
-            # If parameters have been garbage collected, just load the state
-            # we were given without change.
-            if not any(p is None for p in params):
-                # ^ parameter references are still good
-                for i, p in enumerate(params):
-                    self.shadow_params[i] = to_float_maybe(self.shadow_params[i].to(
-                        device=p.device, dtype=p.dtype
-                    ))
-                    if self.collected_params is not None:
-                        self.collected_params[i] = self.collected_params[i].to(
-                            device=p.device, dtype=p.dtype
-                        )
-        else:
-            raise ValueError(
-                "Tried to `load_state_dict()` with the wrong number of "
-                "parameters in the saved state."
-            )
diff --git a/based/utils/flops.py b/based/utils/flops.py
deleted file mode 100755
index bb1ca79..0000000
--- a/based/utils/flops.py
+++ /dev/null
@@ -1,45 +0,0 @@
-# Adapted from https://github.com/rwightman/pytorch-image-models/blob/master/benchmark.py
-import torch
-
-try:
-    from deepspeed.profiling.flops_profiler import get_model_profile
-    has_deepspeed_profiling = True
-except ImportError as e:
-    has_deepspeed_profiling = False
-
-try:
-    from fvcore.nn import FlopCountAnalysis, flop_count_str, flop_count_table
-    from fvcore.nn import ActivationCountAnalysis
-    has_fvcore_profiling = True
-except ImportError as e:
-    FlopCountAnalysis = None
-    ActivationCountAnalysis = None
-    has_fvcore_profiling = False
-
-
-def profile_deepspeed(model, input_size=(3, 224, 224), input_dtype=torch.float32,
-                      batch_size=1, detailed=False):
-    device, dtype = next(model.parameters()).device, next(model.parameters()).dtype
-    flops, macs, params = get_model_profile(
-        model=model,
-        args=torch.zeros((batch_size,) + input_size, device=device, dtype=input_dtype),
-        print_profile=detailed,  # prints the model graph with the measured profile attached to each module
-        detailed=detailed,  # print the detailed profile
-        warm_up=10,  # the number of warm-ups before measuring the time of each module
-        as_string=False,  # print raw numbers (e.g. 1000) or as human-readable strings (e.g. 1k)
-        output_file=None,  # path to the output file. If None, the profiler prints to stdout.
-        ignore_modules=None)  # the list of modules to ignore in the profiling
-    return macs, 0  # no activation count in DS
-
-
-def profile_fvcore(model, input_size=(3, 224, 224), input_dtype=torch.float32, max_depth=4,
-                   batch_size=1, detailed=False, force_cpu=False):
-    if force_cpu:
-        model = model.to('cpu')
-    device, dtype = next(model.parameters()).device, next(model.parameters()).dtype
-    example_input = torch.zeros((batch_size,) + input_size, device=device, dtype=input_dtype)
-    fca = FlopCountAnalysis(model, example_input)
-    aca = ActivationCountAnalysis(model, example_input)
-    if detailed:
-        print(flop_count_table(fca, max_depth=max_depth))
-    return fca, fca.total(), aca, aca.total()
diff --git a/based/utils/gpu_affinity.py b/based/utils/gpu_affinity.py
deleted file mode 100755
index 8636d50..0000000
--- a/based/utils/gpu_affinity.py
+++ /dev/null
@@ -1,142 +0,0 @@
-import collections
-import math
-import os
-import pathlib
-import re
-
-import pynvml
-
-pynvml.nvmlInit()
-
-
-def systemGetDriverVersion():
-    return pynvml.nvmlSystemGetDriverVersion()
-
-
-def deviceGetCount():
-    return pynvml.nvmlDeviceGetCount()
-
-
-class device:
-    # assume nvml returns list of 64 bit ints
-    _nvml_affinity_elements = math.ceil(os.cpu_count() / 64)
-
-    def __init__(self, device_idx):
-        super().__init__()
-        self.handle = pynvml.nvmlDeviceGetHandleByIndex(device_idx)
-
-    def getName(self):
-        return pynvml.nvmlDeviceGetName(self.handle)
-
-    def getCpuAffinity(self):
-        affinity_string = ''
-        for j in pynvml.nvmlDeviceGetCpuAffinity(
-            self.handle, device._nvml_affinity_elements
-        ):
-            # assume nvml returns list of 64 bit ints
-            affinity_string = '{:064b}'.format(j) + affinity_string
-        affinity_list = [int(x) for x in affinity_string]
-        affinity_list.reverse()  # so core 0 is in 0th element of list
-
-        ret = [i for i, e in enumerate(affinity_list) if e != 0]
-        return ret
-
-
-def set_socket_affinity(gpu_id):
-    dev = device(gpu_id)
-    affinity = dev.getCpuAffinity()
-    os.sched_setaffinity(0, affinity)
-
-
-def set_single_affinity(gpu_id):
-    dev = device(gpu_id)
-    affinity = dev.getCpuAffinity()
-    os.sched_setaffinity(0, affinity[:1])
-
-
-def set_single_unique_affinity(gpu_id, nproc_per_node):
-    devices = [device(i) for i in range(nproc_per_node)]
-    socket_affinities = [dev.getCpuAffinity() for dev in devices]
-
-    siblings_list = get_thread_siblings_list()
-    siblings_dict = dict(siblings_list)
-
-    # remove siblings
-    for idx, socket_affinity in enumerate(socket_affinities):
-        socket_affinities[idx] = list(set(socket_affinity) - set(siblings_dict.values()))
-
-    affinities = []
-    assigned = []
-
-    for socket_affinity in socket_affinities:
-        for core in socket_affinity:
-            if core not in assigned:
-                affinities.append([core])
-                assigned.append(core)
-                break
-    os.sched_setaffinity(0, affinities[gpu_id])
-
-
-def set_socket_unique_affinity(gpu_id, nproc_per_node, mode):
-    device_ids = [device(i) for i in range(nproc_per_node)]
-    socket_affinities = [dev.getCpuAffinity() for dev in device_ids]
-
-    siblings_list = get_thread_siblings_list()
-    siblings_dict = dict(siblings_list)
-
-    # remove siblings
-    for idx, socket_affinity in enumerate(socket_affinities):
-        socket_affinities[idx] = list(set(socket_affinity) - set(siblings_dict.values()))
-
-    socket_affinities_to_device_ids = collections.defaultdict(list)
-
-    for idx, socket_affinity in enumerate(socket_affinities):
-        socket_affinities_to_device_ids[tuple(socket_affinity)].append(idx)
-
-    for socket_affinity, device_ids in socket_affinities_to_device_ids.items():
-        devices_per_group = len(device_ids)
-        cores_per_device = len(socket_affinity) // devices_per_group
-        for group_id, device_id in enumerate(device_ids):
-            if device_id == gpu_id:
-                if mode == 'interleaved':
-                    affinity = list(socket_affinity[group_id::devices_per_group])
-                elif mode == 'continuous':
-                    affinity = list(socket_affinity[group_id*cores_per_device:(group_id+1)*cores_per_device])
-                else:
-                    raise RuntimeError('Unknown set_socket_unique_affinity mode')
-
-                # reintroduce siblings
-                affinity += [siblings_dict[aff] for aff in affinity if aff in siblings_dict]
-                os.sched_setaffinity(0, affinity)
-
-
-def get_thread_siblings_list():
-    path = '/sys/devices/system/cpu/cpu*/topology/thread_siblings_list'
-    thread_siblings_list = []
-    pattern = re.compile(r'(\d+)\D(\d+)')
-    for fname in pathlib.Path(path[0]).glob(path[1:]):
-        with open(fname) as f:
-            content = f.read().strip()
-            res = pattern.findall(content)
-            if res:
-                pair = tuple(map(int, res[0]))
-                thread_siblings_list.append(pair)
-    return thread_siblings_list
-
-
-def set_affinity(gpu_id, nproc_per_node, mode='socket'):
-    if mode == 'socket':
-        set_socket_affinity(gpu_id)
-    elif mode == 'single':
-        set_single_affinity(gpu_id)
-    elif mode == 'single_unique':
-        set_single_unique_affinity(gpu_id, nproc_per_node)
-    elif mode == 'socket_unique_interleaved':
-        set_socket_unique_affinity(gpu_id, nproc_per_node, 'interleaved')
-    elif mode == 'socket_unique_continuous':
-        set_socket_unique_affinity(gpu_id, nproc_per_node, 'continuous')
-    else:
-        raise RuntimeError('Unknown affinity mode')
-
-    affinity = os.sched_getaffinity(0)
-    return affinity
diff --git a/based/utils/hf.py b/based/utils/hf.py
deleted file mode 100644
index 406d3b0..0000000
--- a/based/utils/hf.py
+++ /dev/null
@@ -1,18 +0,0 @@
-""" Source: https://github.com/state-spaces/mamba/blob/main/mamba_ssm/utils/hf.py """
-
-import json
-import torch
-from transformers.utils import WEIGHTS_NAME, CONFIG_NAME
-from transformers.utils.hub import cached_file
-
-
-def load_config_hf(model_name):
-    resolved_archive_file = cached_file(model_name, CONFIG_NAME, _raise_exceptions_for_missing_entries=False)
-    return json.load(open(resolved_archive_file))
-
-def load_state_dict_hf(model_name, device=None, dtype=None):
-    # If not fp32, then we don't want to load directly to the GPU
-    mapped_device = "cpu" if dtype not in [torch.float32, None] else device
-    resolved_archive_file = cached_file(model_name, WEIGHTS_NAME, _raise_exceptions_for_missing_entries=False)
-    return torch.load(resolved_archive_file, map_location=mapped_device)
-
diff --git a/based/utils/utils.py b/based/utils/utils.py
deleted file mode 100755
index 7532459..0000000
--- a/based/utils/utils.py
+++ /dev/null
@@ -1,148 +0,0 @@
-import logging
-import warnings
-from typing import List, Sequence
-
-import pytorch_lightning as pl
-import rich.syntax
-import rich.tree
-from omegaconf import DictConfig, OmegaConf
-from pytorch_lightning.utilities import rank_zero_only
-
-
-# Copied from https://docs.python.org/3/howto/logging-cookbook.html#using-a-context-manager-for-selective-logging
-class LoggingContext:
-    def __init__(self, logger, level=None, handler=None, close=True):
-        self.logger = logger
-        self.level = level
-        self.handler = handler
-        self.close = close
-
-    def __enter__(self):
-        if self.level is not None:
-            self.old_level = self.logger.level
-            self.logger.setLevel(self.level)
-        if self.handler:
-            self.logger.addHandler(self.handler)
-
-    def __exit__(self, et, ev, tb):
-        if self.level is not None:
-            self.logger.setLevel(self.old_level)
-        if self.handler:
-            self.logger.removeHandler(self.handler)
-        if self.handler and self.close:
-            self.handler.close()
-        # implicit return of None => don't swallow exceptions
-
-
-def get_logger(name=__name__) -> logging.Logger:
-    """Initializes multi-GPU-friendly python logger."""
-
-    logger = logging.getLogger(name)
-
-    # this ensures all logging levels get marked with the rank zero decorator
-    # otherwise logs would get multiplied for each GPU process in multi-GPU setup
-    for level in ("debug", "info", "warning", "error", "exception", "fatal", "critical"):
-        setattr(logger, level, rank_zero_only(getattr(logger, level)))
-
-    return logger
-
-
-def extras(config: DictConfig) -> None:
-    """A couple of optional utilities, controlled by main config file:
-    - disabling warnings
-    - forcing debug friendly configuration
-    - verifying experiment name is set when running in experiment mode
-    Modifies DictConfig in place.
-    Args:
-        config (DictConfig): Configuration composed by Hydra.
-    """
-
-    log = get_logger(__name__)
-
-    # disable python warnings if <config.ignore_warnings=True>
-    if config.get("ignore_warnings"):
-        log.info("Disabling python warnings! <config.ignore_warnings=True>")
-        warnings.filterwarnings("ignore")
-
-    # verify experiment name is set when running in experiment mode
-    if config.get("experiment_mode") and not config.get("name"):
-        log.info(
-            "Running in experiment mode without the experiment name specified! "
-            "Use `python run.py mode=exp name=experiment_name`"
-        )
-        log.info("Exiting...")
-        exit()
-
-    # force debugger friendly configuration if <config.trainer.fast_dev_run=True>
-    # debuggers don't like GPUs and multiprocessing
-    if config.trainer.get("fast_dev_run"):
-        log.info("Forcing debugger friendly configuration! <config.trainer.fast_dev_run=True>")
-        if config.trainer.get("gpus"):
-            config.trainer.gpus = 0
-        if config.datamodule.get("pin_memory"):
-            config.datamodule.pin_memory = False
-        if config.datamodule.get("num_workers"):
-            config.datamodule.num_workers = 0
-
-
-@rank_zero_only
-def print_config(
-    config: DictConfig,
-    fields: Sequence[str] = (
-        "trainer",
-        "model",
-        "datamodule",
-        "train",
-        "eval",
-        "callbacks",
-        "logger",
-        "seed",
-        "name",
-    ),
-    resolve: bool = True,
-) -> None:
-    """Prints content of DictConfig using Rich library and its tree structure.
-    Args:
-        config (DictConfig): Configuration composed by Hydra.
-        fields (Sequence[str], optional): Determines which main fields from config will
-        be printed and in what order.
-        resolve (bool, optional): Whether to resolve reference fields of DictConfig.
-    """
-
-    style = "dim"
-    tree = rich.tree.Tree("CONFIG", style=style, guide_style=style)
-
-    for field in fields:
-        branch = tree.add(field, style=style, guide_style=style)
-
-        config_section = config.get(field)
-        branch_content = str(config_section)
-        if isinstance(config_section, DictConfig):
-            branch_content = OmegaConf.to_yaml(config_section, resolve=resolve)
-
-        branch.add(rich.syntax.Syntax(branch_content, "yaml"))
-
-    rich.print(tree)
-
-    with open("config_tree.txt", "w") as fp:
-        rich.print(tree, file=fp)
-
-@rank_zero_only
-def print_rank_zero(*args, **kwargs) -> None:
-    return print(*args, **kwargs)
-
-def finish(
-    config: DictConfig,
-    model: pl.LightningModule,
-    datamodule: pl.LightningDataModule,
-    trainer: pl.Trainer,
-    callbacks: List[pl.Callback],
-    logger: List[pl.loggers.LightningLoggerBase],
-) -> None:
-    """Makes sure everything closed properly."""
-
-    # without this sweeps with wandb logger might crash!
-    for lg in logger:
-        if isinstance(lg, pl.loggers.wandb.WandbLogger):
-            import wandb
-            wandb.finish()
Submodule evaluate ffc7b5c...0000000 (submodule deleted)
diff --git a/notebooks/03-24-quick-start.ipynb b/notebooks/03-24-quick-start.ipynb
deleted file mode 100644
index 27453ef..0000000
--- a/notebooks/03-24-quick-start.ipynb
+++ /dev/null
@@ -1,599 +0,0 @@
-{
- "cells": [
-  {
-   "cell_type": "code",
-   "execution_count": 1,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "%load_ext autoreload\n",
-    "%autoreload 2"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 2,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "# Step 1: Run the setup.py in the based/ directory to install the package"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 3,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "# Step 2: Set to download to a path with sufficient space\n",
-    "! export TRANSFORMERS_CACHE=/var/cr05_data/sim_data"
-   ]
-  },
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "### Loading models"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 4,
-   "metadata": {},
-   "outputs": [
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "/var/cr05_data/sim_data/miniconda3/envs/based/lib/python3.8/site-packages/tqdm-4.66.1-py3.8.egg/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
-      "  from .autonotebook import tqdm as notebook_tqdm\n"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "No module named 'causal_attention_cuda'\n",
-      "No module named 'causal_attention_cuda'\n",
-      "Successfully imported the causal dot product kernel! \n",
-      "Successfully imported the FLA triton kernels! \n"
-     ]
-    }
-   ],
-   "source": [
-    "# Step 3: Download the Based model\n",
-    "\n",
-    "import torch\n",
-    "from transformers import AutoTokenizer\n",
-    "from based.models.gpt import GPTLMHeadModel\n",
-    "\n",
-    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
-    "model = GPTLMHeadModel.from_pretrained_hf(\"hazyresearch/based-360m\").to(\"cuda\")"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 5,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "# Optional: download the baselines\n",
-    "do_download = False\n",
-    "\n",
-    "if do_download:\n",
-    "    import torch\n",
-    "    from transformers import AutoTokenizer\n",
-    "    from based.models.mamba import MambaLMHeadModel\n",
-    "    from based.models.transformer.gpt import GPTLMHeadModel as AttentionGPTLMHeadModel\n",
-    "\n",
-    "    # Attention\n",
-    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
-    "    model = AttentionGPTLMHeadModel.from_pretrained_hf(\"hazyresearch/attn-360m\").to(\"cuda\")\n",
-    "\n",
-    "    # Mamba\n",
-    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
-    "    model = MambaLMHeadModel.from_pretrained_hf(\"hazyresearch/mamba-360m\").to(\"cuda\")"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 6,
-   "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "text/plain": [
-       "GPTLMHeadModel(\n",
-       "  (transformer): GPTModel(\n",
-       "    (embeddings): GPT2Embeddings(\n",
-       "      (word_embeddings): Embedding(50264, 1024)\n",
-       "    )\n",
-       "    (layers): ModuleList(\n",
-       "      (0): Block(\n",
-       "        (mixer): BaseConv(\n",
-       "          (in_proj): Linear(in_features=1024, out_features=4096, bias=True)\n",
-       "          (out_proj): Linear(in_features=2048, out_features=1024, bias=True)\n",
-       "          (conv): ShortConvolution(\n",
-       "            (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n",
-       "          )\n",
-       "        )\n",
-       "        (dropout1): Dropout(p=0, inplace=False)\n",
-       "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
-       "        (norm1): RMSNorm()\n",
-       "        (mlp): GatedMlp(\n",
-       "          (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
-       "          (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
-       "        )\n",
-       "        (dropout2): Dropout(p=0, inplace=False)\n",
-       "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
-       "        (norm2): RMSNorm()\n",
-       "      )\n",
-       "      (1): Block(\n",
-       "        (mixer): LinearAttention(\n",
-       "          (feature_map): TaylorExp()\n",
-       "          (proj_q): Linear(in_features=1024, out_features=256, bias=False)\n",
-       "          (proj_k): Linear(in_features=1024, out_features=256, bias=False)\n",
-       "          (proj_v): Linear(in_features=1024, out_features=1024, bias=False)\n",
-       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
-       "        )\n",
-       "        (dropout1): Dropout(p=0, inplace=False)\n",
-       "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
-       "        (norm1): RMSNorm()\n",
-       "        (mlp): GatedMlp(\n",
-       "          (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
-       "          (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
-       "        )\n",
-       "        (dropout2): Dropout(p=0, inplace=False)\n",
-       "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
-       "        (norm2): RMSNorm()\n",
-       "      )\n",
-       "      (2): Block(\n",
-       "        (mixer): SlidingAttention(\n",
-       "          (rotary_emb): RotaryEmbedding()\n",
-       "          (Wqkv): FusedDense(in_features=1024, out_features=3072, bias=False)\n",
-       "          (inner_attn): FlashSelfAttention(\n",
-       "            (drop): Dropout(p=0, inplace=False)\n",
-       "          )\n",
-       "          (inner_cross_attn): FlashCrossAttention(\n",
-       "            (drop): Dropout(p=0, inplace=False)\n",
-       "          )\n",
-       "          (out_proj): FusedDense(in_features=1024, out_features=1024, bias=False)\n",
-       "        )\n",
-       "        (dropout1): Dropout(p=0, inplace=False)\n",
-       "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
-       "        (norm1): RMSNorm()\n",
-       "        (mlp): GatedMlp(\n",
-       "          (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
-       "          (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
-       "        )\n",
-       "        (dropout2): Dropout(p=0, inplace=False)\n",
-       "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
-       "        (norm2): RMSNorm()\n",
-       "      )\n",
-       "      (3-5): 3 x Block(\n",
-       "        (mixer): BaseConv(\n",
-       "          (in_proj): Linear(in_features=1024, out_features=4096, bias=True)\n",
-       "          (out_proj): Linear(in_features=2048, out_features=1024, bias=True)\n",
-       "          (conv): ShortConvolution(\n",
-       "            (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n",
-       "          )\n",
-       "        )\n",
-       "        (dropout1): Dropout(p=0, inplace=False)\n",
-       "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
-       "        (norm1): RMSNorm()\n",
-       "        (mlp): GatedMlp(\n",
-       "          (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
-       "          (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
-       "        )\n",
-       "        (dropout2): Dropout(p=0, inplace=False)\n",
-       "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
-       "        (norm2): RMSNorm()\n",
-       "      )\n",
-       "      (6): Block(\n",
-       "        (mixer): LinearAttention(\n",
-       "          (feature_map): TaylorExp()\n",
-       "          (proj_q): Linear(in_features=1024, out_features=256, bias=False)\n",
-       "          (proj_k): Linear(in_features=1024, out_features=256, bias=False)\n",
-       "          (proj_v): Linear(in_features=1024, out_features=1024, bias=False)\n",
-       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
-       "        )\n",
-       "        (dropout1): Dropout(p=0, inplace=False)\n",
-       "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
-       "        (norm1): RMSNorm()\n",
-       "        (mlp): GatedMlp(\n",
-       "          (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
-       "          (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
-       "        )\n",
-       "        (dropout2): Dropout(p=0, inplace=False)\n",
-       "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
-       "        (norm2): RMSNorm()\n",
-       "      )\n",
-       "      (7): Block(\n",
-       "        (mixer): SlidingAttention(\n",
-       "          (rotary_emb): RotaryEmbedding()\n",
-       "          (Wqkv): FusedDense(in_features=1024, out_features=3072, bias=False)\n",
-       "          (inner_attn): FlashSelfAttention(\n",
-       "            (drop): Dropout(p=0, inplace=False)\n",
-       "          )\n",
-       "          (inner_cross_attn): FlashCrossAttention(\n",
-       "            (drop): Dropout(p=0, inplace=False)\n",
-       "          )\n",
-       "          (out_proj): FusedDense(in_features=1024, out_features=1024, bias=False)\n",
-       "        )\n",
-       "        (dropout1): Dropout(p=0, inplace=False)\n",
-       "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
-       "        (norm1): RMSNorm()\n",
-       "        (mlp): GatedMlp(\n",
-       "          (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
-       "          (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
-       "        )\n",
-       "        (dropout2): Dropout(p=0, inplace=False)\n",
-       "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
-       "        (norm2): RMSNorm()\n",
-       "      )\n",
-       "      (8-10): 3 x Block(\n",
-       "        (mixer): BaseConv(\n",
-       "          (in_proj): Linear(in_features=1024, out_features=4096, bias=True)\n",
-       "          (out_proj): Linear(in_features=2048, out_features=1024, bias=True)\n",
-       "          (conv): ShortConvolution(\n",
-       "            (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n",
-       "          )\n",
-       "        )\n",
-       "        (dropout1): Dropout(p=0, inplace=False)\n",
-       "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
-       "        (norm1): RMSNorm()\n",
-       "        (mlp): GatedMlp(\n",
-       "          (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
-       "          (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
-       "        )\n",
-       "        (dropout2): Dropout(p=0, inplace=False)\n",
-       "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
-       "        (norm2): RMSNorm()\n",
-       "      )\n",
-       "      (11): Block(\n",
-       "        (mixer): LinearAttention(\n",
-       "          (feature_map): TaylorExp()\n",
-       "          (proj_q): Linear(in_features=1024, out_features=256, bias=False)\n",
-       "          (proj_k): Linear(in_features=1024, out_features=256, bias=False)\n",
-       "          (proj_v): Linear(in_features=1024, out_features=1024, bias=False)\n",
-       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
-       "        )\n",
-       "        (dropout1): Dropout(p=0, inplace=False)\n",
-       "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
-       "        (norm1): RMSNorm()\n",
-       "        (mlp): GatedMlp(\n",
-       "          (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
-       "          (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
-       "        )\n",
-       "        (dropout2): Dropout(p=0, inplace=False)\n",
-       "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
-       "        (norm2): RMSNorm()\n",
-       "      )\n",
-       "      (12): Block(\n",
-       "        (mixer): SlidingAttention(\n",
-       "          (rotary_emb): RotaryEmbedding()\n",
-       "          (Wqkv): FusedDense(in_features=1024, out_features=3072, bias=False)\n",
-       "          (inner_attn): FlashSelfAttention(\n",
-       "            (drop): Dropout(p=0, inplace=False)\n",
-       "          )\n",
-       "          (inner_cross_attn): FlashCrossAttention(\n",
-       "            (drop): Dropout(p=0, inplace=False)\n",
-       "          )\n",
-       "          (out_proj): FusedDense(in_features=1024, out_features=1024, bias=False)\n",
-       "        )\n",
-       "        (dropout1): Dropout(p=0, inplace=False)\n",
-       "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
-       "        (norm1): RMSNorm()\n",
-       "        (mlp): GatedMlp(\n",
-       "          (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
-       "          (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
-       "        )\n",
-       "        (dropout2): Dropout(p=0, inplace=False)\n",
-       "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
-       "        (norm2): RMSNorm()\n",
-       "      )\n",
-       "      (13-15): 3 x Block(\n",
-       "        (mixer): BaseConv(\n",
-       "          (in_proj): Linear(in_features=1024, out_features=4096, bias=True)\n",
-       "          (out_proj): Linear(in_features=2048, out_features=1024, bias=True)\n",
-       "          (conv): ShortConvolution(\n",
-       "            (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n",
-       "          )\n",
-       "        )\n",
-       "        (dropout1): Dropout(p=0, inplace=False)\n",
-       "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
-       "        (norm1): RMSNorm()\n",
-       "        (mlp): GatedMlp(\n",
-       "          (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
-       "          (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
-       "        )\n",
-       "        (dropout2): Dropout(p=0, inplace=False)\n",
-       "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
-       "        (norm2): RMSNorm()\n",
-       "      )\n",
-       "      (16): Block(\n",
-       "        (mixer): LinearAttention(\n",
-       "          (feature_map): TaylorExp()\n",
-       "          (proj_q): Linear(in_features=1024, out_features=256, bias=False)\n",
-       "          (proj_k): Linear(in_features=1024, out_features=256, bias=False)\n",
-       "          (proj_v): Linear(in_features=1024, out_features=1024, bias=False)\n",
-       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
-       "        )\n",
-       "        (dropout1): Dropout(p=0, inplace=False)\n",
-       "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
-       "        (norm1): RMSNorm()\n",
-       "        (mlp): GatedMlp(\n",
-       "          (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
-       "          (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
-       "        )\n",
-       "        (dropout2): Dropout(p=0, inplace=False)\n",
-       "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
-       "        (norm2): RMSNorm()\n",
-       "      )\n",
-       "      (17): Block(\n",
-       "        (mixer): SlidingAttention(\n",
-       "          (rotary_emb): RotaryEmbedding()\n",
-       "          (Wqkv): FusedDense(in_features=1024, out_features=3072, bias=False)\n",
-       "          (inner_attn): FlashSelfAttention(\n",
-       "            (drop): Dropout(p=0, inplace=False)\n",
-       "          )\n",
-       "          (inner_cross_attn): FlashCrossAttention(\n",
-       "            (drop): Dropout(p=0, inplace=False)\n",
-       "          )\n",
-       "          (out_proj): FusedDense(in_features=1024, out_features=1024, bias=False)\n",
-       "        )\n",
-       "        (dropout1): Dropout(p=0, inplace=False)\n",
-       "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
-       "        (norm1): RMSNorm()\n",
-       "        (mlp): GatedMlp(\n",
-       "          (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
-       "          (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
-       "        )\n",
-       "        (dropout2): Dropout(p=0, inplace=False)\n",
-       "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
-       "        (norm2): RMSNorm()\n",
-       "      )\n",
-       "      (18-20): 3 x Block(\n",
-       "        (mixer): BaseConv(\n",
-       "          (in_proj): Linear(in_features=1024, out_features=4096, bias=True)\n",
-       "          (out_proj): Linear(in_features=2048, out_features=1024, bias=True)\n",
-       "          (conv): ShortConvolution(\n",
-       "            (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n",
-       "          )\n",
-       "        )\n",
-       "        (dropout1): Dropout(p=0, inplace=False)\n",
-       "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
-       "        (norm1): RMSNorm()\n",
-       "        (mlp): GatedMlp(\n",
-       "          (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
-       "          (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
-       "        )\n",
-       "        (dropout2): Dropout(p=0, inplace=False)\n",
-       "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
-       "        (norm2): RMSNorm()\n",
-       "      )\n",
-       "      (21): Block(\n",
-       "        (mixer): LinearAttention(\n",
-       "          (feature_map): TaylorExp()\n",
-       "          (proj_q): Linear(in_features=1024, out_features=256, bias=False)\n",
-       "          (proj_k): Linear(in_features=1024, out_features=256, bias=False)\n",
-       "          (proj_v): Linear(in_features=1024, out_features=1024, bias=False)\n",
-       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
-       "        )\n",
-       "        (dropout1): Dropout(p=0, inplace=False)\n",
-       "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
-       "        (norm1): RMSNorm()\n",
-       "        (mlp): GatedMlp(\n",
-       "          (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
-       "          (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
-       "        )\n",
-       "        (dropout2): Dropout(p=0, inplace=False)\n",
-       "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
-       "        (norm2): RMSNorm()\n",
-       "      )\n",
-       "      (22): Block(\n",
-       "        (mixer): SlidingAttention(\n",
-       "          (rotary_emb): RotaryEmbedding()\n",
-       "          (Wqkv): FusedDense(in_features=1024, out_features=3072, bias=False)\n",
-       "          (inner_attn): FlashSelfAttention(\n",
-       "            (drop): Dropout(p=0, inplace=False)\n",
-       "          )\n",
-       "          (inner_cross_attn): FlashCrossAttention(\n",
-       "            (drop): Dropout(p=0, inplace=False)\n",
-       "          )\n",
-       "          (out_proj): FusedDense(in_features=1024, out_features=1024, bias=False)\n",
-       "        )\n",
-       "        (dropout1): Dropout(p=0, inplace=False)\n",
-       "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
-       "        (norm1): RMSNorm()\n",
-       "        (mlp): GatedMlp(\n",
-       "          (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
-       "          (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
-       "        )\n",
-       "        (dropout2): Dropout(p=0, inplace=False)\n",
-       "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
-       "        (norm2): RMSNorm()\n",
-       "      )\n",
-       "      (23-26): 4 x Block(\n",
-       "        (mixer): BaseConv(\n",
-       "          (in_proj): Linear(in_features=1024, out_features=4096, bias=True)\n",
-       "          (out_proj): Linear(in_features=2048, out_features=1024, bias=True)\n",
-       "          (conv): ShortConvolution(\n",
-       "            (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n",
-       "          )\n",
-       "        )\n",
-       "        (dropout1): Dropout(p=0, inplace=False)\n",
-       "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
-       "        (norm1): RMSNorm()\n",
-       "        (mlp): GatedMlp(\n",
-       "          (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
-       "          (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
-       "        )\n",
-       "        (dropout2): Dropout(p=0, inplace=False)\n",
-       "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
-       "        (norm2): RMSNorm()\n",
-       "      )\n",
-       "    )\n",
-       "    (drop_f): Dropout(p=0, inplace=False)\n",
-       "    (ln_f): RMSNorm()\n",
-       "  )\n",
-       "  (lm_head): Linear(in_features=1024, out_features=50264, bias=False)\n",
-       ")"
-      ]
-     },
-     "execution_count": 6,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
-   "source": [
-    "# Step 4: Inspect the hybrid structure of Based\n",
-    "model"
-   ]
-  },
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "### Sample next token predictions"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 8,
-   "metadata": {},
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "8\n",
-      "Stan -> ford\n",
-      "ford ->  University\n",
-      " university -> ,\n",
-      " is ->  a\n",
-      " in ->  the\n",
-      " the ->  process\n",
-      " state ->  of\n",
-      " of ->  California\n"
-     ]
-    }
-   ],
-   "source": [
-    "\n",
-    "input = tokenizer(\"Stanford university is in the state of\", return_tensors=\"pt\").to(\"cuda\")\n",
-    "\n",
-    "model.eval()\n",
-    "with torch.no_grad():\n",
-    "    output = model(**input)\n",
-    "print(len(output.logits[0]))\n",
-    "max = output.logits.argmax(dim=-1)[0]\n",
-    "\n",
-    "# next token predictions\n",
-    "for tok, out_tok in zip(input[\"input_ids\"][0], max):\n",
-    "    print(f\"{tokenizer.decode(tok.item())} -> {tokenizer.decode(out_tok.item())}\")\n"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": []
-  },
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "### Generation with Based"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 76,
-   "metadata": {},
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "start=19, limit=21\n",
-      "The capital of California is Sacramento. The capital of Italy is Rome. The capital of France is ->  Paris.\n"
-     ]
-    }
-   ],
-   "source": [
-    "# Inputs\n",
-    "input_text = \"The capital of California is Sacramento. The capital of Italy is Rome. The capital of France is\" \n",
-    "\n",
-    "context_length = 2048\n",
-    "generation_length = 2\n",
-    "tokenizer.padding_side = \"left\"\n",
-    "tokenizer.pad_token = tokenizer.eos_token\n",
-    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
-    "inputs = tokenizer.batch_encode_plus(\n",
-    "    [input_text], return_tensors=\"pt\", padding=True, truncation=True, max_length=context_length\n",
-    ").input_ids.to(\"cuda\")\n",
-    "\n",
-    "limit = inputs.shape[-1] + generation_length\n",
-    "start = inputs.shape[-1]\n",
-    "print(f\"{start=}, {limit=}\")\n",
-    "\n",
-    "# Generate\n",
-    "model.eval()\n",
-    "with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
-    "    with torch.no_grad():\n",
-    "\n",
-    "        fn = model.generate\n",
-    "        generations = fn(\n",
-    "            input_ids=inputs,\n",
-    "            max_length=limit,\n",
-    "            temperature=0.1,\n",
-    "            top_k=1,\n",
-    "            top_p=1.0,\n",
-    "        )\n",
-    "        preds = generations[:, start:]\n",
-    "        pred_ids =  preds[0].tolist()\n",
-    "        pred = tokenizer.decode(pred_ids)\n",
-    "        input_text = tokenizer.decode(inputs[0].tolist())  \n",
-    "\n",
-    "print(f\"{input_text} -> {pred}\")"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": []
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": []
-  }
- ],
- "metadata": {
-  "kernelspec": {
-   "display_name": "dev",
-   "language": "python",
-   "name": "python3"
-  },
-  "language_info": {
-   "codemirror_mode": {
-    "name": "ipython",
-    "version": 3
-   },
-   "file_extension": ".py",
-   "mimetype": "text/x-python",
-   "name": "python",
-   "nbconvert_exporter": "python",
-   "pygments_lexer": "ipython3",
-   "version": "3.8.18"
-  }
- },
- "nbformat": 4,
- "nbformat_minor": 2
-}
diff --git a/notebooks/03-31-decay.ipynb b/notebooks/03-31-decay.ipynb
deleted file mode 100644
index 2e63de8..0000000
--- a/notebooks/03-31-decay.ipynb
+++ /dev/null
@@ -1,156 +0,0 @@
-{
- "cells": [
-  {
-   "cell_type": "code",
-   "execution_count": 1,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "import torch\n",
-    "import torch.nn as nn\n",
-    "from einops import rearrange"
-   ]
-  },
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "#### Fixed-decay with projections\n",
-    "\n",
-    "The objective of this notebook is to provide an example of the fixed-decay approach discussed in the Based paper (https://arxiv.org/abs/2402.18668). While Based achieves high-quality with *no* decay whatsoever, the following addition may be helpful to your use case. "
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 2,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "# inputs\n",
-    "\n",
-    "b, h, n, d, f = 2, 4, 64, 16, 16\n",
-    "eps = 1e-12\n",
-    "d_model = h * d\n",
-    "q = torch.randn(b, h, n, f)\n",
-    "k = torch.randn(b, h, n, f)\n",
-    "v = torch.randn(b, h, n, d)\n",
-    "hidden_states = torch.randn(b, n, d_model)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 3,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "# construct the fixed decay matrices\n",
-    "\n",
-    "class DecayClass(nn.Module):\n",
-    "    def __init__(self, l_max, decay_const=-3, decay_denom=False, n_kv_heads=16):\n",
-    "        super().__init__()\n",
-    "        self.l_max = l_max\n",
-    "        assert self.l_max > 0, print(f'double check l_max')\n",
-    "        decay_const = decay_const\n",
-    "        self.decay_denom = decay_denom\n",
-    "        self.num_heads = n_kv_heads\n",
-    "        decay = torch.log(1 - 2 ** (decay_const - torch.arange(self.num_heads, dtype=torch.float)))\n",
-    "        self.register_buffer(\"decay\", decay)\n",
-    "    \n",
-    "    def forward(self):\n",
-    "        index = torch.arange(self.l_max).to(self.decay)\n",
-    "        mask = torch.tril(torch.ones(self.l_max, self.l_max).to(self.decay))\n",
-    "        mask = torch.masked_fill(index[:, None] - index[None, :], ~mask.bool(), float(\"inf\"))\n",
-    "        mask = torch.exp(mask * self.decay[:, None, None])\n",
-    "        mask = torch.nan_to_num(mask)\n",
-    "        if self.decay_denom:\n",
-    "            mask = mask / mask.sum(dim=-1, keepdim=True).sqrt()\n",
-    "        return mask, torch.exp(self.decay)\n",
-    "\n",
-    "\n",
-    "decay_cls = DecayClass(l_max=n, decay_const=-3, decay_denom=False, n_kv_heads=h)\n"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 4,
-   "metadata": {},
-   "outputs": [
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "/var/cr05_data/sim_data/miniconda3/envs/based/lib/python3.8/site-packages/tqdm-4.66.1-py3.8.egg/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
-      "  from .autonotebook import tqdm as notebook_tqdm\n"
-     ]
-    }
-   ],
-   "source": [
-    "# plug into linear attention\n",
-    "\n",
-    "# Version 1: default, no decay (https://github.com/HazyResearch/based/blob/9db60a33d20e6c024de97703715768da9d872e30/based/models/mixers/linear_attention.py#L136)\n",
-    "A_qk = torch.einsum(\"bhnd,bhmd->bhnm\", q, k) \n",
-    "A_qk = torch.tril(A_qk)        \n",
-    "y = torch.einsum(\"bhnm,bhme->bhne\", A_qk.to(q.dtype), v.to(q.dtype))\n",
-    "z = 1 / (torch.einsum(\"bhld,bhld->bhl\", q, k.cumsum(2)) + eps)\n",
-    "y = y * z[..., None]\n",
-    "y = rearrange(y, 'b h l d -> b l (h d)')\n",
-    "\n",
-    "\n",
-    "# Version 2: with decay\n",
-    "use_decay_proj = True\n",
-    "decay_proj = nn.Linear(d_model, h)\n",
-    "cumsum_matrix = torch.tril(torch.ones((n, n))).to(q.device, q.dtype)\n",
-    "\n",
-    "decay = decay_cls()\n",
-    "decay, decay_recurrent = decay if decay is not None else (None, None)\n",
-    "\n",
-    "A_qk = torch.einsum(\"bhnd,bhmd->bhnm\", q, k) \n",
-    "if decay is not None:\n",
-    "    decay = decay[:, :n, :n]\n",
-    "    if len(decay.shape) == 3:\n",
-    "        decay = decay.unsqueeze(0)\n",
-    "    if use_decay_proj:\n",
-    "        dt_out = decay_proj(hidden_states) # (b l d) --> (b, l, h)\n",
-    "        assert decay.shape[2] >= n, f\"decay matrix {decay.shape} to short for sequence length {l}\"\n",
-    "        decay_mat = dt_out.transpose(1,2).unsqueeze(-1) * decay   # (b, h, l, 1) * (1, h, l, l)\n",
-    "    elif decay is not None:\n",
-    "        decay_mat = decay\n",
-    "    A_qk = A_qk * decay_mat\n",
-    "else:\n",
-    "    A_qk = A_qk * cumsum_matrix       \n",
-    "out = torch.einsum(\"bhnm,bhme->bhne\", A_qk.to(hidden_states.dtype), v.to(hidden_states.dtype))\n",
-    "z = 1 / (torch.einsum(\"bhld,bhld->bhl\", q, k.cumsum(2)) + eps)\n",
-    "y = out * z[..., None]\n",
-    "y = y.to(hidden_states.dtype)\n"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": []
-  }
- ],
- "metadata": {
-  "kernelspec": {
-   "display_name": "based",
-   "language": "python",
-   "name": "python3"
-  },
-  "language_info": {
-   "codemirror_mode": {
-    "name": "ipython",
-    "version": 3
-   },
-   "file_extension": ".py",
-   "mimetype": "text/x-python",
-   "name": "python",
-   "nbconvert_exporter": "python",
-   "pygments_lexer": "ipython3",
-   "version": "3.8.18"
-  }
- },
- "nbformat": 4,
- "nbformat_minor": 2
-}
diff --git a/setup.py b/setup.py
index 2f4ee05..dd0ed95 100644
--- a/setup.py
+++ b/setup.py
@@ -17,7 +17,7 @@ _REQUIRED = [
     "dill==0.3.6",
     "multiprocess==0.70.14",
     "huggingface-hub==0.19.4",
-    "transformers==4.36.2",
+    "transformers",
     "einops==0.7.0",
     "ftfy==6.1.3",
     "opt-einsum==3.3.0",
@@ -25,14 +25,11 @@ _REQUIRED = [
     "pydantic-core==2.14.6",
     "pykeops==2.2",
     "python-dotenv==1.0.0",
-    "sentencepiece==0.1.99",
+    "sentencepiece",
     "tokenizers==0.15.0",
     "six==1.16.0",
     "scikit-learn==1.3.2",
-    "lm-eval==0.4.1",
     "ninja==1.11.1.1",
-    "flash-attn==2.5.2",
-    "causal-conv1d",
 ]
 
 _OPTIONAL = {
@@ -53,10 +50,10 @@ _OPTIONAL = {
 
 
 setup(
-    name='based',
+    name='tktrainer',
     version="0.0.1",
-    packages=find_packages(include=['based', 'based.*']),
-    author="Based",
+    packages=find_packages(include=['tktrainer', 'tktrainer.*']),
+    author="TK Train",
     author_email="",
     description="",
     python_requires=">=3.8",
Submodule synthetic eac8015...0000000 (submodule deleted)
diff --git a/train/configs/datamodule/cifar.yaml b/train/configs/datamodule/cifar.yaml
deleted file mode 100644
index 87d33e5..0000000
--- a/train/configs/datamodule/cifar.yaml
+++ /dev/null
@@ -1,12 +0,0 @@
-_name_: cifar
-permute: null
-grayscale: False
-tokenize: False
-augment: False
-cutout: False
-random_erasing: False
-val_split: 0.1
-seed: 42 # For validation split
-__train_len: 50000
-__max_len: 1024
-num_classes: 10
\ No newline at end of file
diff --git a/train/configs/datamodule/listops.yaml b/train/configs/datamodule/listops.yaml
deleted file mode 100644
index a6645df..0000000
--- a/train/configs/datamodule/listops.yaml
+++ /dev/null
@@ -1,8 +0,0 @@
-_name_: listops
-l_max: 2048
-append_bos: False
-append_eos: True
-n_workers: 4 # Only used for tokenizing once
-max_vocab: 20 # Actual size 18
-# __train_len: 96000
-__l_max: ${.l_max}
\ No newline at end of file
diff --git a/train/configs/datamodule/openwebtext.yaml b/train/configs/datamodule/openwebtext.yaml
index af7c909..14858c9 100755
--- a/train/configs/datamodule/openwebtext.yaml
+++ b/train/configs/datamodule/openwebtext.yaml
@@ -2,7 +2,7 @@ _target_: train.datamodules.language_modeling_hf.LMDataModule
 dataset_name: openwebtext
 dataset_config_name: null
 tokenizer_name: gpt2
-cache_dir: ${oc.env:DATA_DIR,${data_dir}}/openwebtext/cache
+cache_dir: /scratch/openwebtext/cache
 max_length: 1024
 val_ratio: 0.0005
 val_split_seed: 2357
diff --git a/train/configs/datamodule/scrolls.yaml b/train/configs/datamodule/scrolls.yaml
deleted file mode 100644
index 2ac7b86..0000000
--- a/train/configs/datamodule/scrolls.yaml
+++ /dev/null
@@ -1,13 +0,0 @@
-_target_: train.datamodules.scrolls.ScrollsDataModule
-dataset_name: tau/scrolls
-dataset_config_name: gov_report
-cache_dir: ${oc.env:DATA_DIR,${data_dir}}/scrolls/cache
-max_length: 2048
-add_eos: True
-batch_size: 4  # per GPU
-batch_size_eval: ${eval:${.batch_size} * 2}
-num_workers: 64  # For preprocessing only
-use_shmem: False
-shuffle: True
-pin_memory: True
-__train_len: ${div_up:374337375694, ${.max_length}}
diff --git a/train/configs/datamodule/thepile.yaml b/train/configs/datamodule/thepile.yaml
deleted file mode 100755
index 041af7e..0000000
--- a/train/configs/datamodule/thepile.yaml
+++ /dev/null
@@ -1,14 +0,0 @@
-_target_: train.datamodules.language_modeling_hf.LMDataModule
-dataset_name: EleutherAI/pile
-# dataset_config_name: 
-tokenizer_name: gpt2
-cache_dir: ${oc.env:DATA_DIR,${data_dir}}/wikitext103/cache
-max_length: 2048
-add_eos: True
-batch_size: 4  # per GPU
-batch_size_eval: ${eval:${.batch_size} * 2}
-num_workers: 64  # For preprocessing only
-use_shmem: False
-shuffle: True
-pin_memory: True
-__train_len: ${div_up:374337375694, ${.max_length}}
diff --git a/train/configs/datamodule/wiki_bio.yaml b/train/configs/datamodule/wiki_bio.yaml
deleted file mode 100644
index 6119d37..0000000
--- a/train/configs/datamodule/wiki_bio.yaml
+++ /dev/null
@@ -1,15 +0,0 @@
-_target_: train.datamodules.continual_lm.LMDataModule
-dataset_name: wikitext
-# EleutherAI/pile
-dataset_config_name: wikitext-103-v1
-tokenizer_name: gpt2
-cache_dir: ${oc.env:DATA_DIR,${data_dir}}/wikitext103/cache
-max_length: 2048
-add_eos: True
-batch_size: 4  # per GPU
-batch_size_eval: ${eval:${.batch_size} * 2}
-num_workers: 64  # For preprocessing only
-use_shmem: False
-shuffle: True
-pin_memory: True
-__train_len: ${div_up:374337375694, ${.max_length}}
diff --git a/train/configs/datamodule/wikitext103.yaml b/train/configs/datamodule/wikitext103.yaml
deleted file mode 100644
index a25e847..0000000
--- a/train/configs/datamodule/wikitext103.yaml
+++ /dev/null
@@ -1,15 +0,0 @@
-_target_: train.datamodules.language_modeling_hf.LMDataModule
-dataset_name: wikitext
-# EleutherAI/pile
-dataset_config_name: wikitext-103-v1
-tokenizer_name: gpt2
-cache_dir: ${oc.env:DATA_DIR,${data_dir}}/wikitext103/cache
-max_length: 2048
-add_eos: True
-batch_size: 4  # per GPU
-batch_size_eval: ${eval:${.batch_size} * 2}
-num_workers: 64  # For preprocessing only
-use_shmem: False
-shuffle: True
-pin_memory: True
-__train_len: ${div_up:374337375694, ${.max_length}}
diff --git a/train/configs/experiment/bearcat/gpt-160m.yaml b/train/configs/experiment/bearcat/gpt-160m.yaml
deleted file mode 100644
index 1328d7c..0000000
--- a/train/configs/experiment/bearcat/gpt-160m.yaml
+++ /dev/null
@@ -1,91 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/gpt3m-flash.yaml
-  - override /datamodule: slim6B
-
-train: 
-  optimizer:
-    lr: 0.0008
-    betas: [0.9, 0.95]
-    _target_: apex.optimizers.FusedAdam
-    adam_w_mode: true
-    weight_decay: 0.1
-  
-  scheduler: 
-    lr_min: 0.00008
-    _target_: train.optim.timm_lr_scheduler.TimmCosineLRScheduler
-    warmup_t: 200
-    t_initial: 19800
-    t_in_epochs: false
-    warmup_prefix: true
-    warmup_lr_init: 0.000001
-
-trainer: 
-  # this interval is in terms of batch_idx not in terms of global_step, so we need 
-  # to multiply by accumulate_grad_batches
-  val_check_interval: ${eval:1000 * ${.accumulate_grad_batches}}
-  max_steps: 20000
-
-
-datamodule:
-  _target_: train.datamodules.language_modeling_hf.LMDataModule   
-  batch_size: 16  # per gpu
-  batch_size_eval: 32
-#  num_predict_batches_eval: 100
-  # global_batch_size: ${..train.global_batch_size}
-  # max_steps: ${..trainer.max_steps}
-  # num_test_samples: 1000
-  # num_valid_samples: 1000
-
-
-expt_name: gpt-160m-test
-name: ${.expt_name}
-
-
-callbacks:
-  model_checkpoint:
-    dirpath: /home/bfs/quinn/BearCat/based/checkpoints/${expt_name}
-
-resume: True
-
-model:
-  config:
-    n_embd: 768
-    n_head: 12
-    n_layer: 12
-    _target_: "transformers.GPT2Config"
-    rms_norm: true
-    fused_mlp: false
-    attn_pdrop: 0
-    embd_pdrop: 0
-    n_positions: 2048
-    resid_pdrop: 0
-    mlp_fc1_bias: false
-    mlp_fc2_bias: false
-    fused_bias_fc: true
-    out_proj_bias: false
-    qkv_proj_bias: false
-    use_flash_attn: true
-    residual_in_fp32: true
-    activation_function: "swiglu" # flag
-    rotary_emb_fraction: 0.5
-    fused_dropout_add_ln: true
-    max_position_embeddings: 0
-    pad_vocab_size_multiple: 16
-    reorder_and_upcast_attn: false
-    scale_attn_by_inverse_layer_idx: false
-    fused_dense: false
-    # mlp_type: "alt"   # flagging alt MLP (Simran)
-    ff_mult: 4        # flagging alt MLP (Simran)
-
-#####################################################################
-
-
-# datamodule:
-#   batch_size: 8  # per gpu
-#   batch_size_eval: 32
-
-# expt_name: 02-20-based-360m
-# name: ${.expt_name}
-
-
diff --git a/train/configs/experiment/example/based-360m.yaml b/train/configs/experiment/example/based-360m.yaml
deleted file mode 100644
index 63ab118..0000000
--- a/train/configs/experiment/example/based-360m.yaml
+++ /dev/null
@@ -1,112 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/gpt3m-flash.yaml
-  - override /datamodule: wikitext103
-
-train: 
-  optimizer:
-    lr: 0.0008
-    betas: [0.9, 0.95]
-    _target_: apex.optimizers.FusedAdam
-    adam_w_mode: true
-    weight_decay: 0.1
-  
-  scheduler: 
-    lr_min: 0.00008
-    _target_: train.optim.timm_lr_scheduler.TimmCosineLRScheduler
-    warmup_t: 200
-    t_initial: 19800
-    t_in_epochs: false
-    warmup_prefix: true
-    warmup_lr_init: 0.000001
-
-trainer: 
-  # this interval is in terms of batch_idx not in terms of global_step, so we need 
-  # to multiply by accumulate_grad_batches
-  val_check_interval: ${eval:1000 * ${.accumulate_grad_batches}}
-  max_steps: 20000
-
-
-datamodule:
-  batch_size: 8  # per gpu
-  batch_size_eval: 32
-
-expt_name: 02-20-based-360m
-name: ${.expt_name}
-
-callbacks:
-  model_checkpoint:
-    dirpath: /var/cr01_data/sabri_data/checkpoints/${expt_name}
-
-resume: True
-do_test: True
-
-model:
-  _target_: based.models.gpt.GPTLMHeadModel
-  _recursive_: false
-  config:
-    alt_mixer_layers: 
-      - 1
-      - 6
-      - 11
-      - 16
-      - 21
-
-    alt_mixer_2_layers:
-      - 2
-      - 7
-      - 12
-      - 17
-      - 22
-
-    mixer:
-      _target_: based.models.mixers.convolution.BaseConv
-      l_max: ${....datamodule.max_length}
-      use_bias: True
-      expand_proj: 4
-      kernel_sizes: 3
-    
-    alt_mixer: 
-      _target_: based.models.mixers.linear_attention.LinearAttention  
-      l_max: ${....datamodule.max_length}
-      feature_map: 
-        _target_: based.models.mixers.linear_attention.TaylorExp
-        input_dim: ${..feature_dim}
-      feature_dim: 16
-      num_heads: 16
-
-    alt_mixer_2:
-      _target_: based.models.mixers.slide_attention.SlidingAttention
-      window_size: 128 
-      num_heads: 16
-      causal: true
-
-    n_embd: 1024
-
-    special_initializer: true
-    n_head: 16
-    n_layer: 27
-    _target_: based.models.gpt.GPT2MixerConfig
-    rms_norm: true
-    fused_mlp: false
-    attn_pdrop: 0
-    embd_pdrop: 0
-    n_positions: 2048
-    resid_pdrop: 0
-    mlp_fc1_bias: false
-    mlp_fc2_bias: false
-    fused_bias_fc: true
-    out_proj_bias: false
-    qkv_proj_bias: false
-    use_flash_attn: true
-    residual_in_fp32: true
-    activation_function: "swiglu"    
-    rotary_emb_fraction: 1           # flagging 
-    fused_dropout_add_ln: true
-    max_position_embeddings: 0   # flagging 
-    pad_vocab_size_multiple: 8
-    reorder_and_upcast_attn: false
-    scale_attn_by_inverse_layer_idx: false
-
-    n_inner: ${eval:2 * ${.n_embd}}
-
diff --git a/train/configs/experiment/pile/base.yaml b/train/configs/experiment/pile/base.yaml
deleted file mode 100755
index cc8d9c0..0000000
--- a/train/configs/experiment/pile/base.yaml
+++ /dev/null
@@ -1,74 +0,0 @@
-# @package _global_
-defaults:
-  - override /trainer: default # choose trainer from 'configs/trainer/'
-  - override /model: null
-  - override /datamodule: thepile
-  - override /optimizer: adamw-apex  # slight speedup (1-2%) over Pytorch AdamW
-  - override /scheduler: cosine-warmup-timm
-  - override /callbacks: [default, norm-monitor]
-  - override /metrics: [perplexity, num-tokens]
-  - override /logger: wandb
-
-# all parameters below will be merged with parameters from default configurations set above
-# this allows you to overwrite only specified parameters
-
-task:
-  _target_: train.tasks.seq.SequenceLMModel
-
-seed: 1111
-
-trainer:
-  accelerator: gpu
-  devices: 8
-  num_nodes: 1
-  accumulate_grad_batches: ${div_up:${train.global_batch_size}, ${eval:${trainer.devices} * ${datamodule.batch_size} * ${trainer.num_nodes}}}
-  max_steps: 800000
-  val_check_interval: ${eval:2000 * ${.accumulate_grad_batches}}
-  check_val_every_n_epoch: null  # We don't care about epoch boundary
-  precision: bf16
-  gradient_clip_val: 1.0
-  strategy: null
-
-datamodule:
-  batch_size: 16  # Per GPU
-  batch_size_eval: ${.batch_size}  # Fused dense only support batch size at most 64k
-  max_length: 2048
-  fault_tolerant: True
-  ddp: ${eval:"${trainer.devices} > 1"}
-
-train:
-  gpu_mem: ${eval:"round(float(__import__('subprocess').check_output('nvidia-smi -i 0 --query-gpu=memory.total --format=csv,noheader,nounits', shell=True).strip().decode()) / 1000)"}
-  global_batch_size: 256
-  optimizer:
-    lr: 6e-4
-    weight_decay: 0.1
-  optimizer_param_grouping:
-    bias_weight_decay: False
-    normalization_weight_decay: False
-  scheduler:
-    t_in_epochs: False
-    t_initial: 600000
-    warmup_lr_init: 1e-6
-    warmup_t: ${eval:0.01 * ${trainer.max_steps}}
-    lr_min: ${eval:0.1 * ${train.optimizer.lr}}
-  loss_fn:
-    # This is faster and uses less memory than torch.nn.CrossEntropyLoss.
-    # It's also more numerically stable if we're using DeepSpeed 16 bits.
-    _target_: flash_attn.losses.cross_entropy.CrossEntropyLoss
-    inplace_backward: True  # to save memory
-
-eval:
-  log_on_step: True  # 1 training epoch takes too long, we want to see metrics per train step
-
-callbacks:
-  model_checkpoint:
-    monitor: val/loss
-    mode: min
-    save_top_k: 3
-    save_last: True
-    every_n_train_steps: 1000
-    dirpath: /var/cr01_data/sim_data/checkpoints/${oc.select:name,''} #${work_dir}/checkpoints/${oc.select:name,''}
-    filename: step_{step}
-    auto_insert_metric_name: False
-  early_stopping: null
-
diff --git a/train/configs/experiment/pile/gpt3-2.7B-flash-8k.yaml b/train/configs/experiment/pile/gpt3-2.7B-flash-8k.yaml
deleted file mode 100755
index f126d18..0000000
--- a/train/configs/experiment/pile/gpt3-2.7B-flash-8k.yaml
+++ /dev/null
@@ -1,18 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/gpt3xl-flash-8k.yaml
-
-model:
-  config:
-    n_embd: 2560
-    n_head: 32
-    n_layer: 32
-    initializer_range: ${eval:"(2 / (${.n_embd} * 5)) ** 0.5"}
-    mlp_checkpoint_lvl: 0
-
-datamodule:
-  batch_size: ${eval:"1 if ${train.gpu_mem} < 40 else (2 if ${train.gpu_mem} < 80 else 4)"}
-
-train:
-  optimizer:
-    lr: 1.6e-4
diff --git a/train/configs/experiment/pile/gpt3-2.7B-flash-hdim128-rotary-8k.yaml b/train/configs/experiment/pile/gpt3-2.7B-flash-hdim128-rotary-8k.yaml
deleted file mode 100755
index 09fdee9..0000000
--- a/train/configs/experiment/pile/gpt3-2.7B-flash-hdim128-rotary-8k.yaml
+++ /dev/null
@@ -1,18 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/gpt3xl-flash-rotary-8k.yaml
-
-model:
-  config:
-    n_embd: 2560
-    n_head: 20
-    n_layer: 32
-    initializer_range: ${eval:"(2 / (${.n_embd} * 5)) ** 0.5"}
-    mlp_checkpoint_lvl: 0
-
-datamodule:
-  batch_size: ${eval:"1 if ${train.gpu_mem} < 24 else (2 if ${train.gpu_mem} < 40 else (4 if ${train.gpu_mem} < 80 else 8))"}
-
-train:
-  optimizer:
-    lr: 1.6e-4
diff --git a/train/configs/experiment/pile/gpt3-2.7B-flash-hdim128-rotary.yaml b/train/configs/experiment/pile/gpt3-2.7B-flash-hdim128-rotary.yaml
deleted file mode 100755
index d5caafd..0000000
--- a/train/configs/experiment/pile/gpt3-2.7B-flash-hdim128-rotary.yaml
+++ /dev/null
@@ -1,18 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/gpt3xl-flash-rotary.yaml
-
-model:
-  config:
-    n_embd: 2560
-    n_head: 20
-    n_layer: 32
-    initializer_range: ${eval:"(2 / (${.n_embd} * 5)) ** 0.5"}
-    mlp_checkpoint_lvl: 0
-
-datamodule:
-  batch_size: ${eval:"4 if ${train.gpu_mem} < 24 else (8 if ${train.gpu_mem} < 40 else (16 if ${train.gpu_mem} < 80 else 32))"}
-
-train:
-  optimizer:
-    lr: 1.6e-4
diff --git a/train/configs/experiment/pile/gpt3-2.7B-flash-hdim128.yaml b/train/configs/experiment/pile/gpt3-2.7B-flash-hdim128.yaml
deleted file mode 100755
index 9fd391b..0000000
--- a/train/configs/experiment/pile/gpt3-2.7B-flash-hdim128.yaml
+++ /dev/null
@@ -1,18 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/gpt3xl-flash.yaml
-
-model:
-  config:
-    n_embd: 2560
-    n_head: 20  # Headdim 128 is faster than headdim 80
-    n_layer: 32
-    initializer_range: ${eval:"(2 / (${.n_embd} * 5)) ** 0.5"}
-    mlp_checkpoint_lvl: 0
-
-datamodule:
-  batch_size: ${eval:"1 if ${train.gpu_mem} < 40 else (2 if ${train.gpu_mem} < 80 else 4)"}
-
-train:
-  optimizer:
-    lr: 1.6e-4
diff --git a/train/configs/experiment/pile/gpt3-2.7B-flash-rotary-8k.yaml b/train/configs/experiment/pile/gpt3-2.7B-flash-rotary-8k.yaml
deleted file mode 100755
index b259a29..0000000
--- a/train/configs/experiment/pile/gpt3-2.7B-flash-rotary-8k.yaml
+++ /dev/null
@@ -1,18 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/gpt3xl-flash-rotary-8k.yaml
-
-model:
-  config:
-    n_embd: 2560
-    n_head: 32
-    n_layer: 32
-    initializer_range: ${eval:"(2 / (${.n_embd} * 5)) ** 0.5"}
-    mlp_checkpoint_lvl: 0
-
-datamodule:
-  batch_size: ${eval:"1 if ${train.gpu_mem} < 24 else (2 if ${train.gpu_mem} < 40 else (4 if ${train.gpu_mem} < 80 else 8))"}
-
-train:
-  optimizer:
-    lr: 1.6e-4
diff --git a/train/configs/experiment/pile/gpt3-2.7B-flash-rotary.yaml b/train/configs/experiment/pile/gpt3-2.7B-flash-rotary.yaml
deleted file mode 100755
index 1e1684c..0000000
--- a/train/configs/experiment/pile/gpt3-2.7B-flash-rotary.yaml
+++ /dev/null
@@ -1,18 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/gpt3xl-flash-rotary.yaml
-
-model:
-  config:
-    n_embd: 2560
-    n_head: 32
-    n_layer: 32
-    initializer_range: ${eval:"(2 / (${.n_embd} * 5)) ** 0.5"}
-    mlp_checkpoint_lvl: 0
-
-datamodule:
-  batch_size: ${eval:"4 if ${train.gpu_mem} < 24 else (8 if ${train.gpu_mem} < 40 else (16 if ${train.gpu_mem} < 80 else 32))"}
-
-train:
-  optimizer:
-    lr: 1.6e-4
diff --git a/train/configs/experiment/pile/gpt3-2.7B-flash.yaml b/train/configs/experiment/pile/gpt3-2.7B-flash.yaml
deleted file mode 100755
index 0dbfc21..0000000
--- a/train/configs/experiment/pile/gpt3-2.7B-flash.yaml
+++ /dev/null
@@ -1,18 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/gpt3xl-flash.yaml
-
-model:
-  config:
-    n_embd: 2560
-    n_head: 32
-    n_layer: 32
-    initializer_range: ${eval:"(2 / (${.n_embd} * 5)) ** 0.5"}
-    mlp_checkpoint_lvl: 0
-
-datamodule:
-  batch_size: ${eval:"1 if ${train.gpu_mem} < 40 else (2 if ${train.gpu_mem} < 80 else 4)"}
-
-train:
-  optimizer:
-    lr: 1.6e-4
diff --git a/train/configs/experiment/pile/gpt3-2.7B-hf-hdim128.yaml b/train/configs/experiment/pile/gpt3-2.7B-hf-hdim128.yaml
deleted file mode 100755
index cc365d9..0000000
--- a/train/configs/experiment/pile/gpt3-2.7B-hf-hdim128.yaml
+++ /dev/null
@@ -1,17 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/gpt3xl-hf.yaml
-
-model:
-  config:
-    n_embd: 2560
-    n_head: 128
-    n_layer: 32
-
-# OOM on A100 80GB even with batch_size = 1
-datamodule:
-  batch_size: 1
-
-train:
-  optimizer:
-    lr: 1.6e-4
diff --git a/train/configs/experiment/pile/gpt3-2.7B-hf.yaml b/train/configs/experiment/pile/gpt3-2.7B-hf.yaml
deleted file mode 100755
index ff0a7a7..0000000
--- a/train/configs/experiment/pile/gpt3-2.7B-hf.yaml
+++ /dev/null
@@ -1,16 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/gpt3xl-hf.yaml
-
-model:
-  config:
-    n_embd: 2560
-    n_head: 32
-    n_layer: 32
-
-datamodule:
-  batch_size: 1
-
-train:
-  optimizer:
-    lr: 1.6e-4
diff --git a/train/configs/experiment/pile/gpt3l-flash-8k.yaml b/train/configs/experiment/pile/gpt3l-flash-8k.yaml
deleted file mode 100755
index ccbbebf..0000000
--- a/train/configs/experiment/pile/gpt3l-flash-8k.yaml
+++ /dev/null
@@ -1,10 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/gpt3l-flash.yaml
-
-datamodule:
-  max_length: 8192
-  batch_size: ${eval:"1 if ${train.gpu_mem} < 40 else (2 if ${train.gpu_mem} < 80 else 4)"}
-
-train:
-  global_batch_size: 64
diff --git a/train/configs/experiment/pile/gpt3l-flash-rotary-30B.yaml b/train/configs/experiment/pile/gpt3l-flash-rotary-30B.yaml
deleted file mode 100755
index 74c6bb9..0000000
--- a/train/configs/experiment/pile/gpt3l-flash-rotary-30B.yaml
+++ /dev/null
@@ -1,10 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/gpt3l-flash-rotary.yaml
-
-trainer:
-  max_steps: 60000
-
-train:
-  scheduler:
-    t_initial: ${trainer.max_steps}
diff --git a/train/configs/experiment/pile/gpt3l-flash-rotary-8k.yaml b/train/configs/experiment/pile/gpt3l-flash-rotary-8k.yaml
deleted file mode 100755
index 2b3ba31..0000000
--- a/train/configs/experiment/pile/gpt3l-flash-rotary-8k.yaml
+++ /dev/null
@@ -1,8 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/gpt3l-flash-8k.yaml
-
-model:
-  config:
-    max_position_embeddings: 0  # Disable absolute position embedding
-    rotary_emb_fraction: 0.5
diff --git a/train/configs/experiment/pile/gpt3l-flash-rotary.yaml b/train/configs/experiment/pile/gpt3l-flash-rotary.yaml
deleted file mode 100755
index f285632..0000000
--- a/train/configs/experiment/pile/gpt3l-flash-rotary.yaml
+++ /dev/null
@@ -1,8 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/gpt3l-flash.yaml
-
-model:
-  config:
-    max_position_embeddings: 0  # Disable absolute position embedding
-    rotary_emb_fraction: 0.5
diff --git a/train/configs/experiment/pile/gpt3l-flash.yaml b/train/configs/experiment/pile/gpt3l-flash.yaml
deleted file mode 100755
index 14672c4..0000000
--- a/train/configs/experiment/pile/gpt3l-flash.yaml
+++ /dev/null
@@ -1,24 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/gpt3s-flash.yaml
-  - override /optimizer: adamw-zero
-
-model:
-  config:
-    n_embd: 1536
-    n_head: 16
-    n_layer: 24
-    # mlp_checkpoint_lvl: 1  # To fit batch_size 8
-
-datamodule:
-  batch_size: ${eval:"2 if ${train.gpu_mem} < 24 else (4 if ${train.gpu_mem} < 40 else (8 if ${train.gpu_mem} < 80 else 16))"}
-
-train:
-  optimizer:
-    lr: 2.5e-4
-
-trainer:
-  strategy:
-    _target_: train.utils.ddp_zero1.DDPStrategyZero1
-    find_unused_parameters: False
-    gradient_as_bucket_view: True
diff --git a/train/configs/experiment/pile/gpt3l-hf.yaml b/train/configs/experiment/pile/gpt3l-hf.yaml
deleted file mode 100755
index f70af10..0000000
--- a/train/configs/experiment/pile/gpt3l-hf.yaml
+++ /dev/null
@@ -1,16 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/gpt3s-hf.yaml
-
-model:
-  config:
-    n_embd: 1536
-    n_head: 16
-    n_layer: 24
-
-datamodule:
-  batch_size: 2
-
-train:
-  optimizer:
-    lr: 2.5e-4
diff --git a/train/configs/experiment/pile/gpt3m-flash-8k.yaml b/train/configs/experiment/pile/gpt3m-flash-8k.yaml
deleted file mode 100755
index d75e6d3..0000000
--- a/train/configs/experiment/pile/gpt3m-flash-8k.yaml
+++ /dev/null
@@ -1,10 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/gpt3m-flash.yaml
-
-datamodule:
-  max_length: 8192
-  batch_size: ${eval:"2 if ${train.gpu_mem} < 24 else (4 if ${train.gpu_mem} < 40 else 8)"}
-
-train:
-  global_batch_size: 64
diff --git a/train/configs/experiment/pile/gpt3m-flash-rotary-30B.yaml b/train/configs/experiment/pile/gpt3m-flash-rotary-30B.yaml
deleted file mode 100755
index 0463075..0000000
--- a/train/configs/experiment/pile/gpt3m-flash-rotary-30B.yaml
+++ /dev/null
@@ -1,10 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/gpt3m-flash-rotary.yaml
-
-trainer:
-  max_steps: 60000
-
-train:
-  scheduler:
-    t_initial: ${trainer.max_steps}
diff --git a/train/configs/experiment/pile/gpt3m-flash-rotary-8k.yaml b/train/configs/experiment/pile/gpt3m-flash-rotary-8k.yaml
deleted file mode 100755
index f217ac5..0000000
--- a/train/configs/experiment/pile/gpt3m-flash-rotary-8k.yaml
+++ /dev/null
@@ -1,8 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/gpt3m-flash-8k.yaml
-
-model:
-  config:
-    max_position_embeddings: 0  # Disable absolute position embedding
-    rotary_emb_fraction: 0.5
diff --git a/train/configs/experiment/pile/gpt3m-flash-rotary.yaml b/train/configs/experiment/pile/gpt3m-flash-rotary.yaml
deleted file mode 100755
index 654de88..0000000
--- a/train/configs/experiment/pile/gpt3m-flash-rotary.yaml
+++ /dev/null
@@ -1,26 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/gpt3m-flash.yaml
-
-train: 
-  max_steps: 20000
-
-trainer: 
-  # this interval is in terms of batch_idx not in terms of global_step, so we need 
-  # to multiply by accumulate_grad_batches
-  val_check_interval: ${eval:1000 * ${.accumulate_grad_batches}}
-
-
-datamodule:
-  _target_: train.datamodules.language_modeling_neox.NeoxLMDataModule   
-  batch_size: 8  # per gpu
-  batch_size_eval: 16
-  global_batch_size: ${..train.global_batch_size}
-  max_steps: ${..train.max_steps}
-  num_test_samples: 1000
-  num_valid_samples: 1000
-
-model:
-  config:
-    max_position_embeddings: 0  # Disable absolute position embedding
-    rotary_emb_fraction: 0.5
\ No newline at end of file
diff --git a/train/configs/experiment/pile/gpt3m-flash.yaml b/train/configs/experiment/pile/gpt3m-flash.yaml
deleted file mode 100755
index 830b2d5..0000000
--- a/train/configs/experiment/pile/gpt3m-flash.yaml
+++ /dev/null
@@ -1,16 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/gpt3s-flash.yaml
-  - override /model/gpt2model: gpt2-medium
-
-# Can enable mlp_checkpoint_lvl to fit batch_size 16 to A100 40GB
-# model:
-#   config:
-#     mlp_checkpoint_lvl: 1
-
-datamodule:
-  batch_size: ${eval:"4 if ${train.gpu_mem} < 24 else (8 if ${train.gpu_mem} < 40 else (16 if ${train.gpu_mem} < 80 else 32))"}
-
-train:
-  optimizer:
-    lr: 3.0e-4
diff --git a/train/configs/experiment/pile/gpt3m-hf.yaml b/train/configs/experiment/pile/gpt3m-hf.yaml
deleted file mode 100755
index e0e09e4..0000000
--- a/train/configs/experiment/pile/gpt3m-hf.yaml
+++ /dev/null
@@ -1,11 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/gpt3s-hf.yaml
-  - override /model/gpt2model: gpt2-medium
-
-datamodule:
-  batch_size: 4
-
-train:
-  optimizer:
-    lr: 3.0e-4
diff --git a/train/configs/experiment/pile/gpt3s-flash-8k.yaml b/train/configs/experiment/pile/gpt3s-flash-8k.yaml
deleted file mode 100755
index 06ce645..0000000
--- a/train/configs/experiment/pile/gpt3s-flash-8k.yaml
+++ /dev/null
@@ -1,10 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/gpt3s-flash.yaml
-
-datamodule:
-  max_length: 8192
-  batch_size: ${eval:"2 if ${train.gpu_mem} < 24 else (4 if ${train.gpu_mem} < 40 else 8)"}
-
-train:
-  global_batch_size: 64
diff --git a/train/configs/experiment/pile/gpt3s-flash-rotary-30B.yaml b/train/configs/experiment/pile/gpt3s-flash-rotary-30B.yaml
deleted file mode 100755
index d434480..0000000
--- a/train/configs/experiment/pile/gpt3s-flash-rotary-30B.yaml
+++ /dev/null
@@ -1,10 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/gpt3s-flash-rotary.yaml
-
-trainer:
-  max_steps: 60000
-
-train:
-  scheduler:
-    t_initial: ${trainer.max_steps}
diff --git a/train/configs/experiment/pile/gpt3s-flash-rotary-8k.yaml b/train/configs/experiment/pile/gpt3s-flash-rotary-8k.yaml
deleted file mode 100755
index bdee876..0000000
--- a/train/configs/experiment/pile/gpt3s-flash-rotary-8k.yaml
+++ /dev/null
@@ -1,8 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/gpt3s-flash-8k.yaml
-
-model:
-  config:
-    max_position_embeddings: 0  # Disable absolute position embedding
-    rotary_emb_fraction: 0.5
diff --git a/train/configs/experiment/pile/gpt3s-flash-rotary.yaml b/train/configs/experiment/pile/gpt3s-flash-rotary.yaml
deleted file mode 100755
index 41176ee..0000000
--- a/train/configs/experiment/pile/gpt3s-flash-rotary.yaml
+++ /dev/null
@@ -1,8 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/gpt3s-flash.yaml
-
-model:
-  config:
-    max_position_embeddings: 0  # Disable absolute position embedding
-    rotary_emb_fraction: 0.5
diff --git a/train/configs/experiment/pile/gpt3s-flash.yaml b/train/configs/experiment/pile/gpt3s-flash.yaml
deleted file mode 100755
index 45302fd..0000000
--- a/train/configs/experiment/pile/gpt3s-flash.yaml
+++ /dev/null
@@ -1,18 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/base.yaml
-  - override /model: gpt2
-  - override /model/gpt2model: gpt2-small
-
-model:
-  config:
-    # n_positions is already set to ${datamodule.max_length}
-    residual_in_fp32: True
-    use_flash_attn: True
-    fused_dropout_add_ln: True
-    fused_mlp: True
-    fused_bias_fc: True
-    pad_vocab_size_multiple: 8
-
-datamodule:
-  batch_size: ${eval:"8 if ${train.gpu_mem} < 24 else (16 if ${train.gpu_mem} < 40 else 32)"}
diff --git a/train/configs/experiment/pile/gpt3s-hf.yaml b/train/configs/experiment/pile/gpt3s-hf.yaml
deleted file mode 100755
index 4591217..0000000
--- a/train/configs/experiment/pile/gpt3s-hf.yaml
+++ /dev/null
@@ -1,12 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/base.yaml
-  - override /model: gpt2-hf
-  - override /model/gpt2model: gpt2-small
-
-datamodule:
-  batch_size: 8
-
-train:
-  # Use the standard torch.nn.CrossEntropyLoss
-  loss_fn: null
diff --git a/train/configs/experiment/pile/gpt3xl-flash-8k.yaml b/train/configs/experiment/pile/gpt3xl-flash-8k.yaml
deleted file mode 100755
index d411906..0000000
--- a/train/configs/experiment/pile/gpt3xl-flash-8k.yaml
+++ /dev/null
@@ -1,10 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/gpt3xl-flash.yaml
-
-datamodule:
-  max_length: 8192
-  batch_size: ${eval:"1 if ${train.gpu_mem} < 40 else (2 if ${train.gpu_mem} < 80 else 4)"}
-
-train:
-  global_batch_size: 128
diff --git a/train/configs/experiment/pile/gpt3xl-flash-rotary-60B.yaml b/train/configs/experiment/pile/gpt3xl-flash-rotary-60B.yaml
deleted file mode 100755
index 48e4213..0000000
--- a/train/configs/experiment/pile/gpt3xl-flash-rotary-60B.yaml
+++ /dev/null
@@ -1,10 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/gpt3xl-flash-rotary.yaml
-
-trainer:
-  max_steps: 60000
-
-train:
-  scheduler:
-    t_initial: ${trainer.max_steps}
diff --git a/train/configs/experiment/pile/gpt3xl-flash-rotary-8k.yaml b/train/configs/experiment/pile/gpt3xl-flash-rotary-8k.yaml
deleted file mode 100755
index d4c4cbe..0000000
--- a/train/configs/experiment/pile/gpt3xl-flash-rotary-8k.yaml
+++ /dev/null
@@ -1,8 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/gpt3xl-flash-8k.yaml
-
-model:
-  config:
-    max_position_embeddings: 0  # Disable absolute position embedding
-    rotary_emb_fraction: 0.5
diff --git a/train/configs/experiment/pile/gpt3xl-flash-rotary.yaml b/train/configs/experiment/pile/gpt3xl-flash-rotary.yaml
deleted file mode 100755
index f05f705..0000000
--- a/train/configs/experiment/pile/gpt3xl-flash-rotary.yaml
+++ /dev/null
@@ -1,8 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/gpt3xl-flash.yaml
-
-model:
-  config:
-    max_position_embeddings: 0  # Disable absolute position embedding
-    rotary_emb_fraction: 0.5
diff --git a/train/configs/experiment/pile/gpt3xl-flash.yaml b/train/configs/experiment/pile/gpt3xl-flash.yaml
deleted file mode 100755
index 3647d4c..0000000
--- a/train/configs/experiment/pile/gpt3xl-flash.yaml
+++ /dev/null
@@ -1,35 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/gpt3s-flash.yaml
-  - override /optimizer: adamw-zero
-
-model:
-  config:
-    n_embd: 2048
-    n_head: 16
-    n_layer: 24
-
-datamodule:
-  batch_size: ${eval:"1 if ${train.gpu_mem} < 24 else (2 if ${train.gpu_mem} < 40 else (4 if ${train.gpu_mem} < 80 else 8))"}
-
-train:
-  global_batch_size: 512
-  optimizer:
-    lr: 2.0e-4
-  scheduler:
-    t_initial: 300000
-
-trainer:
-  strategy:
-    _target_: train.utils.ddp_zero1.DDPStrategyZero1
-    find_unused_parameters: False
-    gradient_as_bucket_view: True
-  max_steps: 400000
-  val_check_interval: ${eval:1000 * ${.accumulate_grad_batches}}
-
-callbacks:
-  model_checkpoint:
-    every_n_train_steps: 1000
-  model_checkpoint_progress:
-    every_n_train_steps: 12500
-    fault_tolerant: False  # Saving takes too long
diff --git a/train/configs/experiment/pile/gpt3xl-hf.yaml b/train/configs/experiment/pile/gpt3xl-hf.yaml
deleted file mode 100755
index b03dc90..0000000
--- a/train/configs/experiment/pile/gpt3xl-hf.yaml
+++ /dev/null
@@ -1,35 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/gpt3s-hf.yaml
-  - override /optimizer: adamw-zero
-
-model:
-  config:
-    n_embd: 2048
-    n_head: 16
-    n_layer: 24
-
-datamodule:
-  batch_size: 2
-
-train:
-  global_batch_size: 512
-  optimizer:
-    lr: 2.0e-4
-  scheduler:
-    t_initial: 300000
-
-trainer:
-  strategy:
-    _target_: train.utils.ddp_zero1.DDPStrategyZero1
-    find_unused_parameters: False
-    gradient_as_bucket_view: True
-  max_steps: 400000
-  val_check_interval: ${eval:1000 * ${.accumulate_grad_batches}}
-
-callbacks:
-  model_checkpoint:
-    every_n_train_steps: 1000
-  model_checkpoint_progress:
-    every_n_train_steps: 12500
-    fault_tolerant: False  # Saving takes too long
diff --git a/train/configs/experiment/reference/attn-1b.yaml b/train/configs/experiment/reference/attn-1b.yaml
deleted file mode 100644
index 7c056bc..0000000
--- a/train/configs/experiment/reference/attn-1b.yaml
+++ /dev/null
@@ -1,74 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/gpt3m-flash.yaml
-
-train: 
-  optimizer:
-    lr: 0.0008
-    betas: [0.9, 0.95]
-    _target_: apex.optimizers.FusedAdam
-    adam_w_mode: true
-    weight_decay: 0.1
-  
-  scheduler: 
-    lr_min: 0.00008
-    _target_: train.optim.timm_lr_scheduler.TimmCosineLRScheduler
-    warmup_t: 200
-    t_initial: 19800
-    t_in_epochs: false
-    warmup_prefix: true
-    warmup_lr_init: 0.000001
-
-trainer: 
-  # this interval is in terms of batch_idx not in terms of global_step, so we need 
-  # to multiply by accumulate_grad_batches
-  val_check_interval: ${eval:1000 * ${.accumulate_grad_batches}}
-  max_steps: 20000
-
-expt_name: 02-25-attn-1b
-name: ${.expt_name}
-
-resume: true
-
-callbacks:
-  model_checkpoint:
-    dirpath: /var/cr05_data/sabri_data/checkpoints/${expt_name}
-
-datamodule:
-  _target_: train.datamodules.language_modeling_neox.NeoxLMDataModule   
-  batch_size: 8  # per gpu
-  batch_size_eval: 8
-  global_batch_size: ${..train.global_batch_size}
-  max_steps: ${..trainer.max_steps}
-  num_test_samples: 1000
-  num_valid_samples: 1000
-
-model:
-  config:
-    n_embd: 1680
-    n_head: 24
-    n_layer: 36
-    _target_: "transformers.GPT2Config"
-    rms_norm: true
-    fused_mlp: false
-    attn_pdrop: 0
-    embd_pdrop: 0
-    n_positions: 2048
-    resid_pdrop: 0
-    mlp_fc1_bias: false
-    mlp_fc2_bias: false
-    fused_bias_fc: true
-    out_proj_bias: false
-    qkv_proj_bias: false
-    use_flash_attn: true
-    residual_in_fp32: true
-    activation_function: "swiglu" # flag
-    rotary_emb_fraction: 0.5
-    fused_dropout_add_ln: true
-    max_position_embeddings: 0
-    pad_vocab_size_multiple: 16
-    reorder_and_upcast_attn: false
-    scale_attn_by_inverse_layer_idx: false
-
-    # mlp_type: "alt"   # flagging alt MLP (Simran)
-    ff_mult: 4        # flagging alt MLP (Simran)
diff --git a/train/configs/experiment/reference/attn-360m.yaml b/train/configs/experiment/reference/attn-360m.yaml
deleted file mode 100755
index 29e0e86..0000000
--- a/train/configs/experiment/reference/attn-360m.yaml
+++ /dev/null
@@ -1,78 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/gpt3m-flash.yaml
-
-train: 
-  optimizer:
-    lr: 0.0008
-    betas: [0.9, 0.95]
-    _target_: apex.optimizers.FusedAdam
-    adam_w_mode: true
-    weight_decay: 0.1
-  
-  scheduler: 
-    lr_min: 0.00008
-    _target_: train.optim.timm_lr_scheduler.TimmCosineLRScheduler
-    warmup_t: 200
-    t_initial: 19800
-    t_in_epochs: false
-    warmup_prefix: true
-    warmup_lr_init: 0.000001
-
-trainer: 
-  # this interval is in terms of batch_idx not in terms of global_step, so we need 
-  # to multiply by accumulate_grad_batches
-  val_check_interval: ${eval:1000 * ${.accumulate_grad_batches}}
-  max_steps: 20000
-
-
-datamodule:
-  _target_: train.datamodules.language_modeling_neox.NeoxLMDataModule   
-  batch_size: 16  # per gpu
-  batch_size_eval: 32
-  num_predict_batches_eval: 100
-  global_batch_size: ${..train.global_batch_size}
-  max_steps: ${..trainer.max_steps}
-  num_test_samples: 1000
-  num_valid_samples: 1000
-
-
-expt_name: 02-21-attn-360m-redo1
-name: ${.expt_name}
-
-
-callbacks:
-  model_checkpoint:
-    dirpath: /var/cr01_data/sabri_data/checkpoints/${expt_name}
-
-resume: True
-
-model:
-  config:
-    n_embd: 1024
-    n_head: 16
-    n_layer: 24
-    _target_: "transformers.GPT2Config"
-    rms_norm: true
-    fused_mlp: false
-    attn_pdrop: 0
-    embd_pdrop: 0
-    n_positions: 2048
-    resid_pdrop: 0
-    mlp_fc1_bias: false
-    mlp_fc2_bias: false
-    fused_bias_fc: true
-    out_proj_bias: false
-    qkv_proj_bias: false
-    use_flash_attn: true
-    residual_in_fp32: true
-    activation_function: "swiglu" # flag
-    rotary_emb_fraction: 0.5
-    fused_dropout_add_ln: true
-    max_position_embeddings: 0
-    pad_vocab_size_multiple: 16
-    reorder_and_upcast_attn: false
-    scale_attn_by_inverse_layer_idx: false
-
-    # mlp_type: "alt"   # flagging alt MLP (Simran)
-    ff_mult: 4        # flagging alt MLP (Simran)
diff --git a/train/configs/experiment/reference/based-1b.yaml b/train/configs/experiment/reference/based-1b.yaml
deleted file mode 100644
index 603796a..0000000
--- a/train/configs/experiment/reference/based-1b.yaml
+++ /dev/null
@@ -1,123 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/gpt3m-flash.yaml
-
-train: 
-  optimizer:
-    lr: 0.0008
-    betas: [0.9, 0.95]
-    _target_: apex.optimizers.FusedAdam
-    adam_w_mode: true
-    weight_decay: 0.1
-  
-  scheduler: 
-    lr_min: 0.00008
-    _target_: train.optim.timm_lr_scheduler.TimmCosineLRScheduler
-    warmup_t: 200
-    t_initial: 19800
-    t_in_epochs: false
-    warmup_prefix: true
-    warmup_lr_init: 0.000001
-
-trainer: 
-  # this interval is in terms of batch_idx not in terms of global_step, so we need 
-  # to multiply by accumulate_grad_batches
-  val_check_interval: ${eval:1000 * ${.accumulate_grad_batches}}
-  max_steps: 20000
-
-
-datamodule:
-  _target_: train.datamodules.language_modeling_neox.NeoxLMDataModule   
-  batch_size: 4  # per gpu
-  batch_size_eval: 4
-  num_predict_batches_eval: 100
-  global_batch_size: ${..train.global_batch_size}
-  max_steps: ${..trainer.max_steps}
-  num_test_samples: 1000
-  num_valid_samples: 1000
-
-expt_name: 02-24-based-1b
-name: ${.expt_name}
-
-callbacks:
-  model_checkpoint:
-    dirpath: /var/cr05_data/sabri_data/checkpoints/${expt_name}
-
-
-resume: True
-do_test: True
-
-model:
-  _target_:  based.models.gpt.GPTLMHeadModel
-  _recursive_: false
-  config:
-    alt_mixer_layers: 
-      - 1
-      - 6
-      - 11
-      - 16
-      - 21
-      - 27
-      - 33
-
-    alt_mixer_2_layers:
-      - 2
-      - 7
-      - 12
-      - 17
-      - 22
-      - 28
-      - 34
-
-    mixer:
-      _target_: based.models.mixers.convolution.BaseConv
-      l_max: ${....datamodule.max_length}
-      use_bias: True
-      expand_proj: 4
-      kernel_sizes: 3
-
-    alt_mixer: 
-      _target_: based.models.mixers.linear_attention.LinearAttention  
-      l_max: ${....datamodule.max_length}
-      feature_map: 
-        _target_: based.models.mixers.linear_attention.TaylorExp
-        input_dim: ${..feature_dim}
-      feature_dim: 16
-      num_heads: 16
-
-    alt_mixer_2:
-      _target_: based.models.mixers.slide_attention.SlidingAttention
-      window_size: 256
-      num_heads: 16
-      causal: true
-
-    n_embd: 1792
-    fixed_decay: True      # flag 
-    decay_const: -3
-    decay_denom: False
-    special_initializer: true
-    n_head: 16
-    n_layer: 36
-    _target_: based.models.gpt.GPT2MixerConfig
-    rms_norm: true
-    fused_mlp: false
-    attn_pdrop: 0
-    embd_pdrop: 0
-    n_positions: 2048
-    resid_pdrop: 0
-    mlp_fc1_bias: false
-    mlp_fc2_bias: false
-    fused_bias_fc: true
-    out_proj_bias: false
-    qkv_proj_bias: false
-    use_flash_attn: true
-    residual_in_fp32: true
-    activation_function: "swiglu"    
-    rotary_emb_fraction: 1           # flagging 
-    fused_dropout_add_ln: true
-    max_position_embeddings: 0   # flagging 
-    pad_vocab_size_multiple: 8
-    reorder_and_upcast_attn: false
-    scale_attn_by_inverse_layer_idx: false
-
-    n_inner: ${eval:2 * ${.n_embd}}
diff --git a/train/configs/experiment/reference/based-360m.yaml b/train/configs/experiment/reference/based-360m.yaml
deleted file mode 100644
index df879d4..0000000
--- a/train/configs/experiment/reference/based-360m.yaml
+++ /dev/null
@@ -1,118 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/gpt3m-flash.yaml
-
-train: 
-  optimizer:
-    lr: 0.0008
-    betas: [0.9, 0.95]
-    _target_:  apex.optimizers.FusedAdam
-    adam_w_mode: true
-    weight_decay: 0.1
-  
-  scheduler: 
-    lr_min: 0.00008
-    _target_: train.optim.timm_lr_scheduler.TimmCosineLRScheduler
-    warmup_t: 200
-    t_initial: 19800
-    t_in_epochs: false
-    warmup_prefix: true
-    warmup_lr_init: 0.000001
-
-trainer: 
-  # this interval is in terms of batch_idx not in terms of global_step, so we need 
-  # to multiply by accumulate_grad_batches
-  val_check_interval: ${eval:1000 * ${.accumulate_grad_batches}}
-  max_steps: 20000
-
-
-datamodule:
-  _target_: train.datamodules.language_modeling_neox.NeoxLMDataModule   
-  batch_size: 8  # per gpu
-  batch_size_eval: 32
-  num_predict_batches_eval: 100
-  global_batch_size: ${..train.global_batch_size}
-  max_steps: ${..trainer.max_steps}
-  num_test_samples: 1000
-  num_valid_samples: 1000
-
-expt_name: 02-20-based-360m
-name: ${.expt_name}
-
-callbacks:
-  model_checkpoint:
-    dirpath: /var/cr01_data/sabri_data/checkpoints/${expt_name}
-
-resume: False
-do_test: True
-
-model:
-  _target_: based.models.gpt.GPTLMHeadModel
-  _recursive_: false
-  config:
-    alt_mixer_layers: 
-      - 1
-      - 6
-      - 11
-      - 16
-      - 21
-
-    alt_mixer_2_layers:
-      - 2
-      - 7
-      - 12
-      - 17
-      - 22
-
-    mixer:
-      _target_: based.models.mixers.convolution.BaseConv
-      l_max: ${....datamodule.max_length}
-      use_bias: True
-      expand_proj: 4
-      kernel_sizes: 3
-    
-    alt_mixer: 
-      _target_: based.models.mixers.linear_attention.LinearAttention  
-      l_max: ${....datamodule.max_length}
-      feature_map: 
-        _target_: based.models.mixers.linear_attention.TaylorExp
-        input_dim: ${..feature_dim}
-      feature_dim: 16
-      num_heads: 16
-      parallel_implementation: "fla_parallel"
-
-    alt_mixer_2:
-      _target_: based.models.mixers.slide_attention.SlidingAttention
-      window_size: 256 
-      num_heads: 16
-      causal: true
-
-    n_embd: 1024
-
-    special_initializer: true
-    n_head: 16
-    n_layer: 27
-    _target_: based.models.gpt.GPT2MixerConfig
-    rms_norm: true
-    fused_mlp: false
-    attn_pdrop: 0
-    embd_pdrop: 0
-    n_positions: 2048
-    resid_pdrop: 0
-    mlp_fc1_bias: false
-    mlp_fc2_bias: false
-    fused_bias_fc: true
-    out_proj_bias: false
-    qkv_proj_bias: false
-    use_flash_attn: true
-    residual_in_fp32: true
-    activation_function: "swiglu"    
-    rotary_emb_fraction: 1           # flagging 
-    fused_dropout_add_ln: true
-    max_position_embeddings: 0   # flagging 
-    pad_vocab_size_multiple: 8
-    reorder_and_upcast_attn: false
-    scale_attn_by_inverse_layer_idx: false
-
-    n_inner: ${eval:2 * ${.n_embd}}
-
diff --git a/train/configs/experiment/reference/based_1.3b_50b_tok.yaml b/train/configs/experiment/reference/based_1.3b_50b_tok.yaml
deleted file mode 100644
index 5d4c6e7..0000000
--- a/train/configs/experiment/reference/based_1.3b_50b_tok.yaml
+++ /dev/null
@@ -1,123 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/gpt3m-flash.yaml
-
-train: 
-  optimizer:
-    lr: 0.0008
-    betas: [0.9, 0.95]
-    _target_: apex.optimizers.FusedAdam
-    adam_w_mode: true
-    weight_decay: 0.1
-  
-  scheduler: 
-    lr_min: 0.00008
-    _target_: train.optim.timm_lr_scheduler.TimmCosineLRScheduler
-    warmup_t: 1000
-    t_initial: 99000
-    t_in_epochs: false
-    warmup_prefix: true
-    warmup_lr_init: 0.000001
-
-trainer: 
-  # this interval is in terms of batch_idx not in terms of global_step, so we need 
-  # to multiply by accumulate_grad_batches
-  val_check_interval: ${eval:1000 * ${.accumulate_grad_batches}}
-  max_steps: 100000
-
-
-datamodule:
-  _target_: train.datamodules.language_modeling_neox.NeoxLMDataModule   
-  batch_size: 8  # per gpu
-  batch_size_eval: 8
-  num_predict_batches_eval: 100
-  global_batch_size: ${..train.global_batch_size}
-  max_steps: ${..trainer.max_steps}
-  num_test_samples: 1000
-  num_valid_samples: 1000
-  
-
-expt_name: based-1b-50b-tokens
-name: ${.expt_name}
-
-callbacks:
-  model_checkpoint:
-    dirpath: /var/cr01_data/sim_data/checkpoints/${expt_name}
-
-resume: True
-do_test: True
-
-model:
-  _target_:  based.models.gpt.GPTLMHeadModel
-  _recursive_: false
-  config:
-    alt_mixer_layers: 
-      - 1
-      - 6
-      - 11
-      - 16
-      - 21
-      - 27
-      - 33
-
-    alt_mixer_2_layers:
-      - 2
-      - 7
-      - 12
-      - 17
-      - 22
-      - 28
-      - 34
-
-    mixer:
-      _target_: based.models.mixers.convolution.BaseConv
-      l_max: ${....datamodule.max_length}
-      use_bias: True
-      expand_proj: 4
-      kernel_sizes: 3
-
-    alt_mixer: 
-      _target_: based.models.mixers.linear_attention.LinearAttention  
-      l_max: ${....datamodule.max_length}
-      feature_map: 
-        _target_: based.models.mixers.linear_attention.TaylorExp
-        input_dim: ${..feature_dim}
-      feature_dim: 16
-      num_heads: 16
-      parallel_implementation: fla_parallel
-
-    alt_mixer_2:
-      _target_: based.models.mixers.slide_attention.SlidingAttention
-      window_size: 256
-      num_heads: 16
-      causal: true
-
-    n_embd: 1792
-    fixed_decay: False   
-    decay_const: -3
-    decay_denom: False
-    special_initializer: true
-    n_head: 16
-    n_layer: 36
-    _target_: based.models.gpt.GPT2MixerConfig
-    rms_norm: true
-    fused_mlp: false
-    attn_pdrop: 0
-    embd_pdrop: 0
-    n_positions: 2048
-    resid_pdrop: 0
-    mlp_fc1_bias: false
-    mlp_fc2_bias: false
-    fused_bias_fc: true
-    out_proj_bias: false
-    qkv_proj_bias: false
-    use_flash_attn: true
-    residual_in_fp32: true
-    activation_function: "swiglu"    
-    rotary_emb_fraction: 1           # flagging 
-    fused_dropout_add_ln: true
-    max_position_embeddings: 0   # flagging 
-    pad_vocab_size_multiple: 8
-    reorder_and_upcast_attn: false
-    scale_attn_by_inverse_layer_idx: false
-    n_inner: ${eval:2 * ${.n_embd}}
diff --git a/train/configs/experiment/reference/mamba-1.3b_50b_tok.yaml b/train/configs/experiment/reference/mamba-1.3b_50b_tok.yaml
deleted file mode 100644
index 1edddd2..0000000
--- a/train/configs/experiment/reference/mamba-1.3b_50b_tok.yaml
+++ /dev/null
@@ -1,53 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/gpt3m-flash.yaml
-  - override /model: mamba
-
-train: 
-  optimizer:
-    lr: 0.0008
-    betas: [0.9, 0.95]
-    _target_: apex.optimizers.FusedAdam
-    adam_w_mode: true
-    weight_decay: 0.1
-  
-  scheduler: 
-    lr_min: 0.00008
-    _target_: train.optim.timm_lr_scheduler.TimmCosineLRScheduler
-    warmup_t: 1000
-    t_initial: 99000
-    t_in_epochs: false
-    warmup_prefix: true
-    warmup_lr_init: 0.000001
-
-expt_name: mamba-1b-50b-tokens
-name: ${.expt_name}
-
-callbacks:
-  model_checkpoint:
-    dirpath: /var/cr05_data/sim_data/checkpoints/${expt_name}
-
-
-trainer: 
-  # this interval is in terms of batch_idx not in terms of global_step, so we need 
-  # to multiply by accumulate_grad_batches
-  val_check_interval: ${eval:1000 * ${.accumulate_grad_batches}}
-  max_steps: 100000
-
-datamodule:
-  _target_: train.datamodules.language_modeling_neox.NeoxLMDataModule   
-  batch_size: 8  # per gpu
-  batch_size_eval: 8
-  num_predict_batches_eval: 100
-  global_batch_size: ${..train.global_batch_size}
-  max_steps: ${..trainer.max_steps}
-  num_test_samples: 1000
-  num_valid_samples: 1000
-
-model:
-  config:
-    d_model: 2048
-    n_layer: 46
-    rms_norm: true
-    residual_in_fp32: true
-    pad_vocab_size_multiple: 16
diff --git a/train/configs/experiment/reference/mamba-1b.yaml b/train/configs/experiment/reference/mamba-1b.yaml
deleted file mode 100644
index ef303f7..0000000
--- a/train/configs/experiment/reference/mamba-1b.yaml
+++ /dev/null
@@ -1,53 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/gpt3m-flash.yaml
-  - override /model: mamba
-
-train: 
-  optimizer:
-    lr: 0.0008
-    betas: [0.9, 0.95]
-    _target_: apex.optimizers.FusedAdam
-    adam_w_mode: true
-    weight_decay: 0.1
-  
-  scheduler: 
-    lr_min: 0.00008
-    _target_: train.optim.timm_lr_scheduler.TimmCosineLRScheduler
-    warmup_t: 200
-    t_initial: 19800
-    t_in_epochs: false
-    warmup_prefix: true
-    warmup_lr_init: 0.000001
-
-expt_name: 02-22-mamba-1b
-name: ${.expt_name}
-
-callbacks:
-  model_checkpoint:
-    dirpath: /var/cr05_data/sabri_data/checkpoints/${expt_name}
-
-
-trainer: 
-  # this interval is in terms of batch_idx not in terms of global_step, so we need 
-  # to multiply by accumulate_grad_batches
-  val_check_interval: ${eval:1000 * ${.accumulate_grad_batches}}
-  max_steps: 20000
-
-
-datamodule:
-  _target_: train.datamodules.language_modeling_neox.NeoxLMDataModule   
-  batch_size: 8  # per gpu
-  batch_size_eval: 8
-  global_batch_size: ${..train.global_batch_size}
-  max_steps: ${..trainer.max_steps}
-  num_test_samples: 1000
-  num_valid_samples: 1000
-
-model:
-  config:
-    d_model: 2048
-    n_layer: 46
-    rms_norm: true
-    residual_in_fp32: true
-    pad_vocab_size_multiple: 16
diff --git a/train/configs/experiment/reference/mamba-360m.yaml b/train/configs/experiment/reference/mamba-360m.yaml
deleted file mode 100755
index 9301832..0000000
--- a/train/configs/experiment/reference/mamba-360m.yaml
+++ /dev/null
@@ -1,54 +0,0 @@
-# @package _global_
-defaults:
-  - /experiment/pile/gpt3m-flash.yaml
-  - override /model: mamba
-
-train: 
-  optimizer:
-    lr: 0.0008
-    betas: [0.9, 0.95]
-    _target_: apex.optimizers.FusedAdam
-    adam_w_mode: true
-    weight_decay: 0.1
-  
-  scheduler: 
-    lr_min: 0.00008
-    _target_: train.optim.timm_lr_scheduler.TimmCosineLRScheduler
-    warmup_t: 200
-    t_initial: 19800
-    t_in_epochs: false
-    warmup_prefix: true
-    warmup_lr_init: 0.000001
-
-trainer: 
-  # this interval is in terms of batch_idx not in terms of global_step, so we need 
-  # to multiply by accumulate_grad_batches
-  val_check_interval: ${eval:1000 * ${.accumulate_grad_batches}}
-  max_steps: 20000
-
-
-datamodule:
-  _target_: train.datamodules.language_modeling_neox.NeoxLMDataModule   
-  batch_size: 8  # per gpu
-  batch_size_eval: 32
-  num_predict_batches_eval: 100
-  global_batch_size: ${..train.global_batch_size}
-  max_steps: ${..trainer.max_steps}
-  num_test_samples: 1000
-  num_valid_samples: 1000
-
-expt_name: 02-21-mamba-360m
-name: ${.expt_name}
-
-callbacks:
-  model_checkpoint:
-    dirpath: /var/cr01_data/sabri_data/checkpoints/${expt_name}
-
-resume: True
-
-model:
-  config:
-    n_layer: 46
-    rms_norm: true
-    residual_in_fp32: true
-    pad_vocab_size_multiple: 16
diff --git a/train/csrc/causal_dot_prod/__init__.py b/train/csrc/causal_dot_prod/__init__.py
deleted file mode 100755
index 09d47f2..0000000
--- a/train/csrc/causal_dot_prod/__init__.py
+++ /dev/null
@@ -1,98 +0,0 @@
-#
-# Copyright (c) 2020 Idiap Research Institute, http://www.idiap.ch/
-# Written by Angelos Katharopoulos <angelos.katharopoulos@idiap.ch>,
-# Apoorv Vyas <avyas@idiap.ch>
-#
-from .causal_attention import causal_dot_product
-# from .causal_attention_bf16 import causal_dot_product as causal_dot_product_bf16
-
-
-# import torch
-
-# # from .causal_product_cpu import causal_dot_product as causal_dot_product_cpu, \
-# #     causal_dot_backward as causal_dot_backward_cpu
-
-# # from causal_product_cpu import causal_dot_product as causal_dot_product_cpu, \
-# #     causal_dot_backward as causal_dot_backward_cpu
-
-# # try:
-# #     from .causal_product_cuda import \
-# #         causal_dot_product as causal_dot_product_cuda, \
-# #         causal_dot_backward as causal_dot_backward_cuda
-# # except ImportError:
-# #     causal_dot_product_cuda = causal_dot_backward_cuda = None
-
-
-# # from .causal_attention import causal_dot_product as causal_dot_product_cpu, \
-# #     causal_dot_backward as causal_dot_backward_cpu
-
-# try:
-#     from causal_attention_cuda import \
-#         causal_dot_product as causal_dot_product_cuda, \
-#         causal_dot_backward as causal_dot_backward_cuda
-# except ImportError as e:
-#     print(e)
-#     causal_dot_product_cuda = causal_dot_backward_cuda = None
-
-
-# class CausalDotProduct(torch.autograd.Function):
-#     """Compute the weighted sum of values but attending only to previous
-#     values."""
-#     dot = {
-#         # "cpu": causal_dot_product_cpu,
-#         "cuda": causal_dot_product_cuda
-#     }
-#     dot_backward = {
-#         # "cpu": causal_dot_backward_cpu,
-#         "cuda": causal_dot_backward_cuda
-#     }
-
-#     @staticmethod
-#     def forward(ctx, Q, K, V):
-#         # Save the inputs for the gradient computation
-#         ctx.save_for_backward(Q, K, V)
-
-#         # Create the output tensor
-#         device = Q.device
-#         N, H, L, _ = Q.shape
-#         _, _, _, M = V.shape
-#         product = torch.zeros((N, H, L, M), dtype=Q.dtype, device=device)
-
-#         # Actually perform the dot product
-#         CausalDotProduct.dot[device.type](
-#             Q.data,
-#             K.data,
-#             V.data,
-#             product
-#         )
-#         # breakpoint()
-#         # CausalDotProduct.dot[device.type](Q.data, K.data, V.data, product)
-
-#         return product
-
-#     @staticmethod
-#     def backward(ctx, grad_out):
-#         # Extract the saved tensors
-#         Q, K, V = ctx.saved_tensors
-
-#         # Allocate memory for the gradients
-#         grad_Q = torch.zeros_like(Q)
-#         grad_K = torch.zeros_like(K)
-#         grad_V = torch.zeros_like(V)
-
-#         # Actually compute the gradients
-#         CausalDotProduct.dot_backward[Q.device.type](
-#             Q.data,
-#             K.data,
-#             V.data,
-#             grad_out,
-#             grad_Q,
-#             grad_K,
-#             grad_V
-#         )
-
-#         return grad_Q, grad_K, grad_V
-
-
-# # Alias the autograd functions to python style snake case naming
-# causal_dot_product = CausalDotProduct.apply
\ No newline at end of file
diff --git a/train/csrc/causal_dot_prod/causal_attention.cpp b/train/csrc/causal_dot_prod/causal_attention.cpp
deleted file mode 100755
index 4fa7757..0000000
--- a/train/csrc/causal_dot_prod/causal_attention.cpp
+++ /dev/null
@@ -1,225 +0,0 @@
-//
-// Copyright (c) 2020 Idiap Research Institute, http://www.idiap.ch/
-// Written by Angelos Katharopoulos <angelos.katharopoulos@idiap.ch>,
-// Apoorv Vyas <avyas@idiap.ch>
-//
-
-#include <torch/extension.h>
-
-
-/**
- * Compute a*b^T and save it into out.
- *
- * a \in R^A
- * b \in R^B
- */
-inline void vvt_dot(float *a, float *b, float *out, int A, int B) {
-    for (int i=0; i<A; i++) {
-        float * bi = b;
-        for (int j=0; j<B; j++) {
-            *out += (*a) * (*bi);
-            out++;
-            bi++;
-        }
-        a++;
-    }
-}
-
-
-/**
- * Implement a vector matrix product v*m and save it into out.
- *
- * v \in R^A
- * m \in R^{AxB}
- */
-inline void vm_dot(float *v, float *m, float *out, int A, int B) {
-    // TODO: Consider removing the zeroing part and assuming out already
-    //       contains 0s
-    for (int i=0; i<B; i++) {
-        out[i] = 0;
-    }
-
-    for (int i=0; i<A; i++) {
-        float *oi = out;
-        for (int j=0; j<B; j++) {
-            *oi += (*v) * (*m);
-            oi++;
-            m++;
-        }
-        v++;
-    }
-}
-
-
-/**
- * Implement a vector transposed-matrix product and save it into out.
- *
- * v \in R^B
- * m \in R^{AxB}
- */
-inline void vmt_dot(float *v, float *m, float *out, int A, int B) {
-    for (int i=0; i<A; i++) {
-        float *vi = v;
-        float s = 0;
-        for (int j=0; j<B; j++) {
-            s += (*vi) * (*m);
-            vi++;
-            m++;
-        }
-        // TODO: Should we be aggregating? See the comment on vm_dot.
-        *out = s;
-        out++;
-    }
-}
-
-
-/**
- * Compute the causally masked dot products of queries, keys and values.
- *
- * Basically compute V_j' = (Q_{0:j} * K_{0:j}^T) * V_{0:j} for all j. The
- * computation is done efficiently by changing the order of the dot products.
- */
-void causal_dot_product(
-    const torch::Tensor queries,
-    const torch::Tensor keys,
-    const torch::Tensor values,
-    torch::Tensor product
-) {
-    // Extract some shapes
-    int N = queries.size(0);
-    int H = queries.size(1);
-    int L = queries.size(2);
-    int E = queries.size(3);
-    int M = values.size(3);
-
-    // Create accessors for all the arguments
-    auto qa = queries.accessor<float, 4>();
-    auto ka = keys.accessor<float, 4>();
-    auto va = values.accessor<float, 4>();
-    auto pa = product.accessor<float, 4>();
-
-    #pragma omp parallel for collapse(2)
-    for (int n=0; n<N; n++) {
-        for (int h=0; h<H; h++) {
-            auto kv = torch::zeros({E, M}, queries.options());
-            float *kvp = kv.data_ptr<float>();
-            for (int l=0; l<L; l++) {
-                vvt_dot(
-                    &ka[n][h][l][0],
-                    &va[n][h][l][0],
-                    kvp,
-                    E,
-                    M
-                );
-                vm_dot(
-                    &qa[n][h][l][0],
-                    kvp,
-                    &pa[n][h][l][0],
-                    E,
-                    M
-                );
-            }
-        }
-    }
-}
-
-
-/**
- * Compute the gradients of queries, keys and values given the gradient of the
- * causal_dot_product output.
- *
- * Make sure that everything is computed in O(N D^2) complexity.
- */
-void causal_dot_backward(
-    const torch::Tensor queries,
-    const torch::Tensor keys,
-    const torch::Tensor values,
-    const torch::Tensor grad_out,
-    torch::Tensor grad_queries,
-    torch::Tensor grad_keys,
-    torch::Tensor grad_values
-) {
-    // Extract some shapes
-    int N = queries.size(0);
-    int H = queries.size(1);
-    int L = queries.size(2);
-    int E = queries.size(3);
-    int M = values.size(3);
-
-    // Create accessors for all the arguments
-    auto qa = queries.accessor<float, 4>();
-    auto ka = keys.accessor<float, 4>();
-    auto va = values.accessor<float, 4>();
-    auto ga = grad_out.accessor<float, 4>();
-    auto gqa = grad_queries.accessor<float, 4>();
-    auto gka = grad_keys.accessor<float, 4>();
-    auto gva = grad_values.accessor<float, 4>();
-
-    #pragma omp parallel for collapse(2)
-    for (int n=0; n<N; n++) {
-        for (int h=0; h<H; h++) {
-            auto kv = torch::zeros({E, M}, queries.options());
-            float *kvp = kv.data_ptr<float>();
-
-            // Compute the gradient wrt the queries
-            for (int l=0; l<L; l++) {
-                vvt_dot(
-                    &ka[n][h][l][0],
-                    &va[n][h][l][0],
-                    kvp,
-                    E,
-                    M
-                );
-                vmt_dot(
-                    &ga[n][h][l][0],
-                    kvp,
-                    &gqa[n][h][l][0],
-                    E,
-                    M
-                );
-            }
-
-            // Compute the gradient wrt the keys and values
-            kv.zero_();
-            for (int l=L-1; l>=0; l--) {
-                vvt_dot(
-                    &qa[n][h][l][0],
-                    &ga[n][h][l][0],
-                    kvp,
-                    E,
-                    M
-                );
-                vmt_dot(
-                    &va[n][h][l][0],
-                    kvp,
-                    &gka[n][h][l][0],
-                    E,
-                    M
-                );
-                vm_dot(
-                    &ka[n][h][l][0],
-                    kvp,
-                    &gva[n][h][l][0],
-                    E,
-                    M
-                );
-            }
-        }
-    }
-}
-
-
-PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
-    m.def(
-        "causal_dot_product",
-        &causal_dot_product,
-        "Compute the weighted sum of values but attending only to previous "
-        "values."
-    );
-    m.def(
-        "causal_dot_backward",
-        &causal_dot_backward,
-        "Compute the gradient of queries, keys and values given the gradient "
-        "of causal_dot_product."
-    );
-}
\ No newline at end of file
diff --git a/train/csrc/causal_dot_prod/causal_attention.py b/train/csrc/causal_dot_prod/causal_attention.py
deleted file mode 100755
index 92a4b71..0000000
--- a/train/csrc/causal_dot_prod/causal_attention.py
+++ /dev/null
@@ -1,95 +0,0 @@
-#
-# Copyright (c) 2020 Idiap Research Institute, http://www.idiap.ch/
-# Written by Angelos Katharopoulos <angelos.katharopoulos@idiap.ch>,
-# Apoorv Vyas <avyas@idiap.ch>
-#
-
-import torch
-
-# from .causal_product_cpu import causal_dot_product as causal_dot_product_cpu, \
-#     causal_dot_backward as causal_dot_backward_cpu
-
-# from causal_product_cpu import causal_dot_product as causal_dot_product_cpu, \
-#     causal_dot_backward as causal_dot_backward_cpu
-
-# try:
-#     from .causal_product_cuda import \
-#         causal_dot_product as causal_dot_product_cuda, \
-#         causal_dot_backward as causal_dot_backward_cuda
-# except ImportError:
-#     causal_dot_product_cuda = causal_dot_backward_cuda = None
-
-
-# from .causal_attention import causal_dot_product as causal_dot_product_cpu, \
-#     causal_dot_backward as causal_dot_backward_cpu
-
-try:
-    from causal_attention_cuda import \
-        causal_dot_product as causal_dot_product_cuda, \
-        causal_dot_backward as causal_dot_backward_cuda
-except ImportError as e:
-    print(e)
-    causal_dot_product_cuda = causal_dot_backward_cuda = None
-
-
-class CausalDotProduct(torch.autograd.Function):
-    """Compute the weighted sum of values but attending only to previous
-    values."""
-    dot = {
-        # "cpu": causal_dot_product_cpu,
-        "cuda": causal_dot_product_cuda
-    }
-    dot_backward = {
-        # "cpu": causal_dot_backward_cpu,
-        "cuda": causal_dot_backward_cuda
-    }
-
-    @staticmethod
-    def forward(ctx, Q, K, V):
-        # Save the inputs for the gradient computation
-        ctx.save_for_backward(Q, K, V)
-
-        # Create the output tensor
-        device = Q.device
-        N, H, L, _ = Q.shape
-        _, _, _, M = V.shape
-        product = torch.zeros((N, H, L, M), dtype=Q.dtype, device=device)
-
-        # Actually perform the dot product
-        CausalDotProduct.dot[device.type](
-            Q.data,
-            K.data,
-            V.data,
-            product
-        )
-        # breakpoint()
-        # CausalDotProduct.dot[device.type](Q.data, K.data, V.data, product)
-
-        return product
-
-    @staticmethod
-    def backward(ctx, grad_out):
-        # Extract the saved tensors
-        Q, K, V = ctx.saved_tensors
-
-        # Allocate memory for the gradients
-        grad_Q = torch.zeros_like(Q)
-        grad_K = torch.zeros_like(K)
-        grad_V = torch.zeros_like(V)
-
-        # Actually compute the gradients
-        CausalDotProduct.dot_backward[Q.device.type](
-            Q.data,
-            K.data,
-            V.data,
-            grad_out,
-            grad_Q,
-            grad_K,
-            grad_V
-        )
-
-        return grad_Q, grad_K, grad_V
-
-
-# Alias the autograd functions to python style snake case naming
-causal_dot_product = CausalDotProduct.apply
\ No newline at end of file
diff --git a/train/csrc/causal_dot_prod/causal_attention_cuda.cu b/train/csrc/causal_dot_prod/causal_attention_cuda.cu
deleted file mode 100755
index e2970e1..0000000
--- a/train/csrc/causal_dot_prod/causal_attention_cuda.cu
+++ /dev/null
@@ -1,1483 +0,0 @@
-//
-// Copyright (c) 2020 Idiap Research Institute, http://www.idiap.ch/
-// Written by Angelos Katharopoulos <angelos.katharopoulos@idiap.ch>,
-// Apoorv Vyas <avyas@idiap.ch>
-//
-
-//
-// For modifications made inside namespace nvidia (authored by jdemouth):
-//
-// Copyright (c) 2021 NVIDIA CORPORATION. All rights reserved.
-// 
-// Permission is hereby granted, free of charge, to any person obtaining a copy of
-// this software and associated documentation files (the "Software"), to deal in
-// the Software without restriction, including without limitation the rights to
-// use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of
-// the Software, and to permit persons to whom the Software is furnished to do so,
-// subject to the following conditions:
-// 
-// The above copyright notice and this permission notice shall be included in all
-// copies or substantial portions of the Software.
-// 
-// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS
-// FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR
-// COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER
-// IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
-// CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
-//
-
-#include <torch/extension.h>
-#include <assert.h>
-#include <stdio.h>
-
-#define ENABLE_NVIDIA_OPTIMIZATIONS
-
-#ifdef ENABLE_NVIDIA_OPTIMIZATIONS
-namespace nvidia {
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-constexpr int THREADS_PER_WARP = 32;
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-constexpr int LOW_OCCUPANCY_THRESHOLD = 40; // TODO: Make it HW specific (like 1/2 SMs).
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-static inline __device__ __host__ int div_up(int m, int n) {
-  return (m + n-1) / n;
-}
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-static inline __device__ __host__ int round_up(int m, int n) {
-  return div_up(m, n) * n;
-}
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-template< typename T >
-struct Lmha_params {
-
-  // The output buffer. Dimensions [B, H, L, M].
-  T *out;
-
-  // The input Qs. Dimensions [B, H, L, E].
-  const T *q;
-  // The input Ks. Dimensions [B, H, L, E].
-  const T *k;
-  // The input Vs. Dimensions [B, H, L, M].
-  const T *v;
-
-  // The different dimensions.
-  int B, L, H, E, M;
-
-  // The strides for the different tensors.
-  int q_stride_B, q_stride_H, q_stride_L;
-  int k_stride_B, k_stride_H, k_stride_L;
-  int v_stride_B, v_stride_H, v_stride_L;
-  int o_stride_B, o_stride_H, o_stride_L;
-};
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-template< int E, bool GO_BACKWARD, int WARPS, int COLS_PER_THREAD = 4 >
-__global__ __launch_bounds__(WARPS * THREADS_PER_WARP)
-void lmha_low_occupancy_kernel(Lmha_params<float> params) {
-
-  // The number of threads per block.
-  constexpr int THREADS_PER_BLOCK = WARPS * THREADS_PER_WARP;
-  // The number of rows per thread.
-  constexpr int ROWS_PER_THREAD = E / THREADS_PER_WARP;
-  // The number of steps per iteration.
-  constexpr int COLS_PER_ITER = WARPS * COLS_PER_THREAD;
-
-  // Make sure E is a multiple of the warp size.
-  static_assert(E % THREADS_PER_WARP == 0, "");
-
-  // Shared memory to store V/O.
-  __shared__ float smem_v[COLS_PER_ITER], smem_o[COLS_PER_ITER];
-  // Shared memory buffer to performance the reductions.
-  __shared__ float smem_reds[E * WARPS]; 
-
-  // The sequence processed by that block.
-  const int bi = blockIdx.z;
-  // The head processed by that block.
-  const int hi = blockIdx.y;
-  // The hidden cell in the V/output buffers.
-  const int vi = blockIdx.x;
-
-  // The linear index of the thread.
-  const int tidx = threadIdx.x;
-
-  // Decompose the block in warp/lane.
-  const int warp = tidx / THREADS_PER_WARP;
-  const int lane = tidx % THREADS_PER_WARP;
-
-  // The base offset loaded by the thread in Q and K.
-  int offset_q = bi*params.q_stride_B + hi*params.q_stride_H + lane;
-  int offset_k = bi*params.k_stride_B + hi*params.k_stride_H + lane;
-
-  // If we walk backward, account for the extra offset.
-  if( GO_BACKWARD ) {
-    offset_q += (params.L-1)*params.q_stride_L;
-    offset_k += (params.L-1)*params.k_stride_L;
-  }
-
-  // Position the warp at the beginning of the proper timestep.
-  if( GO_BACKWARD ) {
-    offset_q -= warp*COLS_PER_THREAD*params.q_stride_L;
-    offset_k -= warp*COLS_PER_THREAD*params.k_stride_L;
-  } else {
-    offset_q += warp*COLS_PER_THREAD*params.q_stride_L;
-    offset_k += warp*COLS_PER_THREAD*params.k_stride_L;
-  }
-
-  // Determine the base pointers for Q and K.
-  const float *ptr_q = &params.q[offset_q];
-  const float *ptr_k = &params.k[offset_k];
-
-  // Is a given row valid?
-  int valid_qk[ROWS_PER_THREAD];
-  #pragma unroll
-  for( int ii = 0; ii < ROWS_PER_THREAD; ++ii ) {
-    valid_qk[ii] = lane + ii*THREADS_PER_WARP < params.E;
-  }
-
-  // The offset to the position loaded by the thread in V.
-  int offset_v = bi*params.v_stride_B + hi*params.v_stride_H + vi;
-  int offset_o = bi*params.o_stride_B + hi*params.o_stride_H + vi;
-
-  // If we walk backward, account for the extra offset.
-  if( GO_BACKWARD ) {
-    offset_v += (params.L-1)*params.v_stride_L;
-    offset_o += (params.L-1)*params.o_stride_L;
-  }
-
-  // We load/store a strided matrix of COLS_PER_ITER x OUTPUTS_PER_BLOCK.
-  if( GO_BACKWARD ) {
-    offset_v -= tidx*params.v_stride_L;
-    offset_o -= tidx*params.o_stride_L;
-  } else {
-    offset_v += tidx*params.v_stride_L;
-    offset_o += tidx*params.o_stride_L;
-  }
-
-  // Determine the base pointer for V.
-  const float *ptr_v = &params.v[offset_v];
-  // The output pointer. 
-  float *ptr_o = &params.out[offset_o];
-
-  // The running KVs.
-  float running_kv[ROWS_PER_THREAD];
-  #pragma unroll
-  for( int ri = 0; ri < ROWS_PER_THREAD; ++ri ) {
-    running_kv[ri] = 0.f;
-  }
-
-  // Iterate over the timesteps. TODO: Use params.loop_count!!!
-  for( int iter = 0; iter < params.L; iter += COLS_PER_ITER ) {
-
-    // Each thread loads a matrix of elements.
-    float q[ROWS_PER_THREAD][COLS_PER_THREAD], k[ROWS_PER_THREAD][COLS_PER_THREAD];
-
-    // Trigger the memory loads for Q and K.
-    #pragma unroll
-    for( int ci = 0; ci < COLS_PER_THREAD; ++ci ) {
-      #pragma unroll
-      for( int ri = 0; ri < ROWS_PER_THREAD; ++ri ) {
-
-        // For Q/K, each warp loads from various timesteps. 
-        int ti = iter + warp*COLS_PER_THREAD;
-        if( GO_BACKWARD ) {
-          ti = params.L - 1 - ti;
-        }
-
-        // Is it a valid access?
-        int valid;
-        if( GO_BACKWARD ) {
-          valid = valid_qk[ri] && ti - ci >= 0;
-        } else {
-          valid = valid_qk[ri] && ti + ci < params.L;
-        }
-
-        // The extra offset to add.
-        if( GO_BACKWARD ) {
-          offset_q = ri*THREADS_PER_WARP - ci*params.q_stride_L;
-          offset_k = ri*THREADS_PER_WARP - ci*params.k_stride_L;
-        } else {
-          offset_q = ri*THREADS_PER_WARP + ci*params.q_stride_L;
-          offset_k = ri*THREADS_PER_WARP + ci*params.k_stride_L;
-        }
-
-        // Load Q/K if they are valid.
-        q[ri][ci] = valid ? ptr_q[offset_q] : 0.f;
-        k[ri][ci] = valid ? ptr_k[offset_k] : 0.f;
-      }
-    }
-
-    // For the V tensor, we assign contiguous thread to different loads. So, ti is different.
-    int ti = iter + tidx;
-    if( GO_BACKWARD ) {
-      ti = params.L - 1 - ti;
-    }
-
-    // Is it a valid access?
-    int valid_vo = tidx < COLS_PER_ITER;
-    if( GO_BACKWARD ) {
-      valid_vo &= ti >= 0;
-    } else {
-      valid_vo &= ti < params.L;
-    }
-
-    // Trigger the loads for V. 
-    float ldg_v = valid_vo ? *ptr_v : 0.f;
-
-    // Move the load pointers.
-    if( GO_BACKWARD ) {
-      ptr_q -= COLS_PER_ITER*params.q_stride_L;
-      ptr_k -= COLS_PER_ITER*params.k_stride_L;
-      ptr_v -= COLS_PER_ITER*params.v_stride_L;
-    } else {
-      ptr_q += COLS_PER_ITER*params.q_stride_L;
-      ptr_k += COLS_PER_ITER*params.k_stride_L;
-      ptr_v += COLS_PER_ITER*params.v_stride_L;
-    }
-
-    // Store to shared memory.
-    if( tidx < COLS_PER_ITER ) {
-      smem_v[tidx] = ldg_v;
-    }
-
-    // Make sure V is in shared memory.
-    __syncthreads();
-
-    // Read V from shared memory.
-    float v[COLS_PER_THREAD];
-    #pragma unroll
-    for( int ci = 0; ci < COLS_PER_THREAD; ++ci ) {
-      v[ci] = smem_v[warp*COLS_PER_THREAD + ci];
-    }
-
-    // Each thread computes local K*V products.
-    float kv[ROWS_PER_THREAD][COLS_PER_THREAD];
-    #pragma unroll
-    for( int ri = 0; ri < ROWS_PER_THREAD; ++ri ) {
-      #pragma unroll
-      for( int ci = 0; ci < COLS_PER_THREAD; ++ci ) {
-        kv[ri][ci] = 0.f;
-      }
-    }
-
-    // Update the K*V^T product.
-    #pragma unroll
-    for( int ci = 0; ci < COLS_PER_THREAD; ++ci ) {
-      #pragma unroll
-      for( int ri = 0; ri < ROWS_PER_THREAD; ++ri ) {
-        kv[ri][ci] += k[ri][ci] * v[ci];
-      }
-    }
-
-    // We must perform the prefix sums within the thread-block. Start with the thread.
-    #pragma unroll
-    for( int ri = 0; ri < ROWS_PER_THREAD; ++ri ) {
-      #pragma unroll
-      for( int ci = 1; ci < COLS_PER_THREAD; ++ci ) {
-        kv[ri][ci] += kv[ri][ci-1];
-      }
-    }
-
-    // Store the partial sums to shared memory. Unless we have no inter-warp reduction to perform.
-    #pragma unroll
-    for( int ri = 0; ri < ROWS_PER_THREAD; ++ri ) {
-      smem_reds[warp*E + ri*THREADS_PER_WARP + lane] = kv[ri][COLS_PER_THREAD-1];
-    }
-
-    // Make sure the data is in shared memory.
-    __syncthreads();
-
-    // Each thread deals with one or more column(s) of the matrix.
-    constexpr int SUMS_PER_THREAD = (E + THREADS_PER_BLOCK-1) / THREADS_PER_BLOCK;
-    #pragma unroll
-    for( int ii = 0, idx = tidx; ii < SUMS_PER_THREAD; ++ii, idx += THREADS_PER_BLOCK ) {
-      if( idx < E ) {
-        float sum = smem_reds[idx];
-        #pragma unroll
-        for( int jj = 1; jj < WARPS; ++jj ) {
-          smem_reds[idx + jj*E] = sum += smem_reds[idx + jj*E];
-        }
-      }
-    }
-
-    // Make sure the reductions are stored in shared memory.
-    __syncthreads();
-
-    // Each thread updates his partial products.
-    #pragma unroll
-    for( int ri = 0; ri < ROWS_PER_THREAD; ++ri ) {
-      float sum = running_kv[ri];
-      if( warp > 0 ) {
-        sum += smem_reds[(warp-1)*E + lane + ri*THREADS_PER_WARP];
-      }
-      #pragma unroll
-      for( int ci = 0; ci < COLS_PER_THREAD; ++ci ) {
-        kv[ri][ci] += sum;
-      }
-    }
-
-    // Compute the partial output values for that thread.
-    float sum[COLS_PER_THREAD];
-    #pragma unroll
-    for( int ci = 0; ci < COLS_PER_THREAD; ++ci ) {
-      sum[ci] = q[0][ci] * kv[0][ci];
-      #pragma unroll
-      for( int ri = 1; ri < ROWS_PER_THREAD; ++ri ) {
-        sum[ci] += q[ri][ci] * kv[ri][ci];
-      }
-    }
-
-    // Run the parallel reductions inside the warp.
-    #pragma unroll
-    for( int mask = THREADS_PER_WARP / 2; mask >= 1; mask /= 2 ) {
-      #pragma unroll
-      for( int ci = 0; ci < COLS_PER_THREAD; ++ci ) {
-        sum[ci] += __shfl_xor_sync(uint32_t(-1), sum[ci], mask);
-      }
-    }
-
-    // Store the final output to shared memory.
-    if( lane == 0 ) {
-      #pragma unroll
-      for( int ci = 0; ci < COLS_PER_THREAD; ++ci ) {
-        smem_o[warp*COLS_PER_THREAD + ci] = sum[ci];
-      }
-    }
-
-    // Make sure the data is in shared memory.
-    __syncthreads();
-
-    // Store the output.
-    if( valid_vo ) {
-      *ptr_o = smem_o[tidx];
-    }
-
-    // Each thread updates his running kv.
-    #pragma unroll
-    for( int ri = 0; ri < ROWS_PER_THREAD; ++ri ) {
-      running_kv[ri] += smem_reds[(WARPS-1)*E + lane + ri*THREADS_PER_WARP];
-    }
-
-    // Move to next location.
-    if( GO_BACKWARD ) {
-      ptr_o -= COLS_PER_ITER*params.o_stride_L;
-    } else {
-      ptr_o += COLS_PER_ITER*params.o_stride_L;
-    }
-  }
-}
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-template< int E, bool GO_BACKWARD, int WARPS >
-int lmha_low_occupancy_(const Lmha_params<float> &params) {
-
-  // Make sure we are not going to launch an invalid grid.
-  if( params.H > 65535 || params.B > 65535 ) {
-    return 1;
-  }
-
-  // Prepare the grid and trigger the CUDA kernel.
-  dim3 grid;
-  grid.x = params.M;
-  grid.y = params.H;
-  grid.z = params.B;
-  lmha_low_occupancy_kernel<E, GO_BACKWARD, WARPS><<<grid, WARPS*THREADS_PER_WARP>>>(params);
-  return 0;
-}
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-template< int E, bool GO_BACKWARD >
-int lmha_low_occupancy_(const Lmha_params<float> &params, int blocks) {
-         if( params.M * blocks >= 8*LOW_OCCUPANCY_THRESHOLD ) {
-    return lmha_low_occupancy_<E, GO_BACKWARD,  4>(params);
-  } else if( params.M * blocks >= 4*LOW_OCCUPANCY_THRESHOLD ) {
-    return lmha_low_occupancy_<E, GO_BACKWARD,  8>(params);
-  } else {
-    return lmha_low_occupancy_<E, GO_BACKWARD, 16>(params);
-  }
-  return 1;
-}
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-template< int E, typename Params >
-static inline __device__ __host__ int smem_buffer_elts_(const Params &params) {
-  int M = round_up(params.M, 4);
-  return 2*E + 2*M;
-}
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-template< int E, int THREADS_PER_HEAD, bool GO_BACKWARD >
-__global__ 
-void lmha_kernel(Lmha_params<float> params) {
-
-  // Make sure E is a multiple of 4.
-  static_assert(E % 4 == 0, "");
-
-  // The amount of shared memory per buffer (2 buffers for double-buffering).
-  const int smem_buffer_elts = smem_buffer_elts_<E>(params);
-  // The M dimension for shared memory.
-  const int M = round_up(params.M, 4);
-
-  // Shared memory to store Q, K and V. Size is 2*smem_buffer_elts.
-  extern __shared__ float smem_[];
-
-  // The various shared memory buffers.
-  float *smem_q = &smem_[0*E];
-  float *smem_k = &smem_[1*E];
-  float *smem_v = &smem_[2*E];
-  float *smem_o = &smem_[2*E + M];
-
-  // The index of the shared memory buffer (for double-buffering).
-  int smem_curr = 0;
-
-  // The sequence processed by that block.
-  const int bi = blockIdx.y;
-  // The head processed by that block.
-  const int hi = blockIdx.x;
-
-  // The linear index of the thread.
-  const int tidx = threadIdx.x;
-
-  // The offset to the position loaded by the thread in Q.
-  int offset_q = bi*params.q_stride_B + hi*params.q_stride_H + tidx;
-  // The offset to the position loaded by the thread in K.
-  int offset_k = bi*params.k_stride_B + hi*params.k_stride_H + tidx;
-
-  // If we walk backward, account for the extra offset.
-  if( GO_BACKWARD ) {
-    offset_q += (params.L-1)*params.q_stride_L;
-    offset_k += (params.L-1)*params.k_stride_L;
-  }
-
-  // Determine the base pointers for Q and K.
-  const float *ptr_q = &params.q[offset_q];
-  const float *ptr_k = &params.k[offset_k];
-
-  // The offset to the position loaded by the thread in V and O.
-  int offset_v = bi*params.v_stride_B + hi*params.v_stride_H + tidx;
-  int offset_o = bi*params.o_stride_B + hi*params.o_stride_H + tidx;
-
-  // If we walk backward, account for the extra offset.
-  if( GO_BACKWARD ) {
-    offset_v += (params.L-1)*params.v_stride_L;
-    offset_o += (params.L-1)*params.o_stride_L;
-  }
-
-  // Determine the base pointers for V.
-  const float *ptr_v = &params.v[offset_v];
-
-  // Is it an active Q/K thread?
-  const int active_qk = tidx < params.E;
-
-  // Trigger the memory loads for Q and K.
-  float ldg_q = 0.f, ldg_k = 0.f;
-  if( active_qk ) {
-    ldg_q = *ptr_q;
-    ldg_k = *ptr_k;
-  }
-
-  // Is it an active V thread?
-  const int active_v = tidx < params.M;
-
-  // Trigger the memory loads for V. 
-  float ldg_v = 0.f;
-  if( active_v ) {
-    ldg_v = *ptr_v;
-  }
-
-  // Move the load pointers.
-  if( GO_BACKWARD ) {
-    ptr_q -= params.q_stride_L;
-    ptr_k -= params.k_stride_L;
-    ptr_v -= params.v_stride_L;
-  } else {
-    ptr_q += params.q_stride_L;
-    ptr_k += params.k_stride_L;
-    ptr_v += params.v_stride_L;
-  }
-
-  // The number of FLOAT4s per head.
-  constexpr int FLOAT4s_PER_HEAD = E / 4;
-  // The number of FLOAT4s per thread.
-  constexpr int FLOAT4s_PER_THREAD = FLOAT4s_PER_HEAD / THREADS_PER_HEAD;
-
-  // The storage for the K*V^T values.
-  float4 kv[FLOAT4s_PER_THREAD]; 
-  #pragma unroll
-  for( int ii = 0; ii < FLOAT4s_PER_THREAD; ++ii ) {
-    kv[ii] = make_float4(0.f, 0.f, 0.f, 0.f);
-  }
-
-  // The output pointer.
-  float *out_ptr = &params.out[offset_o];
-
-  // Store to shared memory Q and K.
-  if( tidx < E ) { 
-    smem_q[smem_curr*smem_buffer_elts + tidx] = ldg_q; 
-    smem_k[smem_curr*smem_buffer_elts + tidx] = ldg_k; 
-  }
-
-  // Store to shared memory V. All threads store valid values.
-  if( tidx < M ) {
-    smem_v[smem_curr*smem_buffer_elts + tidx] = ldg_v;
-  }
-
-  // The position of the thread in the V dimension.
-  int vo = tidx / THREADS_PER_HEAD;
-  int vi = tidx % THREADS_PER_HEAD;
-
-  // Iterate over the timesteps.
-  for( int ti = 0; ti < params.L; ++ti ) {
-
-    // Is it the last iteration?
-    int is_last = ti == params.L - 1;
-
-    // Trigger the next loads for Q and K.
-    if( !is_last && active_qk ) {
-      ldg_q = *ptr_q;
-      ldg_k = *ptr_k;
-    }
-
-    // Trigger the next loads for V.
-    if( !is_last && active_v ) {
-      ldg_v = *ptr_v;
-    }
-
-    // Move the load pointers.
-    if( GO_BACKWARD ) {
-      ptr_q -= params.q_stride_L;
-      ptr_k -= params.k_stride_L;
-      ptr_v -= params.v_stride_L;
-    } else {
-      ptr_q += params.q_stride_L;
-      ptr_k += params.k_stride_L;
-      ptr_v += params.v_stride_L;
-    }
-
-    // Make sure the data is in shared memory.
-    __syncthreads();
-
-    // Each thread loads 4 values from K.
-    float4 k[FLOAT4s_PER_THREAD];
-    #pragma unroll
-    for( int ii = 0; ii < FLOAT4s_PER_THREAD; ++ii ) {
-      int ki = tidx % THREADS_PER_HEAD * 4 + ii * THREADS_PER_HEAD * 4;
-      k[ii] = *reinterpret_cast<const float4*>(&smem_k[smem_curr*smem_buffer_elts + ki]);
-    }
-
-    // Each thread loads a single V value.
-    float v = 0.f;
-    if( vo < params.M ) {
-      v = *reinterpret_cast<const float *>(&smem_v[smem_curr*smem_buffer_elts + vo]);
-    }
-
-    // Update the K*V^T product.
-    #pragma unroll
-    for( int ii = 0; ii < FLOAT4s_PER_THREAD; ++ii ) {
-      kv[ii].x += k[ii].x * v;
-      kv[ii].y += k[ii].y * v;
-      kv[ii].z += k[ii].z * v;
-      kv[ii].w += k[ii].w * v;
-    }
-
-    // Load the Q values from shared memory.
-    float4 q[FLOAT4s_PER_THREAD]; 
-    #pragma unroll
-    for( int ii = 0; ii < FLOAT4s_PER_THREAD; ++ii ) {
-      int qi = tidx % THREADS_PER_HEAD * 4 + ii * THREADS_PER_HEAD * 4;
-      q[ii] = *reinterpret_cast<const float4*>(&smem_q[smem_curr*smem_buffer_elts + qi]);
-    }
-
-    // Compute the partial output value for that thread.
-    float sum = 0.f;
-    #pragma unroll
-    for( int ii = 0; ii < FLOAT4s_PER_THREAD; ++ii ) {
-      sum += q[ii].x * kv[ii].x;
-      sum += q[ii].y * kv[ii].y;
-      sum += q[ii].z * kv[ii].z;
-      sum += q[ii].w * kv[ii].w;
-    }
-
-    // Finalize the computation of the sum (if we have more than 1 thread per head).
-    if( THREADS_PER_HEAD > 1 ) {
-
-      // Finalize the sum for each head.
-      #pragma unroll
-      for( int mask = THREADS_PER_HEAD / 2; mask >= 1; mask /= 2 ) {
-        sum += __shfl_xor_sync(uint32_t(-1), sum, mask);
-      }
-
-      // Store to shared memory.
-      if( vo < M && vi == 0 ) {
-        smem_o[smem_curr*smem_buffer_elts + vo] = sum;
-      }
-
-      // Make sure the data is in shared memory.
-      __syncthreads();
-
-      // Active threads read the data to store.
-      if( active_v ) {
-        sum = smem_o[smem_curr*smem_buffer_elts + tidx];
-      }
-
-    } // THREADS_PER_HEAD > 1.
-
-    // Store the output. All the threads are active.
-    if( active_v ) {
-      *out_ptr = sum;
-    }
-
-    // Move to next location.
-    if( GO_BACKWARD ) {
-      out_ptr -= params.o_stride_L;
-    } else {
-      out_ptr += params.o_stride_L;
-    }
-
-    // Move the shared memory buffer.
-    smem_curr = (smem_curr + 1) % 2;
-
-    // Store to shared memory for Q and K.
-    if( !is_last && tidx < E ) {
-      smem_q[smem_curr*smem_buffer_elts + tidx] = ldg_q;
-      smem_k[smem_curr*smem_buffer_elts + tidx] = ldg_k;
-    }
-
-    // Store to shared memory for V.
-    if( !is_last && tidx < M ) {
-      smem_v[smem_curr*smem_buffer_elts + tidx] = ldg_v;
-    }
-  }
-}
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-template< int E, int THREADS_PER_HEAD, bool GO_BACKWARD >
-int lmha_(const Lmha_params<float> &params) {
-  // The M dimension rounded up to 4.
-  int M = round_up(params.M, 4);
-
-  // The number of threads in the block.
-  int block = round_up(max(E, M*THREADS_PER_HEAD), 32);
-  if( block > 512 || params.B > 65535 ) {
-    return 1;
-  }
-
-  // Prepare the kernel.
-  dim3 grid(params.H, params.B);
-  size_t smem = smem_buffer_elts_<E>(params)*2*sizeof(float);
-  lmha_kernel<E, THREADS_PER_HEAD, GO_BACKWARD><<<grid, block, smem>>>(params);
-  return 0;
-}
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-template< bool GO_BACKWARD >
-int lmha(const Lmha_params<float> &params) {
-  int blocks = params.B * params.H;
-  int res = 1;
-  if( blocks < LOW_OCCUPANCY_THRESHOLD ) { 
-           if( params.E <=  32 ) {
-      res = lmha_low_occupancy_< 32, GO_BACKWARD>(params, blocks);
-    } else if( params.E <=  64 ) {
-      res = lmha_low_occupancy_< 64, GO_BACKWARD>(params, blocks);
-    } else if( params.E <= 128 ) {
-      res = lmha_low_occupancy_<128, GO_BACKWARD>(params, blocks);
-    } else if( params.E <= 256 ) {
-      res = lmha_low_occupancy_<256, GO_BACKWARD>(params, blocks);
-    }
-  } else {
-           if( params.E <=  32 ) {
-      res = lmha_< 32, 1, GO_BACKWARD>(params);
-    } else if( params.E <=  48 ) {
-      res = lmha_< 48, 1, GO_BACKWARD>(params);
-    } else if( params.E <=  64 ) {
-      res = lmha_< 64, 1, GO_BACKWARD>(params);
-    } else if( params.E <= 128 ) {
-      res = lmha_<128, 2, GO_BACKWARD>(params);
-    } else if( params.E <= 256 ) {
-      res = lmha_<256, 4, GO_BACKWARD>(params);
-    }
-  }
-  return res;
-}
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-template< typename T >
-inline void set_params(Lmha_params<T> &params,
-                       const torch::Tensor q,
-                       const torch::Tensor k,
-                       const torch::Tensor v,
-                       torch::Tensor       o) {
-
-  // Define the pointers.
-  params.out = o.data_ptr<T>();
-  params.q   = q.data_ptr<T>();
-  params.k   = k.data_ptr<T>();
-  params.v   = v.data_ptr<T>();
-
-  // Define the strides.
-  params.q_stride_B = (int) q.stride(0);
-  params.q_stride_H = (int) q.stride(1);
-  params.q_stride_L = (int) q.stride(2);
-  params.k_stride_B = (int) k.stride(0);
-  params.k_stride_H = (int) k.stride(1);
-  params.k_stride_L = (int) k.stride(2);
-  params.v_stride_B = (int) v.stride(0);
-  params.v_stride_H = (int) v.stride(1);
-  params.v_stride_L = (int) v.stride(2);
-  params.o_stride_B = (int) o.stride(0);
-  params.o_stride_H = (int) o.stride(1);
-  params.o_stride_L = (int) o.stride(2);
-
-  // Extract the dimensions.
-  int N = q.size(0);
-  int H = q.size(1);
-  int L = q.size(2);
-  int E = q.size(3);
-  int M = v.size(3);
-
-  params.B = N;
-  params.L = L;
-  params.H  = H;
-  params.E = E;
-  params.M = M;
-}
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-int lmha_fwd(const torch::Tensor queries,
-             const torch::Tensor keys,
-             const torch::Tensor values,
-             torch::Tensor product) {
-
-  // Make sure that we are using the correct GPU device
-  torch::DeviceGuard _guard(queries.device());
-
-  // Make sure the inner-most dimension of the tensors is packed.
-  assert(queries.stride(3) == 1);
-  assert(keys   .stride(3) == 1);
-  assert(values .stride(3) == 1);
-  assert(product.stride(3) == 1);
-
-  // Extract the dimensions.
-  int N = queries.size(0);
-  int H = queries.size(1);
-  int L = queries.size(2);
-  int E = queries.size(3);
-  int M = values.size (3);
-
-  // The structure of params.
-  Lmha_params<float> params;
-  set_params(params, queries, keys, values, product);
-
-  // Launch the kernel.
-  return lmha<false>(params);
-}
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-template< typename T >
-struct Lmha_bwd_params {
-
-  // The output buffer for K. Dimensions [B, H, L, D].
-  T *out_k;
-  // The output buffer for V. Dimensions [B, H, L, D].
-  T *out_v;
-
-  // The input Qs. Dimensions [B, H, L, D].
-  const T *q;
-  // The input Ks. Dimensions [B, H, L, D].
-  const T *k;
-  // The input Vs. Dimensions [B, H, L, D].
-  const T *v;
-  // The input Gs. Dimensions [B, H, L, D].
-  const T *g;
-
-  // The dimensions.
-  int B, L, H, M, E;
-
-  // The strides for the input tensors.
-  int q_stride_B, q_stride_L, q_stride_H;
-  int k_stride_B, k_stride_L, k_stride_H;
-  int v_stride_B, v_stride_L, v_stride_H;
-  int g_stride_B, g_stride_L, g_stride_H;
-
-  // The strides for the outputs.
-  int out_k_stride_B, out_k_stride_L, out_k_stride_H;
-  int out_v_stride_B, out_v_stride_L, out_v_stride_H;
-};
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-template< int D, int THREADS_PER_HEAD >
-__global__ __launch_bounds__(D*THREADS_PER_HEAD*2)
-void lmha_bwd_kernel(Lmha_bwd_params<float> params) {
-
-  // Make sure D is a multiple of 4.
-  static_assert(D % 4 == 0, "");
-
-  // The shared memory buffers.
-  __shared__ struct Smem { float qg[2*D], kv[2*D], out_kv[2*D]; } smem_[2];
-
-  // The index of the shared memory buffer (for double-buffering).
-  int smem_curr = 0;
-
-  // The sequence processed by that block.
-  const int bi = blockIdx.y;
-  // The head processed by that block.
-  const int hi = blockIdx.x;
-
-  // The linear index of the thread.
-  const int tidx = threadIdx.x;
-
-  // Split the threads into two slices.
-  int so = tidx / (D*THREADS_PER_HEAD);
-  int si = tidx % (D*THREADS_PER_HEAD);
-
-  // The strides for B/L/H for the Q/G tensors.
-  int qg_stride_B, qg_stride_L, qg_stride_H;
-  if( so == 0 ) {
-    qg_stride_B = params.q_stride_B;
-    qg_stride_L = params.q_stride_L;
-    qg_stride_H = params.q_stride_H;
-  } else {
-    qg_stride_B = params.g_stride_B;
-    qg_stride_L = params.g_stride_L;
-    qg_stride_H = params.g_stride_H;
-  }
-
-  // The strides for B/L/H for the K/V tensors.
-  int kv_stride_B, kv_stride_L, kv_stride_H;
-  if( so == 0 ) {
-    kv_stride_B = params.k_stride_B;
-    kv_stride_L = params.k_stride_L;
-    kv_stride_H = params.k_stride_H;
-  } else {
-    kv_stride_B = params.v_stride_B;
-    kv_stride_L = params.v_stride_L;
-    kv_stride_H = params.v_stride_H;
-  }
-
-  // The hidden size.
-  int hidden_size_per_head = 0;
-  if( so == 0 ) {
-    hidden_size_per_head = params.E;
-  } else {
-    hidden_size_per_head = params.M;
-  }
-
-  // Where to start reading from.
-  int offset_qg = bi*qg_stride_B + hi*qg_stride_H + si;
-  int offset_kv = bi*kv_stride_B + hi*kv_stride_H + si;
-
-  // We walk backward, account for the extra offset.
-  offset_qg += (params.L-1)*qg_stride_L;
-  offset_kv += (params.L-1)*kv_stride_L;
-
-  // Determine the base pointers for Q, K, V and G.
-  const float *ptr_qg = &(so == 0 ? params.q : params.g)[offset_qg];
-  const float *ptr_kv = &(so == 0 ? params.k : params.v)[offset_kv]; 
-
-  // Is it an active thread?
-  const int active = si < hidden_size_per_head;
-
-  // Trigger the memory loads for Q, K, V and G.
-  float ldg_qg = 0.f, ldg_kv = 0.f;
-  if( active ) {
-    ldg_qg = *ptr_qg;
-    ldg_kv = *ptr_kv;
-  }
-
-  // Move the load pointers (backward).
-  ptr_qg -= qg_stride_L;
-  ptr_kv -= kv_stride_L;
-
-  // The number of FLOAT4s per head.
-  constexpr int FLOAT4s_PER_HEAD = D / 4;
-  // The number of FLOAT4s per thread.
-  constexpr int FLOAT4s_PER_THREAD = FLOAT4s_PER_HEAD / THREADS_PER_HEAD;
-
-  // The storage for the G*Q^T or Q^T*G values.
-  float4 gq[FLOAT4s_PER_THREAD]; 
-  #pragma unroll
-  for( int ii = 0; ii < FLOAT4s_PER_THREAD; ++ii ) {
-    gq[ii] = make_float4(0.f, 0.f, 0.f, 0.f);
-  }
-
-  // The strides for B/L/H for the K/V tensors.
-  int out_kv_stride_B, out_kv_stride_L, out_kv_stride_H;
-  if( so == 0 ) {
-    out_kv_stride_B = params.out_k_stride_B;
-    out_kv_stride_L = params.out_k_stride_L;
-    out_kv_stride_H = params.out_k_stride_H;
-  } else {
-    out_kv_stride_B = params.out_v_stride_B;
-    out_kv_stride_L = params.out_v_stride_L;
-    out_kv_stride_H = params.out_v_stride_H;
-  }
-
-  // Where to start reading from.
-  int offset_out_kv = bi*out_kv_stride_B + hi*out_kv_stride_H + si;
-
-  // We walk backward, account for the extra offset.
-  offset_out_kv += (params.L-1)*out_kv_stride_L;
-
-  // The output pointer.
-  float *ptr_out_kv = &(so == 0 ? params.out_k : params.out_v)[offset_out_kv];
-
-  // Store to shared memory.
-  if( si < D ) { 
-    smem_[smem_curr].qg[so*D + si] = ldg_qg; 
-    smem_[smem_curr].kv[so*D + si] = ldg_kv; 
-  }
-
-  // The position of the thread in the output dimension.
-  int oo = si / THREADS_PER_HEAD % D;
-  int oi = si % THREADS_PER_HEAD * 4;
-
-  // Iterate over the timesteps.
-  for( int ti = 0; ti < params.L; ++ti ) {
-
-    // Is it the last iteration?
-    int is_last = ti == params.L - 1;
-
-    // Trigger the next loads.
-    if( !is_last && active ) {
-      ldg_qg = *ptr_qg;
-      ldg_kv = *ptr_kv;
-    }
-
-    // Move the load pointers.
-    ptr_qg -= qg_stride_L;
-    ptr_kv -= kv_stride_L;
-
-    // Make sure the data is in shared memory.
-    __syncthreads();
-
-    // Each thread loads 4 values from G or Q.
-    float4 g[FLOAT4s_PER_THREAD];
-    #pragma unroll
-    for( int ii = 0; ii < FLOAT4s_PER_THREAD; ++ii ) {
-      float *smem_ptr = &smem_[smem_curr].qg[(so^1)*D + oi];
-      g[ii] = *reinterpret_cast<const float4*>(&smem_ptr[ii*THREADS_PER_HEAD*4]);
-    }
-
-    // Each thread loads a single from Q or G value.
-    float q = smem_[smem_curr].qg[so*D + oo];
-
-    // Update the G*Q^T or Q*G^T product.
-    #pragma unroll
-    for( int ii = 0; ii < FLOAT4s_PER_THREAD; ++ii ) {
-      gq[ii].x += g[ii].x * q;
-      gq[ii].y += g[ii].y * q;
-      gq[ii].z += g[ii].z * q;
-      gq[ii].w += g[ii].w * q;
-    }
-
-    // Load the V or K values from shared memory.
-    float4 v[FLOAT4s_PER_THREAD]; 
-    #pragma unroll
-    for( int ii = 0; ii < FLOAT4s_PER_THREAD; ++ii ) {
-      float *smem_ptr = &smem_[smem_curr].kv[(so^1)*D + oi];
-      v[ii] = *reinterpret_cast<const float4*>(&smem_ptr[ii*THREADS_PER_HEAD*4]);
-    }
-
-    // Compute the partial output value for that thread.
-    float sum = 0.f;
-    #pragma unroll
-    for( int ii = 0; ii < FLOAT4s_PER_THREAD; ++ii ) {
-      sum += v[ii].x * gq[ii].x;
-      sum += v[ii].y * gq[ii].y;
-      sum += v[ii].z * gq[ii].z;
-      sum += v[ii].w * gq[ii].w;
-    }
-
-    // Finalize the computation of the sum (if we have more than 1 thread per head).
-    if( THREADS_PER_HEAD > 1 ) {
-
-      // Finalize the sum for each head.
-      #pragma unroll
-      for( int mask = THREADS_PER_HEAD / 2; mask >= 1; mask /= 2 ) {
-        sum += __shfl_xor_sync(uint32_t(-1), sum, mask);
-      }
-
-      // Store to shared memory.
-      if( oi == 0 ) {
-        smem_[smem_curr].out_kv[so*D + oo] = sum;
-      }
-
-      // Make sure the data is in shared memory.
-      __syncthreads();
-
-      // Active threads read the data to store.
-      if( si < hidden_size_per_head ) {
-        sum = smem_[smem_curr].out_kv[so*D + si];
-      }
-
-    } // THREADS_PER_HEAD > 1.
-
-    // Store the output. All the threads are active.
-    if( si < hidden_size_per_head ) {
-      *ptr_out_kv = sum;
-    }
-
-    // Move to next location.
-    ptr_out_kv -= out_kv_stride_L;
-
-    // Move the shared memory buffer.
-    smem_curr = (smem_curr + 1) % 2;
-
-    // Store to shared memory for Q and K.
-    if( !is_last && si < D ) {
-      smem_[smem_curr].qg[so*D + si] = ldg_qg; 
-      smem_[smem_curr].kv[so*D + si] = ldg_kv; 
-    }
-  }
-}
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-template< int D, int THREADS_PER_HEAD >
-int lmha_bwd_(const Lmha_bwd_params<float> &params) {
-  int block = D*THREADS_PER_HEAD*2;
-  if( block >= 1024 || params.B > 65535 ) {
-    return 1;
-  }
-  dim3 grid(params.H, params.B);
-  lmha_bwd_kernel<D, THREADS_PER_HEAD><<<grid, block>>>(params);
-  return 0;
-}
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-int lmha_bwd(const Lmha_bwd_params<float> &params) {
-  int blocks = params.B * params.H;
-  if( blocks < LOW_OCCUPANCY_THRESHOLD ) { 
-    return 1;
-  }
-
-  int hidden_size_per_head = max(params.E, params.M);
-  int res = 1;
-  if( hidden_size_per_head <= 32 ) {
-    res = lmha_bwd_< 32, 1>(params);
-  } else if( hidden_size_per_head <= 64 ) {
-    res = lmha_bwd_< 64, 1>(params);
-  } else if( hidden_size_per_head <= 128 ) {
-    res = lmha_bwd_<128, 2>(params);
-  } else if( hidden_size_per_head <= 256 ) {
-    res = lmha_bwd_<256, 4>(params);
-  }
-  return res;
-}
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-int lmha_bwd(const torch::Tensor queries,
-             const torch::Tensor keys,
-             const torch::Tensor values,
-             const torch::Tensor grad_out,
-             torch::Tensor grad_queries,
-             torch::Tensor grad_keys,
-             torch::Tensor grad_values) {
-
-  // Make sure that we are using the correct GPU device
-  torch::DeviceGuard _guard(queries.device());
-
-  // Make sure the inner-most dimension of the tensors is packed.
-  assert(queries     .stride(3) == 1);
-  assert(keys        .stride(3) == 1);
-  assert(values      .stride(3) == 1);
-  assert(grad_out    .stride(3) == 1);
-  assert(grad_queries.stride(3) == 1);
-  assert(grad_keys   .stride(3) == 1);
-  assert(grad_values .stride(3) == 1);
-
-  // Extract the dimensions.
-  int N = queries.size(0);
-  int H = queries.size(1);
-  int L = queries.size(2);
-  int E = queries.size(3);
-  int M = values.size (3);
-
-  // Gradient on Q.
-
-  // The structure of params.
-  Lmha_params<float> params;
-  set_params(params, grad_out, values, keys, grad_queries);
-
-  // Launch the kernel.
-  int res = lmha<false>(params);
-  if( res ) {
-    return res;
-  }
-
-  // Gradient on K and V together.
-
-  Lmha_bwd_params<float> bwd_params;
-  bwd_params.out_k = grad_keys.data_ptr<float>();
-  bwd_params.out_v = grad_values.data_ptr<float>();
-  bwd_params.q = queries.data_ptr<float>();
-  bwd_params.k = keys.data_ptr<float>();
-  bwd_params.v = values.data_ptr<float>();
-  bwd_params.g = grad_out.data_ptr<float>();
-
-  bwd_params.B = N;
-  bwd_params.L = L;
-  bwd_params.H = H;
-  bwd_params.E = E;
-  bwd_params.M = M;
-
-  bwd_params.q_stride_B = queries.stride(0);
-  bwd_params.q_stride_H = queries.stride(1);
-  bwd_params.q_stride_L = queries.stride(2);
-  bwd_params.k_stride_B = keys.stride(0);
-  bwd_params.k_stride_H = keys.stride(1);
-  bwd_params.k_stride_L = keys.stride(2);
-  bwd_params.v_stride_B = values.stride(0);
-  bwd_params.v_stride_H = values.stride(1);
-  bwd_params.v_stride_L = values.stride(2);
-  bwd_params.g_stride_B = grad_out.stride(0);
-  bwd_params.g_stride_H = grad_out.stride(1);
-  bwd_params.g_stride_L = grad_out.stride(2);
-
-  bwd_params.out_k_stride_B = grad_keys.stride(0);
-  bwd_params.out_k_stride_H = grad_keys.stride(1);
-  bwd_params.out_k_stride_L = grad_keys.stride(2);
-  bwd_params.out_v_stride_B = grad_values.stride(0);
-  bwd_params.out_v_stride_H = grad_values.stride(1);
-  bwd_params.out_v_stride_L = grad_values.stride(2);
-
-  // Try to run the fused kernel.
-  int fallback = lmha_bwd(bwd_params);
-
-  // If it failed, fallback on separate kernels for K and V.
-  if( fallback ) {
-
-    // Gradient on K.
-
-    // Launch the kernel.
-    set_params(params, values, grad_out, queries, grad_keys);
-    res = lmha<true>(params);
-    if( res ) {
-      return res;
-    }
-
-    // Gradient on V.
-
-    // Launch the kernel.
-    set_params(params, keys, queries, grad_out, grad_values);
-    return lmha<true>(params);
-  }
-
-  // It worked...
-  return 0;
-}
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-} // namespace nvidia
-#endif // #ifdef ENABLE_NVIDIA_OPTIMIZATIONS
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-typedef torch::PackedTensorAccessor32<float, 4, torch::RestrictPtrTraits> float_accessor;
-
-#define E_BLOCK_SIZE 8
-
-__global__ void causal_dot_product_kernel(
-    const float_accessor queries,
-    const float_accessor keys,
-    const float_accessor values,
-    float_accessor result,
-    const int N,
-    const int H,
-    const int L,
-    const int E,
-    const int M
-) {
-    int n = blockIdx.y;
-    int h = blockIdx.z;
-
-    int e_start = blockIdx.x * E_BLOCK_SIZE;
-    int m = threadIdx.x % M;
-
-    extern __shared__ float shared_mem[];
-    float* shared_kv = shared_mem;
-
-    for (int e_local = 0; e_local < E_BLOCK_SIZE && e_local + e_start < E; e_local++) {
-      shared_kv[m + e_local * M] = 0;
-    }
-
-    for (int t=0; t<L; t++) {
-      float res = 0;
-      for (int e_local = 0; e_local < E_BLOCK_SIZE && e_local + e_start < E; e_local++) {
-        shared_kv[e_local*M + m] += keys[n][h][t][e_local + e_start] * values[n][h][t][m];
-        res += queries[n][h][t][e_local + e_start] * shared_kv[e_local*M + m];
-      }
-      atomicAdd(
-          &result[n][h][t][m],
-          res
-      );
-    }
-}
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-void causal_dot_product_(const torch::Tensor queries,
-                         const torch::Tensor keys,
-                         const torch::Tensor values,
-                         torch::Tensor product) {
-    // Make sure that we are using the correct GPU device
-    torch::DeviceGuard _guard(queries.device());
-
-    int N = queries.size(0);
-    int H = queries.size(1);
-    int L = queries.size(2);
-    int E = queries.size(3);
-    int M = values.size(3);
-
-    const int blocks_per_sequence = (E + E_BLOCK_SIZE - 1) / E_BLOCK_SIZE;
-
-    dim3 blockDim(M, 1, 1);
-    dim3 gridDim(blocks_per_sequence, N, H);
-    const int shared_mem_forward = E_BLOCK_SIZE * M * sizeof(float);
-
-    causal_dot_product_kernel<<<gridDim, blockDim, shared_mem_forward>>>(
-      queries.packed_accessor32<float, 4, torch::RestrictPtrTraits>(),
-      keys.packed_accessor32<float, 4, torch::RestrictPtrTraits>(),
-      values.packed_accessor32<float, 4, torch::RestrictPtrTraits>(),
-      product.packed_accessor32<float, 4, torch::RestrictPtrTraits>(),
-      N, H, L, E, M
-    );
-}
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-void causal_dot_product(const torch::Tensor queries,
-                        const torch::Tensor keys,
-                        const torch::Tensor values,
-                        torch::Tensor product) {
-#ifdef ENABLE_NVIDIA_OPTIMIZATIONS
-  int fallback = nvidia::lmha_fwd(queries, keys, values, product);
-#else
-  int fallback = 1;
-#endif
-  if( fallback ) {
-    causal_dot_product_(queries, keys, values, product);
-  }
-}
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-#define M_BLOCK_SIZE 4
-
-// we need shared memory to store
-// kv
-// Backward direction
-// kv_backwards
-// Shared memory usage
-__global__ void causal_dot_backward_query_key_kernel(
-    const float_accessor queries,
-    const float_accessor keys,
-    const float_accessor values,
-    const float_accessor grad_out,
-    float_accessor grad_queries,
-    float_accessor grad_keys,
-    int N,
-    int H,
-    int L,
-    int E,
-    int M
-) {
-    int n = blockIdx.y;
-    int h = blockIdx.z;
-
-    int m_start = blockIdx.x * M_BLOCK_SIZE;
-    int e = threadIdx.x % E;
-
-    extern __shared__ float shared_mem[];
-    const int shared_kv_size = M_BLOCK_SIZE * E;
-    float* shared_kv = shared_mem;
-    float* shared_kv_bw = shared_mem + shared_kv_size;
-
-    for (int m_local = 0; m_local < M_BLOCK_SIZE && m_local + m_start < M; m_local++) {
-      shared_kv[m_local * E + e] = 0;
-      shared_kv_bw[m_local * E + e] = 0;
-    }
-
-    for (int l=0; l<L; l++) {
-      float res = 0, res_bw = 0;
-      int l_b = L - l - 1;
-      for (int m_local = 0; m_local < M_BLOCK_SIZE && m_local + m_start < M; m_local++) {
-        shared_kv[m_local*E + e] += keys[n][h][l][e] * values[n][h][l][m_start + m_local];
-        shared_kv_bw[m_local*E + e] += queries[n][h][l_b][e] * grad_out[n][h][l_b][m_start + m_local];
-        res += grad_out[n][h][l][m_start + m_local] * shared_kv[m_local*E + e];
-        res_bw += values[n][h][l_b][m_start + m_local] * shared_kv_bw[m_local*E + e];
-      }
-      atomicAdd(
-        &grad_queries[n][h][l][e],
-        res
-      );
-      atomicAdd(
-        &grad_keys[n][h][l_b][e],
-        res_bw
-      );
-    }
-}
-
-
-__global__ void causal_dot_backward_value_kernel(
-    const float_accessor queries,
-    const float_accessor keys,
-    const float_accessor values,
-    const float_accessor grad_out,
-    float_accessor grad_keys,
-    float_accessor grad_values,
-    int N,
-    int H,
-    int L,
-    int E,
-    int M
-) {
-    int n = blockIdx.y;
-    int h = blockIdx.z;
-
-    int e_start = blockIdx.x * E_BLOCK_SIZE;
-    int m = threadIdx.x % M;
-
-    extern __shared__ float shared_mem[];
-    float* shared_kv = shared_mem;
-    for (int e_local = 0; e_local < E_BLOCK_SIZE && e_local + e_start < E; e_local++) {
-      shared_kv[m + e_local * M] = 0;
-    }
-
-    for (int l = 0; l < L; l++) {
-        int l_b = L - l -1;
-        float res = 0;
-        for (int e_local = 0; e_local < E_BLOCK_SIZE && e_local + e_start < E; e_local++) {
-          shared_kv[e_local*M + m] += queries[n][h][l_b][e_start + e_local] * grad_out[n][h][l_b][m];
-          res += keys[n][h][l_b][e_start + e_local] * shared_kv[e_local*M + m];
-        }
-        atomicAdd(
-            &grad_values[n][h][l_b][m],
-            res
-        );
-    }
-}
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-void causal_dot_backward_(const torch::Tensor queries,
-                          const torch::Tensor keys,
-                          const torch::Tensor values,
-                          const torch::Tensor grad_out,
-                          torch::Tensor grad_queries,
-                          torch::Tensor grad_keys,
-                          torch::Tensor grad_values) {
-
-    // Make sure that we are using the correct GPU device
-    torch::DeviceGuard _guard(queries.device());
-
-    int N = queries.size(0);
-    int H = queries.size(1);
-    int L = queries.size(2);
-    int E = queries.size(3);
-    int M = values.size(3);
-
-    const int blocks_per_sequence = (M + M_BLOCK_SIZE - 1) / M_BLOCK_SIZE;
-
-    dim3 blockDim(E, 1, 1);
-    dim3 gridDim(blocks_per_sequence, N, H);
-    const int shared_mem_qk_backward = 2 * M_BLOCK_SIZE * E * sizeof(float);
-
-    causal_dot_backward_query_key_kernel<<<gridDim, blockDim, shared_mem_qk_backward>>>(
-      queries.packed_accessor32<float, 4, torch::RestrictPtrTraits>(),
-      keys.packed_accessor32<float, 4, torch::RestrictPtrTraits>(),
-      values.packed_accessor32<float, 4, torch::RestrictPtrTraits>(),
-      grad_out.packed_accessor32<float, 4, torch::RestrictPtrTraits>(),
-      grad_queries.packed_accessor32<float, 4, torch::RestrictPtrTraits>(),
-      grad_keys.packed_accessor32<float, 4, torch::RestrictPtrTraits>(),
-      N, H, L, E, M
-    );
-
-    const int blocks_per_sequence_value = (E + E_BLOCK_SIZE - 1) / E_BLOCK_SIZE;
-
-    dim3 blockDimv(M, 1, 1);
-    dim3 gridDimv(blocks_per_sequence_value, N, H);
-    const int shared_mem_v_backward = E_BLOCK_SIZE * M * sizeof(float);
-    causal_dot_backward_value_kernel<<<gridDimv, blockDimv, shared_mem_v_backward>>>(
-      queries.packed_accessor32<float, 4, torch::RestrictPtrTraits>(),
-      keys.packed_accessor32<float, 4, torch::RestrictPtrTraits>(),
-      values.packed_accessor32<float, 4, torch::RestrictPtrTraits>(),
-      grad_out.packed_accessor32<float, 4, torch::RestrictPtrTraits>(),
-      grad_keys.packed_accessor32<float, 4, torch::RestrictPtrTraits>(),
-      grad_values.packed_accessor32<float, 4, torch::RestrictPtrTraits>(),
-      N, H, L, E, M
-    );
-}
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-void causal_dot_backward(const torch::Tensor queries,
-                         const torch::Tensor keys,
-                         const torch::Tensor values,
-                         const torch::Tensor grad_out,
-                         torch::Tensor grad_queries,
-                         torch::Tensor grad_keys,
-                         torch::Tensor grad_values) {
-#ifdef ENABLE_NVIDIA_OPTIMIZATIONS
-  int fallback = nvidia::lmha_bwd(queries,
-                                  keys,
-                                  values,
-                                  grad_out,
-                                  grad_queries,
-                                  grad_keys,
-                                  grad_values);
-#else
-  int fallback = 1;
-#endif
-  if( fallback ) {
-    // Make sure that the gradient tensors are 0. This is needed because the
-    // bwd pass might have partially executed and filled in some values in
-    // grad_queries or grad_keys.
-    //
-    // This adds a small overhead every time we have to fall back to the old
-    // kernel for the backward pass.
-    grad_queries.zero_();
-    grad_keys.zero_();
-    causal_dot_backward_(queries, keys, values, grad_out, grad_queries, grad_keys, grad_values);
-  }
-}
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
-    m.def(
-        "causal_dot_product",
-        &causal_dot_product,
-        "Compute the weighted sum of values but attending only to previous "
-        "values."
-    );
-    m.def(
-        "causal_dot_backward",
-        &causal_dot_backward,
-        "Compute the gradients for the causal dot product."
-    );
-}
diff --git a/train/csrc/causal_dot_prod/linear_attention.py b/train/csrc/causal_dot_prod/linear_attention.py
deleted file mode 100755
index 040a2cd..0000000
--- a/train/csrc/causal_dot_prod/linear_attention.py
+++ /dev/null
@@ -1,320 +0,0 @@
-import torch
-from torch.utils.benchmark import Compare, Timer
-import matplotlib.pyplot as plt
-from causal_attention_cuda import causal_dot_product
-import opt_einsum as oe
-
-def linear_attention(q, k, v, eps=1e-6, qk_normalize=True, causal=False):
-        
-    # Expand dims for broadcasting to compute linear attention
-    q, k, v = q.unsqueeze(-2), k.unsqueeze(-2), v.unsqueeze(-1)
-    # breakpoint()
-    
-    if causal:
-        a = (q * (k * v).cumsum(dim=2)).sum(dim=-1)
-        if qk_normalize:
-            a /= (q * k.cumsum(dim=2)).sum(dim=-1).clamp(eps)
-    else:
-        a = (q * (k * v).sum(dim=2, keepdim=True)).sum(dim=-1)
-        if qk_normalize:
-            a /= (q * k.sum(dim=2, keepdim=True)).sum(dim=-1).clamp(eps)
-    return a
-        
-def fast_linear_attention(q, k, v, out, eps=1e-6, qk_normalize=True, causal=False):
-    causal_dot_product(q, k, v, out)
-    if qk_normalize:
-        out /= oe.contract('n h l d, n h l d -> n h l', q, k.cumsum(2))[:, :, :, None]
-    return out
-  
-def benchmark_memory(fn, *inputs, desc='', verbose=True, **kwinputs):
-    torch.cuda.empty_cache()
-    torch.cuda.reset_peak_memory_stats()
-    torch.cuda.synchronize()
-    fn(*inputs, **kwinputs)
-    torch.cuda.synchronize()
-    mem = torch.cuda.max_memory_allocated() / ((2 ** 20) * 1000)
-    if verbose:
-        print(f'{desc} max memory: {mem}GB')
-    torch.cuda.empty_cache()
-    return mem
-
-device = 'cuda'
-b = 1
-h = 12
-d = 64
-l = 1024       
-
-
-seqlen_range = [128, 256, 512, 1024, 2048, 4096, 8192]  # , 16384, 32768, 65536]
-results = []
-mem_usage = []
-
-for l in seqlen_range:
-  q = torch.randn(b, h, l, d, device=device)
-  k = torch.randn(b, h, l, d, device=device)
-  v = torch.randn(b, h, l, d, device=device)
-  label = 'Causal Linear Attention'
-  sub_label = f'{l}'
-  results.append(Timer(
-      stmt='attn(q, k, v, qk_normalize=True, causal=True)',
-      globals={'q': q, 'k': k, 'v': v, 'attn': linear_attention},
-      label=label,
-      sub_label=sub_label,
-      description='Sequence Length vs Runtime',
-  ).blocked_autorange(min_run_time=1))
-  mem_usage.append(benchmark_memory(linear_attention, q, k, v, qk_normalize=True, causal=True))
-compare = Compare(results)
-compare.print()
-print(mem_usage)
-
-causal_linear_attn_run_times = []
-causal_linear_attn_mem_usage = []
-for result in results:
-  causal_linear_attn_run_times.append(result.median * 1000)
-  
-for mem in mem_usage:
-  causal_linear_attn_mem_usage.append(mem)
-
-
-
-results = []
-mem_usage = []
-for l in seqlen_range:
-  q = torch.randn(b, h, l, d, device=device)
-  k = torch.randn(b, h, l, d, device=device) 
-  v = torch.randn(b, h, l, d, device=device)
-  label = 'Causal Scaled Dot Product Attention'
-  sub_label = f'{l}'
-  results.append(Timer(
-      stmt='attn(q, k, v, attn_mask=None, is_causal=True)',
-      globals={'q': q, 'k': k, 'v': v, 'attn': torch.nn.functional.scaled_dot_product_attention},
-      label=label,
-      sub_label=sub_label,
-      description='Sequence Length vs Runtime',
-  ).blocked_autorange(min_run_time=1))
-  mem_usage.append(benchmark_memory(torch.nn.functional.scaled_dot_product_attention, q, k, v, attn_mask=None, is_causal=True))
-compare = Compare(results)
-compare.print()
-
-causal_sdp_attn_run_times = []
-causal_sdp_attn_mem_usage = []
-for result in results:
-  causal_sdp_attn_run_times.append(result.median * 1000)
-  
-for mem in mem_usage:
-    causal_sdp_attn_mem_usage.append(mem)
-  
-
-results = []
-mem_usage = []
-for l in seqlen_range:
-  q = torch.randn(b, h, l, d, device=device)
-  k = torch.randn(b, h, l, d, device=device) 
-  v = torch.randn(b, h, l, d, device=device)
-  out = torch.zeros((b, h, l, d), device=device)
-  label = 'Fast Causal Linear Attention'
-  sub_label = f'{l}'
-  results.append(Timer(
-      stmt='attn(q, k, v, out)',
-      globals={'q': q, 'k': k, 'v': v, 'attn': fast_linear_attention, 'out': out},
-      label=label,
-      sub_label=sub_label,
-      description='Sequence Length vs Runtime',
-  ).blocked_autorange(min_run_time=1))
-  mem_usage.append(benchmark_memory(fast_linear_attention, q, k, v, out))
-compare = Compare(results)
-compare.print()
-
-fast_causal_attn_run_times = []
-fast_causal_attn_mem_usage = []
-for result in results:
-  fast_causal_attn_run_times.append(result.median * 1000)
-  
-for mem in mem_usage:
-    fast_causal_attn_mem_usage.append(mem)
-    
-plt.plot(seqlen_range, causal_linear_attn_run_times, '-*', label='Causal Linear Attention')
-plt.plot(seqlen_range, causal_sdp_attn_run_times, '-*', label='Causal Scaled Dot Product Attention')
-plt.plot(seqlen_range, fast_causal_attn_run_times, '-*', label='Fast Causal Linear Attention')
-plt.xlabel('Sequence Length')
-plt.ylabel('Runtime (ms)')
-plt.legend()
-plt.grid()
-plt.savefig('causal_runtime.png')
-
-
-plt.figure()
-plt.plot(seqlen_range, causal_linear_attn_mem_usage, '-*', label='Causal Linear Attention')
-plt.plot(seqlen_range, causal_sdp_attn_mem_usage, '-*', label='Causal Scaled Dot Product Attention')
-plt.plot(seqlen_range, fast_causal_attn_mem_usage, '-*', label='Fast Causal Linear Attention')
-plt.xlabel('Sequence Length')
-plt.ylabel('Memory Usage (GB)')
-plt.legend()
-plt.grid()
-plt.savefig('causal_mem_usage.png')
-
-
-results = []
-mem_usage = []
-for l in seqlen_range:
-  q = torch.randn(b, h, l, d, device=device)
-  k = torch.randn(b, h, l, d, device=device)
-  v = torch.randn(b, h, l, d, device=device)
-  label = 'Bidirectional Linear Attention'
-  sub_label = f'{l}'
-  results.append(Timer(
-      stmt='attn(q, k, v, qk_normalize=True, causal=False)',
-      globals={'q': q, 'k': k, 'v': v, 'attn': linear_attention},
-      label=label,
-      sub_label=sub_label,
-      description='Sequence Length vs Runtime',
-  ).blocked_autorange(min_run_time=1))
-  mem_usage.append(benchmark_memory(linear_attention, q, k, v, qk_normalize=True, causal=False))
-compare = Compare(results)
-compare.print()
-
-bidirectional_linear_attn_run_times = []
-for result in results:
-  bidirectional_linear_attn_run_times.append(result.median * 1000)
-
-bidirectional_linear_mem_usage = []
-for mem in mem_usage:
-    bidirectional_linear_mem_usage.append(mem)
-    
-    
-results = []
-mem_usage = []
-for l in seqlen_range:
-  q = torch.randn(b, h, l, d, device=device)
-  k = torch.randn(b, h, l, d, device=device) 
-  v = torch.randn(b, h, l, d, device=device)
-  label = 'Bidirectional Scaled Dot Product Attention'
-  sub_label = f'{l}'
-  results.append(Timer(
-      stmt='attn(q, k, v, attn_mask=None, is_causal=False)',
-      globals={'q': q, 'k': k, 'v': v, 'attn': torch.nn.functional.scaled_dot_product_attention},
-      label=label,
-      sub_label=sub_label,
-      description='Sequence Length vs Runtime',
-  ).blocked_autorange(min_run_time=1))
-  mem_usage.append(benchmark_memory(torch.nn.functional.scaled_dot_product_attention, q, k, v, attn_mask=None, is_causal=False))
-compare = Compare(results)
-compare.print()
-
-bidirectional_sdp_attn_run_times = []
-for result in results:
-  bidirectional_sdp_attn_run_times.append(result.median * 1000)
-  
-bidirectional_sdp_mem_usage = []
-for mem in mem_usage:
-    bidirectional_sdp_mem_usage.append(mem)
-    
-plt.figure()
-plt.plot(seqlen_range, bidirectional_linear_attn_run_times, '-*', label='Bidirectional Linear Attention')
-plt.plot(seqlen_range, bidirectional_sdp_attn_run_times, '-*', label='Bidirectional Scaled Dot Product Attention')
-plt.xlabel('Sequence Length')
-plt.ylabel('Runtime (ms)')
-plt.legend()
-plt.grid(True)
-plt.savefig('bidirectional_runtime.png')
-
-
-plt.figure()
-plt.plot(seqlen_range, bidirectional_linear_mem_usage, '-*', label='Bidirectional Linear Attention')
-plt.plot(seqlen_range, bidirectional_sdp_mem_usage, '-*', label='Bidirectional Scaled Dot Product Attention')
-plt.xlabel('Sequence Length')
-plt.ylabel('Memory Usage (GB)')
-plt.legend()
-plt.grid(True)
-plt.savefig('bidirectional_mem_usage.png')
-
-# repeats = 11
-
-# import time
-
-# linear_attention(q, k, v, qk_normalize=False, causal=True)
-# torch.cuda.synchronize()
-# start = time.time()
-# for _ in range(repeats):
-#     linear_attention(q, k, v, qk_normalize=False, causal=True)
-# torch.cuda.synchronize()
-# end = time.time()
-
-# print("Linear Attention Time taken: ", (end - start) / repeats)
-
-
-# torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, is_causal=True)
-# torch.cuda.synchronize()
-# start = time.time()
-# for _ in range(repeats):
-#     torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, is_causal=True)
-# torch.cuda.synchronize()
-# end = time.time()
-
-# print("Scaled Dot Product Attention Time taken: ", (end - start) / repeats)
-
-# d = 768
-# q = torch.randn(b, h, l, d, device=device)
-# k = torch.randn(b, h, l, d, device=device)
-# v = torch.randn(b, h, l, d, device=device)
-
-
-# repeats = 5
-# linear_attention(q, k, v, qk_normalize=True, causal=True)
-# torch.cuda.synchronize()
-# start = time.time()
-# for _ in range(repeats):
-#     linear_attention(q, k, v, qk_normalize=True, causal=True)
-# torch.cuda.synchronize()
-# end = time.time()
-# print("Linear Attention Time taken: ", (end - start) / repeats)
-
-
-# torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, is_causal=True)
-# torch.cuda.synchronize()
-# start = time.time()
-# for _ in range(repeats):
-#     torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, is_causal=True)
-# torch.cuda.synchronize()
-# end = time.time()
-
-# print("Scaled Dot Product Attention Time taken: ", (end - start) / repeats)
-
-# # print(linear_attention(q, k, v, qk_normalize=True, causal=True).shape)
-
-
-
-
-# #Linear Attention VS sequence length
-# b = 4
-# h = 12
-# l = 1024
-# d = 64        
-# q = torch.randn(b, h, l, d)
-# k = torch.randn(b, h, l, d)
-# v = torch.randn(b, h, l, d)
-
-
-# # from torch.utils.benchmark import Compare, Timer
-
-# # results = []
-# # seqlen_range = [128, 256, 512, 1024, 2048, 4096, 8192]
-# # for L in seqlen_range:
-# #   x = torch.randnn(b, L, D, device=device)
-# #   attn = CausalSelfAttention(D, H, L)
-# #   attn.to(device)
-# #   label = 'Multi-headed Causal Self Attention'
-# #   sub_label = f'{L}'
-# #   results.append(Timer(
-# #       stmt='attn(x)',
-# #       globals={'x': x, 'attn': attn},
-# #       label=label,
-# #       sub_label=sub_label,
-# #       description='Sequence Length vs Runtime',
-# #   ).blocked_autorange(min_run_time=1))
-# # compare = Compare(results)
-# # compare.print()
-
-# # medians = []
-# # for result in results:
-# #   medians.append(result.median * 1000)
\ No newline at end of file
diff --git a/train/csrc/causal_dot_prod/setup.py b/train/csrc/causal_dot_prod/setup.py
deleted file mode 100755
index d095497..0000000
--- a/train/csrc/causal_dot_prod/setup.py
+++ /dev/null
@@ -1,47 +0,0 @@
-import torch
-from setuptools import setup
-from torch.utils.cpp_extension import BuildExtension, CUDAExtension, CUDA_HOME
-import subprocess
-
-def get_last_arch_torch():
-    arch = torch.cuda.get_arch_list()[-1]
-    print(f"Found arch: {arch} from existing torch installation")
-    return arch
-
-def get_cuda_bare_metal_version(cuda_dir):
-    raw_output = subprocess.check_output([cuda_dir + "/bin/nvcc", "-V"], universal_newlines=True)
-    output = raw_output.split()
-    release_idx = output.index("release") + 1
-    release = output[release_idx].split(".")
-    bare_metal_major = release[0]
-    bare_metal_minor = release[1][0]
-
-    return raw_output, bare_metal_major, bare_metal_minor
-
-def append_nvcc_threads(nvcc_extra_args):
-    _, bare_metal_major, bare_metal_minor = get_cuda_bare_metal_version(CUDA_HOME)
-    if int(bare_metal_major) >= 11 and int(bare_metal_minor) >= 2:
-        return nvcc_extra_args + ["--threads", "4"]
-    return nvcc_extra_args
-
-arch = get_last_arch_torch()
-# [MP] make install more flexible here
-sm_num = arch[-2:]
-cc_flag = ['--generate-code=arch=compute_80,code=compute_80']
-
-
-setup(
-    name='causal_attention_cuda_cpp',
-    ext_modules=[
-        CUDAExtension('causal_attention_cuda', [
-            #'causal_attention.cpp',
-            'causal_attention_cuda.cu',
-        ],
-        extra_compile_args={'cxx': ['-O3'],
-                             'nvcc': append_nvcc_threads(['-O3', '-lineinfo', '--use_fast_math', '-std=c++17'] + cc_flag)
-                             # 'nvcc': append_nvcc_threads(['-O3', '-lineinfo', '--use_fast_math', '-std=c++14'] + cc_flag)
-                            })
-    ],
-    cmdclass={
-        'build_ext': BuildExtension
-    })
diff --git a/train/csrc/causal_dot_prod/test.py b/train/csrc/causal_dot_prod/test.py
deleted file mode 100755
index ddac5e1..0000000
--- a/train/csrc/causal_dot_prod/test.py
+++ /dev/null
@@ -1,34 +0,0 @@
-import torch
-from causal_attention_cuda import causal_dot_product
-torch.random.manual_seed(42)
-
-b = 1
-h = 32
-l = 2048
-d = 64
-device = 'cuda'
-eps = 1e-6
-
-torch.set_default_dtype(torch.bfloat16)
-
-q = torch.randn(b, h, l, d).to(device).contiguous() 
-k = torch.randn(b, h, l, d).to(device).contiguous()
-v = torch.randn(b, h, l, d).to(device).contiguous()
-
-# For valid attention, the dot-prods b/t q and k should be positive
-q, k, v = q.abs(), k.abs(), v.abs()
-
-# Fast linear attention
-z = 1 / (torch.einsum("nhli,nhli->nhl", q, k.cumsum(2)) + eps)
-v_fast = causal_dot_product(q, k, v) * z[..., None]
-
-# Slow linear attention
-q, k, v = q.unsqueeze(-2), k.unsqueeze(-2), v.unsqueeze(-1)
-print(q.shape, k.shape, v.shape)
-v_slow = (q * (k * v).cumsum(dim=2)).sum(dim=-1) / ((q * k.cumsum(dim=2)).sum(dim=-1) + eps)
-
-print((v_fast - v_slow).abs().max())  # 7.444406946888193e-05
-print(v_fast.shape)
-print(v_slow.shape)
-# print(v_fast)
-# print(v_slow)
diff --git a/train/datamodules/language_modeling_hf.py b/train/datamodules/language_modeling_hf.py
index 4f39421..eb72a20 100755
--- a/train/datamodules/language_modeling_hf.py
+++ b/train/datamodules/language_modeling_hf.py
@@ -79,7 +79,7 @@ class LMDataModule(LightningDataModule):
 
     def prepare_data(self):
         if self.cache_dir is None:  # Just download the dataset
-            load_dataset(self.dataset_name, self.dataset_config_name)
+            load_dataset(self.dataset_name, self.dataset_config_name, cache_dir=self.cache_dir)
         else:  # Process the dataset and save it
             self.process_dataset()
 
@@ -100,7 +100,7 @@ class LMDataModule(LightningDataModule):
             if cache_dir.is_dir():
                 return self._load_from_cache(cache_dir)
 
-        raw_datasets = load_dataset(self.dataset_name, self.dataset_config_name)
+        raw_datasets = load_dataset(self.dataset_name, self.dataset_config_name, cache_dir=self.cache_dir)
         # https://github.com/stanford-crfm/mistral/blob/main/src/corpora/auto.py
         if 'validation' not in raw_datasets:
             assert "train" in raw_datasets, "You must have train in raw_datasets to make a validation raw_datasets"
diff --git a/train/optim/param_grouping.py b/train/optim/param_grouping.py
index c0c986b..ff0bd61 100755
--- a/train/optim/param_grouping.py
+++ b/train/optim/param_grouping.py
@@ -9,7 +9,7 @@ try:
 except ImportError:
     FastLayerNorm = None
 
-from based.models.modules.seq_common import PositionalEncoding
+from tktrainer.models.modules.seq_common import PositionalEncoding
 
 
 def group_parameters_for_optimizer(model, optimizer_cfg, bias_weight_decay=False,
