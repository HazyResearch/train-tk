[[36m2024-10-24 19:56:46,044[39m][[34mtrain.training[39m][[32mINFO[39m] - Instantiating trainer <pytorch_lightning.Trainer>
[[36m2024-10-24 19:56:46,049[39m][[34mpytorch_lightning.utilities.rank_zero[39m][[32mINFO[39m] - Using 16bit native Automatic Mixed Precision (AMP)
[[36m2024-10-24 19:56:46,051[39m][[34mpytorch_lightning.utilities.rank_zero[39m][[32mINFO[39m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[[36m2024-10-24 19:56:46,121[39m][[34mpytorch_lightning.utilities.rank_zero[39m][[32mINFO[39m] - GPU available: True (cuda), used: True
[[36m2024-10-24 19:56:46,175[39m][[34mpytorch_lightning.utilities.rank_zero[39m][[32mINFO[39m] - TPU available: False, using: 0 TPU cores
[[36m2024-10-24 19:56:46,175[39m][[34mpytorch_lightning.utilities.rank_zero[39m][[32mINFO[39m] - IPU available: False, using: 0 IPUs
[[36m2024-10-24 19:56:46,176[39m][[34mpytorch_lightning.utilities.rank_zero[39m][[32mINFO[39m] - HPU available: False, using: 0 HPUs
[[36m2024-10-24 19:56:46,177[39m][[34mtrain.training[39m][[32mINFO[39m] - Starting training!
[[36m2024-10-24 19:56:46,184[39m][[34mtrain.utils.utils[39m][[32mINFO[39m] - Load from cache at /scratch/openwebtext/cache/tokenizer_name-gpt2-val_ratio-0.0005-val_split_seed-2357-add_eos-True-detokenize-False
[[36m2024-10-24 19:56:46,386[39m][[34mtrain.utils.utils[39m][[32mINFO[39m] - Load from cache at /scratch/openwebtext/cache/tokenizer_name-gpt2-val_ratio-0.0005-val_split_seed-2357-add_eos-True-detokenize-False
[[36m2024-10-24 19:56:46,588[39m][[34mtrain.callbacks.gpu_affinity[39m][[32mINFO[39m] - 0: thread affinity: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}
[[36m2024-10-24 19:56:46,757[39m][[34mpytorch_lightning.accelerators.cuda[39m][[32mINFO[39m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [5]
Error executing job with overrides: ['experiment=tk/owt_tk_gpts', 'trainer.devices=1']
Traceback (most recent call last):
  File "/home/rahul/code/.venv/lib/python3.12/site-packages/hydra/_internal/utils.py", line 635, in _locate
    obj = import_module(part0)
          ^^^^^^^^^^^^^^^^^^^^
  File "/home/rahul/.local/share/uv/python/cpython-3.12.5-linux-x86_64-gnu/lib/python3.12/importlib/__init__.py", line 90, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1387, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1360, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1324, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'apex'
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File "/home/rahul/code/.venv/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 134, in _resolve_target
    target = _locate(target)
             ^^^^^^^^^^^^^^^
  File "/home/rahul/code/.venv/lib/python3.12/site-packages/hydra/_internal/utils.py", line 637, in _locate
    raise ImportError(
ImportError: Error loading 'apex.optimizers.FusedAdam':
ModuleNotFoundError("No module named 'apex'")
Are you sure that module 'apex' is installed?
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File "/home/rahul/code/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 38, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rahul/code/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 645, in _fit_impl
    self._run(model, ckpt_path=self.ckpt_path)
  File "/home/rahul/code/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1079, in _run
    self.strategy.setup(self)
  File "/home/rahul/code/.venv/lib/python3.12/site-packages/pytorch_lightning/strategies/single_device.p