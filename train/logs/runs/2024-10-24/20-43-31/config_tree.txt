CONFIG
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                                                    
│       accelerator: gpu                                                                                       
│       min_epochs: 1                                                                                          
│       max_epochs: 1000                                                                                       
│       devices: 1                                                                                             
│       num_nodes: 1                                                                                           
│       accumulate_grad_batches: 64                                                                            
│       max_steps: 400000                                                                                      
│       val_check_interval: 64000                                                                              
│       check_val_every_n_epoch: null                                                                          
│       precision: 16                                                                                          
│       gradient_clip_val: 1.0                                                                                 
│       strategy: null                                                                                         
│                                                                                                              
├── model
│   └── _target_: flash_attn.models.gpt.GPTLMHeadModel                                                         
│       _recursive_: true                                                                                      
│       config:                                                                                                
│         _target_: transformers.GPT2Config                                                                    
│         reorder_and_upcast_attn: false                                                                       
│         scale_attn_by_inverse_layer_idx: true                                                                
│         n_positions: 768                                                                                     
│         n_embd: 768                                                                                          
│         n_head: 12                                                                                           
│         n_layer: 12                                                                                          
│         mha_type: tk                                                                                         
│                                                                                                              
├── datamodule
│   └── _target_: train.datamodules.language_modeling_hf.LMDataModule                                          
│       dataset_name: openwebtext                                                                              
│       dataset_config_name: null                                                                              
│       tokenizer_name: gpt2                                                                                   
│       cache_dir: /scratch/openwebtext/cache                                                                  
│       max_length: 768                                                                                        
│       val_ratio: 0.0005                                                                                      
│       val_split_seed: 2357                                                                                   
│       add_eos: true                                                                                          
│       batch_size: 8                                                                                          
│       batch_size_eval: 8                                                                                     
│       num_workers: 32                                                                                        
│       shuffle: true                                                                                          
│       pin_memory: true                                                                                       
│       fault_tolerant: true                                                                                   
│       ddp: false                                                                                             
│                                                                                                              
├── train
│   └── optimizer:                                                                                             
│         _target_: torch.optim.AdamW                                                                          
│         lr: 0.00015                                                                                          
│         weight_decay: 0.1                                                                                    
│       scheduler:                                                                                             
│         _target_: transformers.get_linear_schedule_with_warmup                                               
│         num_warmup_steps: 4000.0                                                                             
│         num_training_steps: 400000                                                                           
│       gpu_mem: 82                                                                                            
│       global_batch_size: 512                                                                                 
│       optimizer_param_grouping:                                                                              
│         bias_weight_decay: false                                                                             
│         normalization_weight_decay: false                                                                    
│       loss_fn:                                                                                               
│         _target_: flash_attn.losses.cross_entropy.CrossEntropyLoss                                           
│         inplace_backward: true                                                                               
│                                                                                                              
├── eval
│   └── metrics:                                                                                               
│         ppl:                                                                                                 
│           _target_: train.metrics.perplexity.Perplexity                                                      
│         num-tokens:                                                                                          
│           _target_: train.metrics.num_tokens.NumTokens                                                       
│       log_on_step: true                                                                                      
│                                                                                                              
├── callbacks
│   └── rich_model_summary:                                                                                    
│         _target_: pytorch_lightning.callbacks.RichModelSummary                                               
│       model_checkpoint:                                                                                      
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                                                
│         monitor: val/loss                                                                                    
│         mode: min                                                                                            
│         save_top_k: 1                                                                                        
│         save_last: true                                                                                      
│         verbose: false                                                                                       
│         dirpath: /home/rahul/simran/based/train/checkpoints/10-20-attn=tk-repo=2-n=768-model=gpt2-small-pin=F
│         filename: step_{step}                                                                                
│         auto_insert_metric_name: false                                                                       
│         every_n_train_steps: 1000                                                                            
│       early_stopping: null                                                                                   
│       learning_rate_monitor:                                                                                 
│         _target_: pytorch_lightning.callbacks.LearningRateMonitor                                            
│         logging_interval: step                                                                               
│       speed_monitor:                                                                                         
│         _target_: train.callbacks.speed_monitor.SpeedMonitor                                                 
│         intra_step_time: true                                                                                
│         inter_step_time: true                                                                                
│         epoch_time: true                                                                                     
│       loss_scale_monitor:                                                                                    
│         _target_: train.callbacks.loss_scale_monitor.LossScaleMonitor                                        
│       params_log:                                                                                            
│         _target_: train.callbacks.params_log.ParamsLog                                                       
│         total_params_log: true                                                                               
│         trainable_params_log: true                                                                           
│         non_trainable_params_log: true                                                                       
│       gpu_affinity:                                                                                          
│         _target_: train.callbacks.gpu_affinity.GpuAffinity                                                   
│       norm_monitor:                                                                                          
│         _target_: train.callbacks.norm_monitor.NormMonitor                                                   
│       model_checkpoint_progress:                                                                             
│         _target_: train.callbacks.model_checkpoint.ModelCheckpointMine                                       
│         fault_tolerant: true                                                                                 
│         every_n_train_steps: 50000                                                                           
│         save_last: false                                                                                     
│         save_top_k: -1                                                                                       
│         dirpath: /home/rahul/simran/based/train/checkpoints/10-20-attn=tk-repo=2-n=768-model=gpt2-small-pin=F
│         filename: progress_step_{step}                                                                       
│         auto_insert_metric_name: false                                                                       
│                                                                                                              
├── logger
│   └── wandb:                                                                                                 
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                                                
│         project: based                                                                                       
│         name: 10-20-attn=tk-repo=2-n=768-model=gpt2-small-pin=F-sync-F-v3                                    
│         save_dir: .                                                                                          
│         mode: online                                                                                         
│         id: 10-20-attn=tk-repo=2-n=768-model=gpt2-small-pin=F-sync-F-v3                                      
│         log_model: false                                                                                     
│         prefix: ''                                                                                           
│         job_type: train                                                                                      
│         group: ''                                                                                            
│         tags: []                                                                                             
│                                                                                                              
├── seed
│   └── 1111                                                                                                   
└── name
    └── 10-20-attn=tk-repo=2-n=768-model=gpt2-small-pin=F-sync-F-v3                                            
